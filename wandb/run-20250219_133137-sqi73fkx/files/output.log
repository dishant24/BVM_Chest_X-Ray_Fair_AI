Loading images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4928/4928 [00:04<00:00, 1026.80it/s, Loaded=4928]
C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py:143: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  training_data_merge['race_encoded'] = label_encoder.fit_transform(training_data_merge['race'])
Loading images: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:00<00:00, 1036.42it/s, Loaded=980]
C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py:143: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  training_data_merge['race_encoded'] = label_encoder.fit_transform(training_data_merge['race'])
(array([0, 1, 2, 3, 4]), array([ 566,  235,  706,  579, 2842]))
(array([0, 1, 2, 3, 4]), array([118,  54, 120, 103, 585]))
Traceback (most recent call last):                                                                                                                                                                                                     
Epoch [1/20.0000], Train AUC: 0.5056, Train Acc: 0.1356, Train Loss: 1.5537, Val AUC: 0.5152, Val Acc: 0.0802, Val Loss: 1.4985
Epoch [2/20.0000], Train AUC: 0.5243, Train Acc: 0.0786, Train Loss: 1.4659, Val AUC: 0.5247, Val Acc: 0.0823, Val Loss: 1.4311
Epoch [3/20.0000], Train AUC: 0.5446, Train Acc: 0.0670, Train Loss: 1.4354, Val AUC: 0.5132, Val Acc: 0.0837, Val Loss: 1.4017
Epoch [4/20.0000], Train AUC: 0.5503, Train Acc: 0.0745, Train Loss: 1.4008, Val AUC: 0.5270, Val Acc: 0.0737, Val Loss: 1.3988
Epoch [5/20.0000], Train AUC: 0.5618, Train Acc: 0.0652, Train Loss: 1.4077, Val AUC: 0.5312, Val Acc: 0.0803, Val Loss: 1.3870
Epoch [6/20.0000], Train AUC: 0.5633, Train Acc: 0.0715, Train Loss: 1.3893, Val AUC: 0.5320, Val Acc: 0.0899, Val Loss: 1.3567
Epoch [7/20.0000], Train AUC: 0.5676, Train Acc: 0.0668, Train Loss: 1.3933, Val AUC: 0.5401, Val Acc: 0.0871, Val Loss: 1.3624
Epoch [8/20.0000], Train AUC: 0.5705, Train Acc: 0.0673, Train Loss: 1.3894, Val AUC: 0.5376, Val Acc: 0.0878, Val Loss: 1.3576
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 568, in <module>
    model_training(model, train_loader, val_loader, criterion,  20, device=device, multi_label=False)
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 368, in model_training
    for batch_idx, (inputs, labels) in enumerate(train_loop):
                                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\tqdm\std.py", line 1181, in __iter__
    for obj in iterable:
               ^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 163, in __getitem__
    img = self.transform(img)
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
          ^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\transforms.py", line 1372, in forward
    return F.rotate(img, angle, self.interpolation, self.expand, self.center, fill)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\functional.py", line 1132, in rotate
    return F_t.rotate(img, matrix=matrix, interpolation=interpolation.value, expand=expand, fill=fill)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\_functional_tensor.py", line 669, in rotate
    return _apply_grid_transform(img, grid, interpolation, fill=fill)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\_functional_tensor.py", line 575, in _apply_grid_transform
    img = _cast_squeeze_out(img, need_cast, need_squeeze, out_dtype)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\_functional_tensor.py", line 532, in _cast_squeeze_out
    def _cast_squeeze_out(img: Tensor, need_cast: bool, need_squeeze: bool, out_dtype: torch.dtype) -> Tensor:

KeyboardInterrupt
