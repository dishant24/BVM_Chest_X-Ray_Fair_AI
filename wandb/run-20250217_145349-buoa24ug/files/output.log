Loading images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58067/58067 [01:56<00:00, 500.04it/s, Loaded=58067]
Loading images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6452/6452 [00:12<00:00, 503.60it/s, Loaded=6452]
Traceback (most recent call last):                                                                                                                                                                                                     
Epoch [1/20], Train AUC: 0.6573, Train Acc: 0.2159, Train Loss: 0.4081, Val AUC: 0.6952, Val Acc: 0.2420, Val Loss: 0.3959
Epoch [2/20], Train AUC: 0.6943, Train Acc: 0.2442, Train Loss: 0.3923, Val AUC: 0.7276, Val Acc: 0.2527, Val Loss: 0.3824
Epoch [3/20], Train AUC: 0.7126, Train Acc: 0.2596, Train Loss: 0.3847, Val AUC: 0.7343, Val Acc: 0.2900, Val Loss: 0.3794
Epoch [4/20], Train AUC: 0.7249, Train Acc: 0.2683, Train Loss: 0.3796, Val AUC: 0.7397, Val Acc: 0.2737, Val Loss: 0.3801
Epoch [5/20], Train AUC: 0.7349, Train Acc: 0.2750, Train Loss: 0.3755, Val AUC: 0.7468, Val Acc: 0.2990, Val Loss: 0.3738
Epoch [6/20], Train AUC: 0.7417, Train Acc: 0.2816, Train Loss: 0.3724, Val AUC: 0.7489, Val Acc: 0.2953, Val Loss: 0.3739
Epoch [7/20], Train AUC: 0.7482, Train Acc: 0.2877, Train Loss: 0.3698, Val AUC: 0.7561, Val Acc: 0.2976, Val Loss: 0.3772
Epoch [8/20], Train AUC: 0.7526, Train Acc: 0.2926, Train Loss: 0.3672, Val AUC: 0.7560, Val Acc: 0.2948, Val Loss: 0.3730
Epoch [9/20], Train AUC: 0.7570, Train Acc: 0.2997, Train Loss: 0.3651, Val AUC: 0.7578, Val Acc: 0.3194, Val Loss: 0.3718
Epoch [10/20], Train AUC: 0.7592, Train Acc: 0.3020, Train Loss: 0.3640, Val AUC: 0.7617, Val Acc: 0.3057, Val Loss: 0.3665
Epoch [11/20], Train AUC: 0.7625, Train Acc: 0.3076, Train Loss: 0.3627, Val AUC: 0.7646, Val Acc: 0.3430, Val Loss: 0.3670
Epoch [12/20], Train AUC: 0.7649, Train Acc: 0.3121, Train Loss: 0.3613, Val AUC: 0.7665, Val Acc: 0.3156, Val Loss: 0.3643
Epoch [13/20], Train AUC: 0.7676, Train Acc: 0.3170, Train Loss: 0.3599, Val AUC: 0.7644, Val Acc: 0.3138, Val Loss: 0.3707
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 421, in <module>
    torch.save(model.state_dict(), 'diagnostic_model.pth')
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 262, in model_training
    inputs, tr_labels = inputs.to(device), tr_labels.to(device)
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\tqdm\std.py", line 1181, in __iter__
    for obj in iterable:
               ^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 139, in __getitem__
    img = self.transform(img)
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
          ^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\transforms.py", line 973, in forward
    return F.resized_crop(img, i, j, h, w, self.size, self.interpolation, antialias=self.antialias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\functional.py", line 650, in resized_crop
    img = resize(img, size, interpolation, antialias=antialias)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\functional.py", line 479, in resize
    return F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\_functional_tensor.py", line 472, in resize
    img = _cast_squeeze_out(img, need_cast=need_cast, need_squeeze=need_squeeze, out_dtype=out_dtype)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\_functional_tensor.py", line 540, in _cast_squeeze_out
    img = img.to(out_dtype)
          ^^^^^^^^^^^^^^^^^
KeyboardInterrupt
