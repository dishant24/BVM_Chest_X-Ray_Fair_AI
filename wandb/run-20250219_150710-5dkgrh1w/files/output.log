Loading images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5903/5903 [00:05<00:00, 1074.34it/s, Loaded=5903]
C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py:143: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  training_data_merge['race_encoded'] = label_encoder.fit_transform(training_data_merge['race'])
Loading images: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:00<00:00, 1051.25it/s, Loaded=980]
C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py:143: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  training_data_merge['race_encoded'] = label_encoder.fit_transform(training_data_merge['race'])
(array([0, 1, 2, 3, 4]), array([ 668,  278,  836,  687, 3434]))
(array([0, 1, 2, 3, 4]), array([118,  54, 120, 103, 585]))
Traceback (most recent call last):                                                                                                                                                                                                     
Epoch [1/20.0000], Train AUC: 0.5525, Train Acc: 0.2350, Train Loss: 1.5980, Val AUC: 0.5447, Val Acc: 0.2124, Val Loss: 1.6023
Epoch [2/20.0000], Train AUC: 0.5781, Train Acc: 0.2572, Train Loss: 1.5831, Val AUC: 0.5604, Val Acc: 0.2674, Val Loss: 1.5911
Epoch [3/20.0000], Train AUC: 0.5763, Train Acc: 0.2512, Train Loss: 1.5835, Val AUC: 0.5570, Val Acc: 0.2565, Val Loss: 1.5955
Epoch [4/20.0000], Train AUC: 0.5887, Train Acc: 0.2650, Train Loss: 1.5756, Val AUC: 0.5513, Val Acc: 0.2383, Val Loss: 1.6047
Epoch [5/20.0000], Train AUC: 0.5926, Train Acc: 0.2739, Train Loss: 1.5737, Val AUC: 0.5777, Val Acc: 0.3007, Val Loss: 1.5775
Epoch [6/20.0000], Train AUC: 0.5895, Train Acc: 0.2657, Train Loss: 1.5747, Val AUC: 0.5612, Val Acc: 0.2397, Val Loss: 1.6039
Epoch [7/20.0000], Train AUC: 0.5900, Train Acc: 0.2665, Train Loss: 1.5748, Val AUC: 0.5678, Val Acc: 0.2566, Val Loss: 1.5930
Epoch [8/20.0000], Train AUC: 0.5965, Train Acc: 0.2823, Train Loss: 1.5671, Val AUC: 0.5594, Val Acc: 0.2566, Val Loss: 1.5997
Epoch [9/20.0000], Train AUC: 0.5943, Train Acc: 0.2642, Train Loss: 1.5729, Val AUC: 0.5893, Val Acc: 0.2397, Val Loss: 1.5752
Epoch [10/20.0000], Train AUC: 0.5930, Train Acc: 0.2664, Train Loss: 1.5703, Val AUC: 0.5533, Val Acc: 0.2327, Val Loss: 1.6129
Epoch [11/20.0000], Train AUC: 0.5960, Train Acc: 0.2796, Train Loss: 1.5701, Val AUC: 0.5800, Val Acc: 0.2725, Val Loss: 1.5833
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 564, in <module>
    model_training(model, train_loader, val_loader, criterion,  20, device=device, multi_label=False)
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 368, in model_training
    for batch_idx, (inputs, labels) in enumerate(train_loop):
                                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\tqdm\std.py", line 1181, in __iter__
    for obj in iterable:
               ^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\CXR_Preprocessing\cxr_preprocessing\chexpert_model.py", line 163, in __getitem__
    img = self.transform(img)
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
          ^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\transforms.py", line 973, in forward
    return F.resized_crop(img, i, j, h, w, self.size, self.interpolation, antialias=self.antialias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\functional.py", line 650, in resized_crop
    img = resize(img, size, interpolation, antialias=antialias)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\functional.py", line 479, in resize
    return F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\_functional_tensor.py", line 462, in resize
    img, need_cast, need_squeeze, out_dtype = _cast_squeeze_in(img, [torch.float32, torch.float64])
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sutariya\Desktop\Thesis\.venv\Lib\site-packages\torchvision\transforms\_functional_tensor.py", line 528, in _cast_squeeze_in
    img = img.to(req_dtype)
          ^^^^^^^^^^^^^^^^^
KeyboardInterrupt
