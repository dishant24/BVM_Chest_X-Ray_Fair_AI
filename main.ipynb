{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from skimage import exposure, io, color\n",
    "from skimage.exposure import match_histograms\n",
    "torch.cuda.empty_cache()\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import skimage\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"cxr_preprocessing\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"DenseNet\",\n",
    "    \"dataset\": \"CheXpert\",\n",
    "    \"epochs\": 20,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path =  r'..\\..\\datasets\\CheXpert-v1.0-small\\train.csv'\n",
    "training_data = pd.read_csv(training_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00001/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00002/study2/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>87</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00002/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>83</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00002/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>83</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00003/study1/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  CheXpert-v1.0-small/train/patient00001/study1/...  Female   68   \n",
       "1  CheXpert-v1.0-small/train/patient00002/study2/...  Female   87   \n",
       "2  CheXpert-v1.0-small/train/patient00002/study1/...  Female   83   \n",
       "3  CheXpert-v1.0-small/train/patient00002/study1/...  Female   83   \n",
       "4  CheXpert-v1.0-small/train/patient00003/study1/...    Male   41   \n",
       "\n",
       "  Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "0         Frontal    AP         1.0                         NaN           NaN   \n",
       "1         Frontal    AP         NaN                         NaN          -1.0   \n",
       "2         Frontal    AP         NaN                         NaN           NaN   \n",
       "3         Lateral   NaN         NaN                         NaN           NaN   \n",
       "4         Frontal    AP         NaN                         NaN           NaN   \n",
       "\n",
       "   Lung Opacity  Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  \\\n",
       "0           NaN          NaN    NaN            NaN        NaN          NaN   \n",
       "1           1.0          NaN   -1.0           -1.0        NaN         -1.0   \n",
       "2           1.0          NaN    NaN           -1.0        NaN          NaN   \n",
       "3           1.0          NaN    NaN           -1.0        NaN          NaN   \n",
       "4           NaN          NaN    1.0            NaN        NaN          NaN   \n",
       "\n",
       "   Pneumothorax  Pleural Effusion  Pleural Other  Fracture  Support Devices  \n",
       "0           0.0               NaN            NaN       NaN              1.0  \n",
       "1           NaN              -1.0            NaN       1.0              NaN  \n",
       "2           NaN               NaN            NaN       1.0              NaN  \n",
       "3           NaN               NaN            NaN       1.0              NaN  \n",
       "4           0.0               NaN            NaN       NaN              NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path                               0\n",
       "Sex                                0\n",
       "Age                                0\n",
       "Frontal/Lateral                    0\n",
       "AP/PA                          32387\n",
       "No Finding                    201033\n",
       "Enlarged Cardiomediastinum    178575\n",
       "Cardiomegaly                  177211\n",
       "Lung Opacity                  105636\n",
       "Lung Lesion                   211470\n",
       "Edema                         137458\n",
       "Consolidation                 152792\n",
       "Pneumonia                     195806\n",
       "Atelectasis                   154971\n",
       "Pneumothorax                  144480\n",
       "Pleural Effusion               90203\n",
       "Pleural Other                 216922\n",
       "Fracture                      211220\n",
       "Support Devices               100197\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "gray_image = io.imread(r'..//..//datasets' + '/' + str(training_data['Path'][10]))\n",
    "\n",
    "\n",
    "clahe_1 = exposure.equalize_adapthist(gray_image, kernel_size=(8, 8), clip_limit=0.01)   # Small tile size\n",
    "clahe_2 = exposure.equalize_adapthist(gray_image, kernel_size=(32, 32), clip_limit=0.01)   # Medium tile size\n",
    "clahe_3 = exposure.equalize_adapthist(gray_image, kernel_size=(16, 16), clip_limit=0.01) # Large tile size\n",
    "\n",
    "\n",
    "# Display results\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "# Original Image\n",
    "ax[0].imshow(gray_image, cmap='gray')\n",
    "ax[0].set_title('Original')\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Low Clip Limit\n",
    "ax[1].imshow(clahe_1, cmap='gray')\n",
    "ax[1].set_title('CLAHE (clip=0.01)')\n",
    "ax[1].axis('off')\n",
    "\n",
    "# Moderate Clip Limit\n",
    "ax[2].imshow(clahe_2, cmap='gray')\n",
    "ax[2].set_title('CLAHE (clip=0.03)')\n",
    "ax[2].axis('off')\n",
    "\n",
    "# High Clip Limit\n",
    "ax[3].imshow(clahe_3, cmap='gray')\n",
    "ax[3].set_title('CLAHE (clip=0.1)')\n",
    "ax[3].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = r'..//..//datasets' + '/' + str(training_data['Path'][10])\n",
    "img = plt.imread(sample_image)\n",
    "equalized_image = exposure.equalize_hist(img)\n",
    "ref_img = plt.imread('../../datasets/reference_img.jpg')\n",
    "img = np.array(img, dtype=np.float16)\n",
    "ref_img = np.array(ref_img, dtype=np.float16)\n",
    "matched_img = exposure.match_histograms(img, ref_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(12, 10))\n",
    "\n",
    "# Show images\n",
    "ax[0, 0].imshow(img, cmap='gray')\n",
    "ax[0, 0].set_title('Original Image')\n",
    "ax[0, 1].imshow(equalized_image, cmap='gray')\n",
    "ax[0, 1].set_title('Equalized Image')\n",
    "ax[0, 2].imshow(matched_img, cmap='gray')\n",
    "ax[0, 2].set_title('Histogram Matched Image')\n",
    "\n",
    "# Histograms (Intensity Distribution)\n",
    "ax[1, 0].hist(img.ravel(), bins=256, range=(0, 255))\n",
    "ax[1, 0].set_title('Original Image Histogram')\n",
    "ax[1, 1].hist(equalized_image.ravel(), bins=256, range=(0, 1))\n",
    "ax[1, 1].set_title('Equalized Image Histogram')\n",
    "ax[1, 2].hist(matched_img.ravel(), bins=256, range=(0, 1))\n",
    "ax[1, 2].set_title('Matched Image Histogram')\n",
    "\n",
    "# Cumulative Histograms\n",
    "ax[2, 0].hist(img.ravel(), bins=256, range=(0, 255), cumulative=True)\n",
    "ax[2, 0].set_title('Original Cumulative Histogram')\n",
    "ax[2, 1].hist(equalized_image.ravel(), bins=256, range=(0, 1), cumulative=True)\n",
    "ax[2, 1].set_title('Equalized Cumulative Histogram')\n",
    "ax[2, 2].hist(matched_img.ravel(), bins=256, range=(0, 1), cumulative=True)\n",
    "ax[2, 2].set_title('Matched Cumulative Histogram')\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = r'..//..//datasets' + '/' + str(training_data['Path'][10])\n",
    "image = plt.imread(sample_image)\n",
    "wimage = image * skimage.filters.window('hann', image.shape)\n",
    "wimage = wimage - wimage.mean()\n",
    "f_shift = np.fft.fft2(wimage)\n",
    "\n",
    "f_shift = np.fft.fftshift(f_shift)\n",
    "amplitude_spectrum = np.abs(f_shift)\n",
    "log_amplitude_spectrum = np.log(amplitude_spectrum)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(log_amplitude_spectrum, cmap=\"gray\")\n",
    "plt.title(\"Original Image Spectrum\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ = read_image(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_f = img_.to(torch.float32)\n",
    "img_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\sutariya\\AppData\\Local\\Temp\\ipykernel_14584\\1595071097.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  demographic_data = pd.read_csv('..\\\\..\\datasets\\CheXpert-v1.0-small\\demographics_CXP.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>insurance_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42142</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "      <td>Private Insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4528</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "      <td>Private Insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55652</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "      <td>Medicare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53157</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "      <td>Medicare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11162</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "      <td>Medicare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   race                ethnicity     insurance_type\n",
       "0       42142  White  Non-Hispanic/Non-Latino  Private Insurance\n",
       "1        4528  White  Non-Hispanic/Non-Latino  Private Insurance\n",
       "2       55652  White  Non-Hispanic/Non-Latino           Medicare\n",
       "3       53157  White  Non-Hispanic/Non-Latino           Medicare\n",
       "4       11162  Asian  Non-Hispanic/Non-Latino           Medicare"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_data = pd.read_csv('..\\\\..\\datasets\\CheXpert-v1.0-small\\demographics_CXP.csv')\n",
    "demographic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_data['race'].value_counts().index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['White', 'Other', 'Unknown', 'Asian', 'Black', 'Pacific Islander',\n",
       "       'Native American', 'Patient Refused'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = training_data['Path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patientid = []\n",
    "for i in path:\n",
    "     id = i.split(sep='/')[2]\n",
    "     id = id.replace(\"patient\", \"\")\n",
    "     patientid.append(float(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patientid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pd.DataFrame(patientid,columns=['patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient['patient_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['subject_id'] = patient['patient_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.subject_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = training_data.merge(demographic_data, on='subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.subject_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_positive_sample(group):\n",
    "    \"\"\"\n",
    "    Selects the row with the highest number of disease-positive labels from a group of records belonging to the same patient.\n",
    "    If all records have zero disease labels, it selects one at random.\n",
    "    \"\"\"\n",
    "    disease_columns = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture'\n",
    "    ]\n",
    "    \n",
    "    # Count the number of positive disease labels for each sample\n",
    "    group['positive_count'] = group[disease_columns].sum(axis=1)\n",
    "    \n",
    "    # Filter to only include rows where at least one disease is positive\n",
    "    positive_cases = group[group['positive_count'] > 0]\n",
    "    \n",
    "    if not positive_cases.empty:\n",
    "        # Select the sample with the highest number of positive labels\n",
    "        selected_sample = positive_cases.loc[positive_cases['positive_count'].idxmax()]\n",
    "    else:\n",
    "        # If no disease-positive samples exist, pick one randomly\n",
    "        selected_sample = group.sample(n=1).iloc[0]\n",
    "    \n",
    "    return selected_sample\n",
    "\n",
    "def sampling_datasets(training_dataset):\n",
    "    \"\"\"\n",
    "    Applies the sampling strategy: selecting one recording per patient (`subject_id`),\n",
    "    preferring samples with at least one disease-positive label.\n",
    "    \"\"\"\n",
    "    # Group by patient ID and apply the selection function\n",
    "    training_dataset = training_dataset.groupby('subject_id', group_keys=False).apply(select_most_positive_sample)\n",
    "    \n",
    "    # Drop the temporary 'positive_count' column\n",
    "    training_dataset.drop(columns=['positive_count'], inplace=True, errors='ignore')\n",
    "    \n",
    "    return training_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = sampling_datasets(training_data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge['Frontal/Lateral'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']] = training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']].replace(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = training_data_merge[training_data_merge['Frontal/Lateral'] == 'Frontal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge[['No Finding']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge[['Edema']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.subject_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx] / 255.0\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images = []\n",
    "for path in training_data_merge['Path']:\n",
    "     full_path = '../../datasets' + '/' + str(path)\n",
    "     img = read_image(full_path)\n",
    "     data_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = 0,0\n",
    "images = [transform(image) for image in data_images]\n",
    "for img in images:\n",
    "     img = img.view(-1)\n",
    "     mean += img.mean().sum()\n",
    "     std += img.std().sum()\n",
    "\n",
    "mean /= len(data_images)\n",
    "std /= len(data_images)\n",
    "\n",
    "print(f\"Mean is : {mean:.4f}, STD is : {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data_merge['No Finding'].values\n",
    "data_labels = torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Lambda(lambda i: i/255),\n",
    "    transforms.Lambda(lambda i: i.to(torch.float32)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data_images,labels,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in dataloader:\n",
    "     print(i.shape)\n",
    "     print(l.shape)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_diagnostic_images_labels(training_data_merge):\n",
    "    data_images = []\n",
    "    paths = tqdm(training_data_merge['Path'], desc=\"Loading images\")\n",
    "    for path in paths:\n",
    "        full_path = '../../datasets' + '/' + str(path)\n",
    "        img = read_image(full_path)\n",
    "        data_images.append(img)\n",
    "        paths.set_postfix({'Loaded': len(data_images)})\n",
    "\n",
    "    data_labels = training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture']].values\n",
    "    data_labels = torch.tensor(data_labels, dtype=torch.float32)\n",
    "    data_labels = torch.argmax(data_labels, dim=1)\n",
    "\n",
    "    return data_images, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(data_images, labels,shuffle=True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop((256,256), scale=(0.7, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.Lambda(lambda i: i/255),\n",
    "        transforms.Lambda(lambda i: i.to(torch.float32)),\n",
    "        transforms.Normalize(mean=[0.50,0.50,0.50], std=[0.28,0.28,0.28])\n",
    "    ])\n",
    "\n",
    "    dataset = MyDataset(data_images,labels,transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=32, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv('../../datasets/train_clean_dataset.csv')\n",
    "validation_dataset = pd.read_csv('../../datasets/validation_clean_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_images,train_labels = store_diagnostic_images_labels(training_dataset)\n",
    "val_data_images, val_lables = store_diagnostic_images_labels(validation_dataset)\n",
    "train_loader = prepare_dataloaders(train_data_images,train_labels, shuffle=True)\n",
    "val_loader = prepare_dataloaders(val_data_images, val_lables,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store pixel values\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "for images, _ in dataloader:\n",
    "    # images shape: (batch_size, 1, 256 * 256)\n",
    "    batch_samples = images.size(0)  # Batch size\n",
    "    images = images.view(batch_samples, -1)  # Flatten pixels\n",
    "    mean += images.mean(dim=1).sum()\n",
    "    std += images.std(dim=1).sum()\n",
    "    num_samples += batch_samples\n",
    "\n",
    "# Final mean and std\n",
    "mean /= num_samples\n",
    "std /= num_samples\n",
    "\n",
    "print(f\"Mean: {mean.item():.4f}, Std: {std.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_races = training_data_merge['race'].value_counts().index[:5]\n",
    "training_data_merge['race'] = training_data_merge['race'].where(training_data_merge['race'].isin(top_5_races))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.get_dummies(training_data_merge, columns=['race'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data_merge[['race_Asian', 'race_Black', 'race_Other', 'race_Unknown', 'race_White']].values\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.argmax(labels, dim=1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Lambda(lambda x: x.to(torch.float32)),\n",
    "    transforms.Lambda(lambda i: i.repeat(3, 1, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet_Model(nn.Module):\n",
    "     def __init__(self, weights, out_feature):\n",
    "          super().__init__()\n",
    "          self.weight = weights\n",
    "          self.out_feature = out_feature\n",
    "          self.encoder = torchvision.models.densenet121(weights=weights)\n",
    "          self.layer1 = nn.Linear(1000, 120)\n",
    "          self.clf = nn.Linear(120, out_feature)\n",
    "\n",
    "     \n",
    "     def encode(self, x):\n",
    "          return self.encoder(x)\n",
    "\n",
    "     def forward(self, x):\n",
    "          z = self.encode(x)\n",
    "          z = self.layer1(z)\n",
    "          return self.clf(z)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet_Model(weights=None, out_feature=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torchvision.models.densenet121(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data_images,labels,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset)) \n",
    "val_size = int(0.10 * len(dataset))  \n",
    "test_size = len(dataset) - train_size - val_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, img in train_loader:\n",
    "     print(label.shape, img.shape)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_roc_auc(y_true, y_scores, log=True, log_name=\"roc_auc_curve\"):\n",
    "\n",
    "    y_scores = np.array(y_scores)\n",
    "    classes = np.unique(y_true) \n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    \n",
    "    total_roc = 0\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    \n",
    "    for i, class_label in enumerate(classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        total_roc += roc_auc\n",
    "        ax.plot(fpr, tpr, lw=2, label=f\"Class {class_label} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "    # Plot settings\n",
    "    ax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"Multi-Class ROC Curve\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Log to wandb\n",
    "    if log:\n",
    "        wandb.log({log_name: wandb.Image(fig)})\n",
    "    else:\n",
    "        print(f\"{log_name} : {total_roc/len(classes)}\")\n",
    "\n",
    "    # Close figure to free memory\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, train_loader, val_loader, num_epochs=10, device=None, is_binary=False):\n",
    "    model = model.to(device)\n",
    "\n",
    "    all_train_labels, all_train_preds = [], []\n",
    "    all_val_labels, all_val_preds = [], []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    early_stopper = EarlyStopper(patience=3)\n",
    "    criterion = nn.BCEWithLogitsLoss() if is_binary else nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\", leave=False)\n",
    "        for inputs, tr_labels in train_loop:\n",
    "            inputs, tr_labels = inputs.to(device), tr_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).to(device)\n",
    "\n",
    "            # Convert predictions & labels\n",
    "            if is_binary:\n",
    "                tr_labels = tr_labels.unsqueeze(dim=1)\n",
    "                tr_preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            else:\n",
    "                tr_preds = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            all_train_labels.extend(tr_labels.cpu().numpy())\n",
    "            all_train_preds.extend(tr_preds)\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, tr_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, vl_labels in val_loop:\n",
    "                inputs, vl_labels = inputs.to(device), vl_labels.to(device)\n",
    "\n",
    "                outputs = model(inputs).to(device)\n",
    "                if is_binary:\n",
    "                    vl_labels = vl_labels.unsqueeze(dim=1)\n",
    "                    vl_preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "                else:\n",
    "                    vl_preds = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "                all_val_labels.extend(vl_labels.cpu().numpy())\n",
    "                all_val_preds.extend(vl_preds)\n",
    "\n",
    "                loss = criterion(outputs, vl_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "        # Log metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        log_roc_auc(all_train_labels, all_train_preds, log=True, log_name='Training ROC-AUC')\n",
    "        log_roc_auc(all_val_labels, all_val_preds, log=True, log_name='Validation ROC-AUC')\n",
    "\n",
    "    # Final ROC-AUC Logging\n",
    "    log_roc_auc(all_train_labels, all_train_preds, log=False, log_name='Training ROC-AUC')\n",
    "    log_roc_auc(all_val_labels, all_val_preds, log=False, log_name='Validation ROC-AUC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('no_finding_model_weights.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_model = DenseNet_Model(weights=weights,out_feature=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in race_model.encoder.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training(race_model,train_loader,val_loader,num_epochs=10,is_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(race_model.state_dict(), 'race_finding_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = DenseNet_Model(weights=None, out_feature=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('race_finding_model_weights.pth',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_state_dict(weights,strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(test_loader, model, device, is_binary=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_test_labels, all_test_preds = [], []\n",
    "\n",
    "    loader = tqdm(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if is_binary:\n",
    "                labels = labels.view(-1, 1)\n",
    "                predicted = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            else:\n",
    "                predicted = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "            all_test_preds.extend(predicted)\n",
    "\n",
    "    log_roc_auc(all_test_labels, all_test_preds, log=True, log_name='Testing ROC-AUC')\n",
    "    log_roc_auc(all_test_labels, all_test_preds, log=False, log_name='Testing ROC-AUC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model(test_loader,test_model,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
