{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\main\\mimic\\dataset\\train_mask_clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def plot_original_and_masked_images(normal_loader, masked_loader, num_images=100):\n",
    "    images_shown = 0\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(6, num_images * 2.5))\n",
    "    fig.suptitle(\"Original vs Masked Images\", fontsize=20)\n",
    "\n",
    "    for (orig_batch, _), (masked_batch, _) in zip(normal_loader, masked_loader):\n",
    "        batch_size = orig_batch.size(0)\n",
    "        for i in range(batch_size):\n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "\n",
    "            orig_img = orig_batch[i]\n",
    "            masked_img = masked_batch[i]\n",
    "\n",
    "            orig_img_np = orig_img.permute(1, 2, 0).numpy()\n",
    "            masked_img_np = orig_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "            axes[images_shown, 0].imshow(orig_img_np, cmap='gray')\n",
    "            axes[images_shown, 0].axis(\"off\")\n",
    "            axes[images_shown, 0].set_title(\"Original\")\n",
    "\n",
    "            axes[images_shown, 1].imshow(masked_img_np, cmap='gray')\n",
    "            axes[images_shown, 1].axis(\"off\")\n",
    "            axes[images_shown, 1].set_title(\"Masked\")\n",
    "\n",
    "            images_shown += 1\n",
    "\n",
    "        if images_shown >= num_images:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from typing import Optional, Tuple, Union, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "from scipy.ndimage import binary_dilation\n",
    "from skimage.morphology import disk\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def decode_rle_numpy(rle_str: Optional[str], shape: Tuple[int, int]) -> np.ndarray:\n",
    "    if pd.isna(rle_str) or not isinstance(rle_str, str):\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "\n",
    "    rle = np.fromiter(map(int, rle_str.strip().split()), dtype=np.int32)\n",
    "    starts = rle[0::2] - 1  \n",
    "    lengths = rle[1::2]\n",
    "\n",
    "    mask = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for start, length in zip(starts, lengths):\n",
    "        mask[start:start+length] = 1\n",
    "    return mask.reshape((shape[1], shape[0])).T\n",
    "\n",
    "class ApplyLungMask:\n",
    "    def __init__(\n",
    "        self,\n",
    "        margin_radius: int = 20,\n",
    "        original_shape: Tuple[int, int] = (1024, 1024),\n",
    "        image_shape: Tuple[int, int] = (224, 224),\n",
    "    ):\n",
    "        self.margin_radius = margin_radius\n",
    "        self.original_shape = original_shape\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    def dilate_mask(self, mask: np.ndarray) -> np.ndarray:\n",
    "        selem = disk(self.margin_radius)\n",
    "        return binary_dilation(mask, structure=selem).astype(np.uint8)\n",
    "\n",
    "    def resize_mask(self, mask: np.ndarray) -> np.ndarray:\n",
    "        resized = cv2.resize(mask, (self.image_shape[1], self.image_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        return resized.astype(np.uint8)\n",
    "\n",
    "    def compute_combined_mask(\n",
    "        self, left_rle: Optional[str], right_rle: Optional[str], heart_rle: Optional[str]\n",
    "    ) -> np.ndarray:\n",
    "        left = self.dilate_mask(decode_rle_numpy(left_rle, self.original_shape))\n",
    "        right = self.dilate_mask(decode_rle_numpy(right_rle, self.original_shape))\n",
    "        heart = self.dilate_mask(decode_rle_numpy(heart_rle, self.original_shape))\n",
    "        combined = np.clip(left + right + heart, 0, 1)\n",
    "        return self.resize_mask(combined)\n",
    "\n",
    "def compute_mask_entry(row: pd.Series, masker: ApplyLungMask) -> Tuple[str, np.ndarray]:\n",
    "    key = row[\"Path\"]\n",
    "    mask = masker.compute_combined_mask(row[\"Left Lung\"], row[\"Right Lung\"], row[\"Heart\"])\n",
    "    return key, mask.astype(np.uint8)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: Union[list, pd.Series],\n",
    "        labels: Union[list, np.ndarray, pd.Series],\n",
    "        dataframe: pd.DataFrame,\n",
    "        masked: bool = False,\n",
    "        transform: torchvision.transforms.Compose = None,\n",
    "        base_dir: Optional[str] = None,\n",
    "        is_multilabel: bool = True,\n",
    "    ):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.labels = labels\n",
    "        self.masked = masked\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.masker = ApplyLungMask(margin_radius=60, original_shape=(1024, 1024), image_shape=(224, 224))\n",
    "        self.mask_cache: Dict[str, np.ndarray] = {}\n",
    "\n",
    "        if self.masked:\n",
    "            rows = [row for _, row in self.df.iterrows()] \n",
    "            results = Parallel(n_jobs=multiprocessing.cpu_count(), backend='loky')(\n",
    "                delayed(compute_mask_entry)(row, self.masker) for row in tqdm(rows)\n",
    "            )\n",
    "            self.mask_cache = dict(results)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        file_path = self.image_paths[idx]\n",
    "        full_path = os.path.join(self.base_dir, file_path) if self.base_dir else file_path\n",
    "\n",
    "        image = Image.open(full_path).convert(\"L\").resize((224, 224))\n",
    "\n",
    "        if self.masked:\n",
    "            image_np = np.array(image)\n",
    "            mask = self.mask_cache[file_path]\n",
    "            masked_image_np = image_np * mask\n",
    "            image = Image.fromarray(masked_image_np.astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_multilabel:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def prepare_mimic_dataloaders(\n",
    "    images_path: Union[pd.Series, list, str],\n",
    "    labels: Union[pd.Series, list, None],\n",
    "    dataframe: pd.DataFrame,\n",
    "    masked: bool,\n",
    "    base_dir: Optional[str] = None,\n",
    "    shuffle: bool = False,\n",
    "    is_multilabel: bool = True,\n",
    ") -> DataLoader:\n",
    "    transform = transforms.Compose(\n",
    "        [   \n",
    "            # CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(\n",
    "                (224, 224), interpolation=transforms.InterpolationMode.BICUBIC\n",
    "            ),\n",
    "            transforms.Lambda(lambda i: i.repeat(3, 1, 1) if i.shape[0] == 1 else i),\n",
    "            transforms.Normalize(mean=[0.5062] * 3, std=[0.2873] * 3),\n",
    "            transforms.RandomResizedCrop(\n",
    "                (200, 200),\n",
    "                scale=(0.9, 1.0),\n",
    "                ratio=(0.9, 1.1),\n",
    "                interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "            ),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(0.3),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "        ]\n",
    "    )\n",
    "    dataset = MyDataset(\n",
    "        images_path,\n",
    "        labels,\n",
    "        dataframe,\n",
    "        masked,\n",
    "        transform,\n",
    "        base_dir,\n",
    "        is_multilabel=is_multilabel,\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=4, shuffle=shuffle, num_workers=12, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.iloc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 111.50it/s]\n",
      "c:\\Users\\sutariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Shared arguments\n",
    "common_args = {\n",
    "    \"Path\": dataset['Path'],\n",
    "    \"labels\": None,\n",
    "    \"dataset\": dataset,\n",
    "    \"base_dir\": '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/',\n",
    "    \"is_multilabel\": True\n",
    "}\n",
    "\n",
    "# Create both datasets\n",
    "masked_dataset = MyDataset(\n",
    "        dataset['Path'],\n",
    "        None,\n",
    "        dataset,\n",
    "        True,\n",
    "        True,\n",
    "        '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/',\n",
    "        is_multilabel=False,\n",
    "    )\n",
    "masked_loader = DataLoader(masked_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "normal_dataset = MyDataset(\n",
    "        dataset['Path'],\n",
    "        None,\n",
    "        dataset,\n",
    "        False,\n",
    "        True,\n",
    "        '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/',\n",
    "        is_multilabel=False,\n",
    "    )\n",
    "normal_loader = DataLoader(normal_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "plot_original_and_masked_images(normal_loader, masked_loader, num_images=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to NumPy array\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        # If grayscale, apply CLAHE directly\n",
    "        if len(img_np.shape) == 2:\n",
    "            img_np = self.clahe.apply(img_np)\n",
    "        # If RGB, convert to LAB, apply CLAHE on L channel\n",
    "        elif len(img_np.shape) == 3 and img_np.shape[2] == 3:\n",
    "            lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "            lab[..., 0] = self.clahe.apply(lab[..., 0])\n",
    "            img_np = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        # Convert back to PIL image\n",
    "        return Image.fromarray(img_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    CLAHETransform(),  # Apply CLAHE first\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomResizedCrop(\n",
    "        (200, 200),\n",
    "        scale=(0.9, 1.0),\n",
    "        ratio=(0.9, 1.1),\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(0.3),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\main\\mimic\\dataset\\validation_clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pth = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/'+ train_data['Path'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_pth, cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clahe = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAAAAACIM/FCAACHI0lEQVR4AWJkQAAeBBOF9R/GY2QAMf8zMPz/+5+V5T8DC9N/JgZGxv9gYWZWHt7bzExMjP8ZGV7AdDDDGFD6L5SmPsVC2EiQ4yGqoKz//6Hu+f///z+QPxiYGBmYmJgZWbQ4OVhZWFj+7Wdm+s/4n+nf32cMDAyMDAz/YR5iZgB7muEfxEAqkkTECNT5MEv///3HwPCfhZWBgZmRAeQBJgbGv8zMzEwsbLzc3BzMjOxcHO8FWP4w/WdhXsD4/9ef3/+YmT9CdcM8AuUyUM9HyB7hhhoPSUHgoASJgDwCwgz/mRj+/wdZ/Y+RgQ3kC2ZGBiZWZgYG5r/MLEysrFz8bJwM//+zCnIzsTH9YeFnYrnyn/H7i8dfeYUusjIxMTEw/n8EMpCBgYEJSsMpiqMIW9ICuxqUBhgZ/v//D3Y/xJ7//xkYmFiYmH4xMzOC/MnCwMDKxsLAyPKLlZmZnY2VmY2d8TcDL8s3jv///gmysjGpczF85fzKxMHnLsLKwcjGyjSZAZTB/jC8gfoA7iEwg5GBAZpoodIkUNg8ArIM5AHmfxCPgNzPyMTEyPwLlJeZGBg4GP//B2Xxf6yMLGysjMy/uFiYWdhY2TlYWJh//2P48+0PIyPTV65f7IL/P355z8gvrifIzfqfnZHpX9IPhj8/f3z6e+g/w99//////wpLETAalplI9hCyRyAxAXUj2DcMDExMTMw/mZiYmRgZGRnYmUDxAML/GZn+MzAysDCzsLMxMjEzszKzsrGwsjMzMTIxMzKy/WL7x8DI9o3lxz9GQdE3rALszFyMjH9/M36R+/jl+7/vnIyWfz7//Pf736/HoKT6n5GB4Tc48GHeYSDdQ6geARWkYI8wMDAyMf9iYgZ5AFQmMYLy9X9mUO4BWQYqdRlAiY6DhY2dieUXMzsLMxs7Cxsjy39WdjY2rvf/GP6xMLL++8jA/vkby58fDMwfWJh/cTF//f3l48+/v/6y8H//+/8/y19mib9///1j+M/w99N/UPiA8iDYR3ACFkME0xySR9j+gGKBkZH5LyMzEygvMzGBHM0I9gioGAXxQMUqAwMjIwMoGBlY2JlYWFhYmFhYWZk52JhYGZn+c3Bx/mcSZvnHyPSR9R/Tr58P73///1Pj42/Wf5xsjAyfvv1lYP3369+/P38YQMZz/f/398///wx//oJqp////v8CWYLFQwSjCMkj4JQFioQ/zOBwYAR5BuoBUIRA3A7yxX+oP0CJioWFlYWZmZmVhYP9DyhZMbIz//7Oz/qHkZH1BxMTw+cvArxv/jxS+v+D+f/7/wxMLH9YGZlZfv5hYWFgYGT7/f/fv3///jAwfGH9z/Dv33+G/2zgUoTh/09InDBCShkIB0yCncaAWSoge4SREZwX/rOBY5nxPyModGD4P9gvoMQLSl+gvM7IwMLGzMLEzMLCwMTMysbB+o+Bken/3z8Mv/5//fefg5+NkfHnv9+///xn/PHpD+tPpv9Mf//x8/z68/MnA+MPNs4fDAyMzP8Yfv/7//c/w6////79Z/zHwMgMyiegKhRUcYKDFux8zAoH3UNIHgFFBsi1kDABsUCGgsyBeALU9gCnM1BVwsDIwMgjwMHIxsnMxMbEyMbD9u8fExMzw99fvwVZ/vxj+svIzsL1+y/LX+afX3/+e/eOk5PpP8fv7+yMLEyMLH++/GRmBVVKzP/+sf799/83IzvDnz///4JSLCgQGRkZ2EBFJchrIPtBEcAE8hkDA0YUwTyE5BEOmLtBkQBKUgzgKgRkAiMDqJACFVyMTEz//zIx/mdgZGThYgUZy8jCwMLAzsb0/z8bE+PfvyzC/38zM/xlE+X4/+8/24+vv5iZOP4ysnGzsLGyMTK8+cXG/v/f918s7Cy//zL+Z+T9+/ff/39//v9i/vX77z9GUOYD+QSUE0GZ8D8jqBoAVWkskNQM8h6IC8FQT4IpJI+w/YPoAvkDlKrAaQvseLDZDIz/mRhB9TNYHwMTMxsLwz8mFmZGxv//BIT+fWNi/s/A8I+Vh4OZi+EF538Onjf/fnx98fX757+sIoYi/xjZuBnYPjKx/vv+7/eP/0wsDMz//oGaCr///PvP/vcnC9v3T5AGKCgpMIJin4GB4R8jOMBBpQHIVnChDwliht8gZ4KUgCQYGJBrdmZmhv+gSgpkEihLg5wPTmag4GdgZGT8z8zEwvT/PwPIxYwsbCxM/5iYGf///8vOKcTzj/HXfwamf8wcbGyczL8lWFg4/rB8efrv3edvP3+xffnIx/afkfX/y8///zMy//3NyvTjPwvTX1CG+88EanX+YmP+++EfyEKQZxiY/v8HlSeglgwoM4Kc+u8/KJ39Z2SCJj9GVlArgwFWAf1nRIoRcG33////30zQNAYWYGRgBBXxjKA0xczK/BcU+3+ZwE0TJiZmFlDcM3Iw/v3P9e8HIyMjGxPD/z9/WH//Yv79/RcL68NffxgFfjNxPGHn4mD9w8jFCypm/7N8YWBhZGH8/5eRgf3PX1aG/7//MP948g1iHRMjI9P//4x/QfUtw39Qqfsf5AJWcCPj3/+/TOBo+QeKULCnWEHeZABFMZjBwMCg9BvUlv3//z8oZEBNn3+g6GBkZGL4xwDyBjPLHzbGf6Dm7n9GJmYWNiYGFkYwYGL4+fUHO+O//ywsjIwM/77+ZP3z+/83Fsb/bOx//3HxCzA8/8r499+vrxxfv4M88u838/+vnH+YmP8x/f/PysT88+8fJtY/3/+x/AOlKiZGFsZ/oDYlKDf8Z2T4B0lk4HT0n+H//5+g6hPkqL//GUAOZIDKI8UIzxdQfmJkYmD4z8gIqgp/gpwJarP+YWBgYmZmZmZh/PsfVMQyM4KqQAYmBlYWZlB9yPzrPSsP0x8WVpb/fxj+MTH9+fmf9c8fLmYWXvYfv7lfsPL9+PeP8Rfnt7cfxf/8ZWZkYmBmYmQF5Q+GX4yMTOw//zIwMX7/ywhKCIxMTKz/GEFlCKjoBSUFiGtByfk/4z9GJra/oNrmP8iroMzy/zcr2Cf/kfKIOMP//0yMzKC4YPjPwMjM9B/kCVCXiZmRmYmR4d9PUFORhY2ZkZWFgZH5PyMrAwsbEzMLKwvj/58Mf5gZGP79+c/w/98/UJuYne0fBxMrJ/f3v39Z/wmxvGFmZOf/xcr5hpmJgYnpDyPbX5a/TIz//jP9+v8f1Bz98+8POO0zMjCyMjIx/2MEpSBwewma20EpmOEfy38GZmZQFQpqoTEwMDH+ZFC7zgzKLcgeYZYAuxcUIf///fvN8J8bFCiMDP8Y2P/9/8vAyMD8m5mJifkfAzMzKMaYmFnAbUYOpp8MLHwM///wsDL8ZvgHCl1GUJnEyfv/PwMXM9M/UD4QFeBiYvjP9Yv1729Ghl9/GZgZmdj+/f3/7z/Tn/8MLH//Mv5jBqUoRqa/zAx/WZkYWX6DUhY4MkC5npGB4R8TrGHB/O//v/9//zIwgrKzd1/1VrAKpKT1h42J6T/z/3//ZJj+MzL++/fnIaiC/c8IYv8HtYuYGZiYGP4xM4EClZGZkZkNlHUY///6x/+PnYlJlOHvf/Z/P/8xsvxn4mJlYGf7+Y+ZkeHfv29ff335J/T/HwsD159/H1lY/v1nZPwDar4xgeL+1x9GkN9ZWLj/MvxnYGL+zfiX8Q+oEAFlDnBnCJTTIU5lgBRl/5lAifrr//+gdPjsjc/Ov6CaDilpsf9mYP3/l4mJ7cfff39BmU7q9+/f//79BVkEimBmZkZQB52VmfUnEzMjMzMrEyOomP/OKPxPiI2BieHP73/szDx8//78+/+fif3fp///f995/vHbj988TKKyzL9Ymf4zCn1nYPzDyPATVDCwMv5n/vmX+cdfJlB6YRNg+Puf6d9/pr8s//7+Y/jNzgjK1KCGFiMoVYF6eKDCmZGBGepDDlCAsDJ9+yYv/B4UIkgxwsACqkT+/2FiZWT4+/sHAzMfHyMT459/fx4yg6tHUK4B5U1WFnCTjIWF8Tcr698/3EKcrIxMf8F15R/mv3+YWFgZf//7w/zv98/PTL9+/Pjz/xsjNyszOxcDE/NXjj+//v3+z/+fgYOBiYHl/8ffbH+//mZm/sfEIsD2nYHtx8+fLH+ZQM2tfyxsoHL/H6gQ/sfAxARqGIMKIUj9BooNBiaGfyzMn97qS39k+fePCckjmj9+/fn95+fvb////Wfm4mf8++U7KyczByvTj19//oEyPtN/FiZmJlZ2VkYmUMOYmeU7KyOTAD/zLwZmUM+dnfnfNwZGhl//WBg4mf78/fOfg+3xT2au/4xsXAz/GL6Bgvo70y9mVnZGpl/s///+5WH6xPgPlNZZ/v1gZGFg4/39h4EFXPgzMDD8+PuXhY3xP/MvdkbGP6Aq7C+o7P33nxFUbDIyMzF9BzW9WBm+vOJROcXGwgzKsdB6pOovOwfDn+8/GXheMzCzMDKycPxlZP7/989/RgWmPz+///zF8IODlYWNhZWVnYn5PxPzHwYWJkYuPvb//9n/soCjjoXlLwPTP0bGv3+YmDmYmf7/+yP04+efv3wsn7gZ2P5x/fr9k+EnC/u/33/BSYj5DzMLM9//71+//f7PzvSH6f+v3wysTOzg8ujPH1ZGUDOBiZEDVI2BSmpQm4Ph37+/DCCfMLOxMv7+//8PC8e3Z/9dGAXEfy35CK/ZuX+yMvz+x83179+Pv6C2yP///KD2LAsDwx9WTj5mpv//r7Czs7KxsoAKlf9/mf/84/0kyPvnDysH22cWZmZG5n+/mLhYGH//Yfz/5wf7N2Y2VqZ/jN+/fWdnZRH+8o+FheEXI8dvxv+///8CNYQYGVgYQFU0ExvX108//v/n/PeLkRVU0vwH9bSY2P6Ain4GRkZQEwjUiPnHxggqEf/9/cnC8JeZiZn1P/Pvf0ycgiwMDE52DEzP976Be4SBk4HpHyhhMYj9/v3nN+OvTyLsTGwsjIysn9lZOdjZ2bh+/Gdk+/uPnYGZkYXhDyMjt9Rn1n/sHFxs/5lYOdgYGH///Mj499+/f8xs7H/Y/7NwMrP+ZGb6zynNwKBy6cevr6C+/58/f5lYORhAlTITO9tvBrZvvxj/sYn++/mW9ef//6z///1l/8P0/w/TL1ZwZQxq/4Ka3aDeF6hlz8TEyvGImZWFhZWZ4dMvThZGZj7uO5u+fXr57sMLAXDdD0pd3d//MPxnZmdkfgTKYX////zxloWTi4eNle0vAzMnOzcofzAxvX3NzPT/13+Gv3xS3P+/sLOw/PvC8EHo7+/f/xiZOP4z/vj+h5GF9T8rNyvTn///vjx/8ImT/x2bwldO0V/cwsyffn3+z8LN/efnz+8cHKBhV4Y/vxkZQcMOr3/+BDXx/oNSwz+Wv78ZvzH9BzWGGH+CRmT/M7Kzv2HlYGNlZP//jYGFgYWVkeE90+/vf8WZ/z77n3L0BihNw2OE5+/ffwxsnEwMMv85f73//p+V++ePbz8+sHGLCnDzcXEy/mD6ycH77x83y9+/TxhYBflYGZj4mf58//jy/XfuL0wMzKycwgp//v9mYGH4x/DvLxfrd9CIJBvLVwbWP3fEPgixM/xh5HnH+5+V7efP//95WP79ZfzLxv6Z4z8L5+8//4R+f/7KwsDM9Psv238GVkbmv28Z/7IwgRoijP//sfBws7yWZvnP8PfP9z8s//8wM/z785fv+3dGJkb2P2KfBbTeMv/4yQT3iPAvZiZ2HobPv3jZuZmkHr77xygOigphQbY3HKwMfzgEOL9w/vz4i5GJ740sAwfHP46/n1l/fvv85tN3UGOTgeHvj9cCTIy8vxgY///j4Pz/m/H7jw8vP3799+E7A8MfQUZOhl//2QVZ/337wML07c8vJk5uUMOHieH/XwYmzv9vOTmEf/5gZP73l4GFjfUfAwPznz8MoHz/7y8LNw/b/39if3//ZGBgY+R5z8747/e/3yzf/rEy/frDwsr0/7Xy9f/ffyEqRA6hv7///P37+9+PPwzcXFJ/f7K/Y+cTFfz28R3Xh4/snLycfwT+/WLgZOZgk2FkYWVg+v7xx5efbx+/+v2PWRjUgGBg/c8lxMXIysr2788/JgYW5h9///37/uPndyY+NkZxrn+MTKws/3///M75/xsT8y92FlYWht+/2EENDU5mUKOdgfHvzx9fhf6ysfz/95eB79c/UBnFzMjFw/z/P6h9wvKPgY2J4a8Y48+fDKxMDGzMP74yMnKwcQp8FvjBwsb9Hh4jLAzfQaUEK+O/vz+4/jKz8n4W4xT8+/Dbb+ZfzGysDF8/gZqPbMycTIy/GNhZ/75/8+kfw7f3f3j+MjN8Yf7L8PcnKwvDOzEBVgZmlj8Mv/8ws7Ez/H8t9+P3Ty4Whnff/rIz//3N9oeJ698PJvZf7Cy8bMx/f4EHJhgZOP9yiP7/++vPP85fjP/+gVpzv//+Z2Jj+sfMxfGc4/efP//+/Wf+z8TG/P/bt1+8v37/YWRg//vv918WXi72P4w/f7M5/n5yQxzmkW3//oJqelahb9+YuYU5/nF+4WNl+vbhNwsH6y92FnCz8yUnMxfnfwY2Bm7WXx9ef/jxn+XbL2ZWVqbfPKBWFSMby8ufb7jEeLk4RX/++feHkYmRk+fT/9+cEp9+Sonwcf/9y8bI/uvbX7b/rEwMLOws/z+8+/ZT/sdfZgZOTgaR379Z//75wwhqwjMw/GX+zcjN+I+NleGX2Pc/oP4u019Wxp/vvv9jYPr8D9RmZeH8ysbO9A/UDnPlf8kkpKG5DuYR1i/CP/4wsPOysH4U5vnx4vZnVsbfDEys7MwsLOz/WVn//2di+v2b+e8HRgEWTqbf7z8zsP799fXD139/fv76J/Cf5f/f/38YRX9+//CZX4KPm4OZ4Q/Df1Yutj9//3N9ZhD78O0HCxMry6/f3/5yc/7+x/CL4f23b++/fv3D8O8vMyMHBzsnKxsjM9uv33zf/jAxs4IGytn+///79zcLqA/84yfDL5YfH74zMDKz/GEBtRi5OFnYWRn+/v3HKKYjvuPFd0Y1Lx6YR9g4fvDy/f37k0WUn/HH8xf/+Rh/cjMx/WNgZmf5x/jvHzsjJ9+fvwzfmXlYvvCwfv3+++ePHyz//vz78fkv07+3DKxMP/6wcDIx8DD++fhHkpOXnZ2D/e/v/7L/PnzjYP7N++Mvwz8O7h+sP1n5OP/8+/6O6+PLL1+/MzAwP2ZiApX5nBzcAtz//nJw/uf8Cxpc+/uTg/HHHyZGJrb/DH/Y2b59/P7+DzMrIyOLwDcWBi4Olr8Mv1n+/gG5i1tz9wdedsZnF/1hHuFkYuBi/P/9H+cHrg+fnn1hYmEF2cEIamuz/2NmZuHkAA0SSjB//fX70/NP37/9/s/y+RXDrz+MbGz/P/39y/br7z+mHywsbHwMP3/8ZP3Hyc7Awfibk+/Xv398jIy8oDqAkYuZhZHp398/3799+PzpNyM3MzPDp/8MHCwsjAzPuL5ICvKzMIAGfX79/vWf4SsLC8MfJoYfzD++fvnLys33/ee/f6xsAkI/GBhYmRk4/3K+Y2b+95+H32nHA/Z/nD9YGNVhHuH5zvKF+S+zxM/7nN/ffWVlYWVlAvV7WJiY/jFycf5j4WJm+iHI9f3bvy8f/n748vXbr5+MUjxvfn79x/DxFy8rwycGJlbWv8zffzNx8LB9Zvv7n/kPO8f/3ywMP7/+ZPonwfKD4+9vTma+/79+cfz58eXP579sv/8w/WPk/Pfv69+//xmVfjD++87FzsP78+9fFhZW0MAwK8uv79++Mn39+fff7++sMt++/2Zl4WNj+f73FyMLF/MPod8/QMN2x42EJd5zPTV5C88jn7//+M/CLvzvwc/fP/9wsDExMzL9Y2RhYWFm/s3KwcrCycTKzMn+5df/tx+/Mf9l+P3j+zdQQ+P3n7///jN+AIUzAxuH+C9Wlm9/BRk5GP59/y/B+Pf3S4Zf33/8/MvG+pcT1G77x/SLjf3j54/f/7MwMjMy/fzN++fbr7+MfxlYX7BzfvnNz/JJleXn37+MTGy//zAw/Pjy5x/LN1C35e+ff8ziLL+//+Jg/fXnDysLN8dHpj+/mVlZmcX/iYp9kv6j/nb/W1iMfGT4z8rCz/7oJwPDHxY2BmYGRlDrjI2JjZGFhZmdC9QdZP7FyPz88befPJ+/ffz+58ffD6Cq4hcDxx9W1p/s/36zMX9jZWBl/Mog+IPp//+3P0V5mTnf/GPgFBH+9YWPj4Wb4TfrTw6GX58+/2H8x8TIxPr97zfQgPw/UKXw7ffPL7//cf18xsPN+/fPlx/8Pz9++vuPkYWBFTSo+ec/09fvbCxsHKJ8D9iYOP8zf/r09z9o9piDj+2IAt/Hn68uffgP88gvRsZ/zGxv3rP8/vmHneHff9BoHWiWjZHlHzMjJycDC6jj9/P+/Q9f/n798e3n9z9//n5nYWJnZP7xlf33NwbmfxxcbN///f7FwfibkfkfOwfrj0//eXgYODiZpNh+fuDm5gK1kJh+vX396etfjl8cDD8/ffr69ysTw2+Gv//Z/jD9+cfw9/cXBpaP7PzcHGzfv/1i5v7x9z/jHzbQID0D03/mP18YObn/c4n++fL1359vrAygcRI29j9cHA++ffj5j1XoM8wjfxkYOdh/P//x/98/5j+MDKwsLP/+MrEws7Kw/f/HzsHA9Jv5L9O7S48+/vr9799vUA+C6ffv/6BY/vftxp+//zmYQP3T/8yMLP///mRkYWdm//WTkYf7+y9mdoafDML8fP+Z2Zj+//767g8b85c/zEw//v1n5v37mek/FysLKOxY/7IwvPzPxMbC8IaFT0TwDRMDO/e/H3/+sP39w/iH+e+/n7yMfxi+3Hlk8Vf279cXfxiZGf4zcPCxMrII/X76j42BgZkV6pFNrH94ub5cBzUM/jMzgcb7f4N8wwyqCtm5/zEysP5heXnjyXcmNubff9kY//1mYmb4x8zCyv7zB4vc19+gCS0+1k9sTEzsDP8YXvL84vr/h4nlAxs79z/mv4zsoBEglv9/Gf/9ZuP89/Xn/9+//jP+Z/z1DdSM/fP9568/gt/Y/n37/4/pOScHN9PPH4xsDIz/WJmZv7P/Z/jD8O8/+wcWxt//f//6xXhdkEdKmO0DywvGn+y8nKBBiR+sfB/+/mHggHqEiZmRnfHjP2ZQl54FPCr6n5GTnZWFiZXzHysr57+vDM/Z7n3m+Pv3HxfPtd9/fv38xcDIyvD359d/P5ml/jJ8///5538GQcbv/34wM/3+85ft9xd2Bi62799ZeBnY/nP9/sfAysb0/88fFp6fvxlYf/z59vPr+x+/vvF/Z/j9++e//zzvePm5QQNvn76z/WT/849V8e/v338YuDl+/fnPzMz0/d2Xr2yMP/8xsPx9/oPhsbjM128SoPqeheXvn49v/jKzf/z+lxPqEcY/XBxfvzP+A43tMTKBejPMoEFEUMsUNLf57cPbn1/+/Pv6T0iV/ceVz38Y//5hYP3359+/P2yM/1+ysP/5z/zztyQzgyCouP7P8PMDGzvzr+8s334wMIEm5FlBywsYWP4xMvz++Pfntx9/vnz78evzrz/vmFj+/Gf6x/iNnePPh3//mZg5GX6CxpRYhfh4GX59+8fC/fUXI/P3X0ysbAz/WP/8+c/48bcgw++/Av9FGL6zMPz//f7NR4Y/LCzsoCFaUKeKgYGFmfnnb8Y/f0HDlr9Y2BhAMcT8/z8TJ+M/Jrbfvz59/v2T8S8Dx3/Oz69/az14zPj37+/P7KCZtP8/mRjY/v/+85+P6T0rOxMrGxPT39+Mn7kY/7Mxff/9+ys38y/uv2wsTKCxzV//fn769fP7j09/fzCxsoqy/LvL/u8nEyO70Cd+VtCQKds/1v+sf0BtrSdCwnx8XD9+8HL8+v7nDxNo2B7UnPzDyPSdhfvJRzWZl/9Y+Lh/v2P5wcjIwgQaY/gDjRFWzj8//oPn1ZlBQ/rMoEEEBsb/bMx/QI2a7++//fr7m5GZ4+f3H78E/jL8/fz775+/P/7+//2f7/8frp8sTL8YmZg4mVl5WEFzVD8Y2Fi4xRlYGP9+//6X+R8j32/WH8zsTH8Yvv/8B6rfGH/zfGbiZwQ3p1jYeVh/s//7BGqx/v7P8I+FiYWNi4n5xw8Gbg5uli+sHOyfuf4x/P7zj4mRmfnvH2amP3+ZP1xU5fr57zMrx38WJlA/joHj329mqEdYPoBGEUETzn+ZWVkZ///7zQIaCmb5w8bB+O/782+/GFj+/fnz/fP3X9+l3j/8/OfPdwYmUJeS8S/br/fMTKws3xmZxZlY/oEKj//srEzs/z6LcL/9/p9dgJ2F4RcP+18Oln9f//z8yMz499dvZlZ2zt8Mnz79EfjD8u/v3ze/BL//Z2MCDXl8ZWBkZfn2gYPz17dvkmIsjJx/frHx/v/H+vv/HwaGPwwcf38zMf37w/Tjrs3Hv0y//4AGLH6w/f3HwP0B1rF6yMjH8u83aLgbNCj+/y8DOJGx/GdhZ/r36+23v/9/fOF+9/Hnjx8/f355/4OB4cdPxr/sf/4xM3/7z/bn7z/QwO3vtxxcP36x/vsj8O8v47+/XxlYOH5z/vzLygpqtTIxfP/64/Pbvz9/f/vNxsT+58/3d18Z/v79y/SDgfPPt7////9kZPzHKPSHgfnn39/sv95wcfz581+UlZmF+RsL+09mjv/MoHk5FoZvLKzM/5lZmF8Lf/v55+d3JrbvbH8ZGP4xs/+Gxshn9l+gaaK//9jZQGNkoEm4PyzMrKBM9u/zV5af7z9843z388+v77///mNi//8bVGj8+fWXken/d1bOf/8YGf78Yf727SvHR2Z+NtBIP2hu96vI77/8THycnG9YmNn//vv3h5ntJwfXX9Z/nzl/geq5v3/Y/vxjZv/z/f/P///+/2FkYPryh4mDlYmLg+k7SPn/P8LsjEzcoOFBFs6/P/8zMzL+Y2djY/jHzMb2ip/3+7c///5zMP0EDb3/4YJ6ZCYrOzvLh6///jKzgMbAQKNg//785+JgZmRg/v7598d3n798//Hz53dQ0+rPf8b/LAKcv//+Yfj+j5GRieHrn79fOb4xsnAw/vjCxv5P5BvbbwZOdhYujl9sX37z/GdmEWBj/8PI9Of3b3ZOpr8fv39l/M764yf7t+8/QDO6H/6x/OH4+4+BmYmF+zMzwz92DhYe+Res/xnYfz38oPnvPyMb05/fbMygse2/f0DDW6DmPOO/d/K8b1nZ//7nYWL4y/z3xz92SIywgHpbLEx///9kYv33/y/LTxYORiZxLkZGjn8Xfr9k+PL9x3/Gb38ZOH7//v/n35+foNj8z8DMzvbjLwtoCJ+JEVSzf2f7x/D/8x9mKfav/xg5fjNz/v7++w8rKIH9+8P4/zdofPzvn68/v3zh/Pn3958vH7/8/M/8/+dPJra/35hAbSJ2Rt5f/5hZ//3lZzVm/vjh72/mT4wMjP/+/Pn9j+H/77//QUPmfxgYWBmYGDkYfn3mZGFkZfjHyMX8m+nn3z+weuTP35+sDMx//v1mZAat8GNhZfrDzfqXiZPh4YfPX779/cMEmnX79efPj2+/f/1g+POXkeEf669/oAry/z8mFo7PjBw/mX4ycIJq+A8iPEyM/7////2T8fdfZgZQqw1sNBtoAO7/z19fPv/++e3Xr+9/Gf5/5mL4xcDw6w/DX1B5/51B6D8jCxMr+1+GX0wcbF/+sf+/J8PJ8J2B/9P//0wMDEz/WVhYWDlZ/v3jZv3Nwv6N+TtoqoeZi4vpC/Nf0PQSAwMD29//4BUITCzMf1iYmP+zMjKycf1nYGF58ZLhx2/2v8x/vv9k/PL1NwMj8+9/jP9Bo1L/vjIwvv/37ycLyx++X+x///9m5WJkYmdi+c/w8fcf5r//OVgZfv5n/QuapmVm+fePgZ3pN+sfpj+fPr7+wvX9xy/wAPp/1t8//rL++8f1H7S0hpXlDWiIgoP9t9AbdsZvn5h+sb38K8fL/fUX/w9m0Ezav39szKxMLIx//v1m+vlF4PcPJuZvLCxs7D84GH79gCQt0FjoH2Y2RlDMMjH8ZmJg5mBn+cPK/PnNN1DT8e+vn7/+/mbiZPn56w8jK/PfPwzMTD+Zfv9h/s3IwcD4/xvnP2ZGnu9/ubgYvv35K8j8jeE/40+O////szCBxrz+/wNNFoFmexlYPr988enrr/9//jEw/P3/m/n9/1//Gf/9+wEa3WIGTY3++/Pt91+WF3x//n5n/PvzF+s7ZnlOzv9/uT+z/P/5/z/TXw52UJOc+S8jx58f7By/WXl+MXz7z8z4kwfajGdjZPv1l53tFwMrIwsj49+///9xcPxhYmP48OXfb/b/H37/Y2Rm/gOaavv/6zvnjz8M//99+83z9yc3+4/fjKzs/38wMP9l+P+Lk/nP799Mv38zMbEwc4ix//rH+IeTi5+blQ1UdDL9Z2L8/ffDqzd/mP8y/GMAJaJfP/8xszOw/WD+DVoXxszC8RfU7mZm+vvyLaiRx8747/Pvn8yKLJw/+Jg+Mvz9DZrVZGZhZmJi+fXvP8PPf8xMDJxsjOysnz795uKCxAjL//9szIysoHUazP/+/vnPwsrGwMzG8PPXv7//QTOcrOw/vnB/+/rz16+f33/9Bo36gULizzv2f7//szMy/2X+y8jI8fPvd0Zm0HQj639WNk6Wv3//Mf1l/w+aKQTNTn//8/Hb56t3v7DyM35kYmBkYmP4zvrtDyMzx5+ff1kZGBlAy73+MTGwMv799Z/1728Gph8czJz//377xynL9u879++/LMz//4PWFP1nZGZl/P6Tm+XTL0a2P6xcrAw/vrH/BMmA+r7/mUEOZvrzh5nt98+/HMzcoNniP3///fj9589vTtY/3z985Pj65cfPf/8Zvv/69fc/C+P/H0ws//6yM/35w/T3LxMDEyMfI7ii/f+Xie3Xv1+sf//9Y2f/w8j4D7QOgPH/vz9fvoBWc/AxcfwW/v/395eff//9AHn/72sGhr+gtXis7L/ZGRnYWf4zsn5lBjWXfzFzsv3/9ek+uzjbz78cv/6w/QKV0syM/xj+s/39w8LG/uvrZ/4/TIzff3Aw/f0LjpE1vxn/M7D8/gSab/32+y/TX1bOv6DFc78//QQtRuP4+fXVR7Z3/37/+vPzP8sfUAZmBXVJuH/9/M7C8uc/C8ufP8x/f/7+zfL79/9/DKBmLCsn03dOJlZOBlZWUOefifn/718MjCxMTL9+//nF9e/Hn08MzP8Z/vxnYPzJysL4nYmFlZ3xK/MvNuY/f1nY2X6BZpF+/2F5zcnHxfT1IScX2y92rt9/GLn/MTIx/fvP+JeZiYWVheMby59vzAw/v4JGvCCNRtCUEGhF7l8m1p//GRnZWcUYWTg4/7148o2B7Rcb6/sPP1nZvv35/wc0xPWf+Rcj499f//4x//0GWhXKClqoA1rC9Y3h32emn2x/fv5nYGXm4Of+9ZvrPzs7CzfPDyZG1t9MTN++sv/+x/7j37+/X5j+/Gf7zcDA8v8bP/N3lv+MPEz/WJj//GP9/+fffzbG30xcTEz//v0FrT/4xirAKcL8m/k/MxvH779/Wf//ZWZgBbUcOZh+//3HCkr2DEx/QYulwDECmk/9/R+0IAGS7ZgZmdiYmb5/+sv8/wPLyx+/f/7/x/6O6fdf0PTYT1ZQsP/7958VtGyPie8PaCkq8///v9i+ghLR7z8/fn9nZ2Fh5GD79vsfKzMr03+uf0z/GVn+szB9/sPCyMTyj+0HEwuoq/pD4MfXb79/gTrBIKP/MjHw/WPiYOH4D1qMxszOzszyj+Hvx1+iP39w/mX79eP7fwZmVnD1y/L7B/f/32zMTH/YfjOz/f/HzMDwhxHsEcafzP9//WdlZmL8Axo/Aa3sY2Nh/PmT+d+nb6D56f8/fvzl+fafhY3l/1+Gbz8YGEHrUH7+By2A+czK+fcnOxvr358//oIm+P8w/P32/z83C8vfv1z/GEBlN+t/JlB7n0Pwz3cGLo5PTP9+sP9jYmL58+fPO4b/P/4wMrL8+fWP+fd/Roa/7H85GP7/YWL8C6rQGf4wsjP9Zf7zno1Z5t8/Vp6P/1h/gVYW/GNkZGH6K8z97dNf1q/sAr/e/WH6x8AESVq/foOm7kArqv8zM/4DzYRygQRYf3/4yviX4d9fJiZmtu88v/4yMvD9YeTk/srEAJqUBPWYmQW+fPvLwsjJ/PXnn/+MLP9ABdDfv6CWAGj8jO0fIzOoP8rI8O8/2z8GZhZOFobvv35++cfGxcz1k/MXK8svZmZQWxk8Wcv058u/P1/ZWdiYhFhBK5wY/oKWFjIwffwryfr3Hxv/G8b/oDwCMpGJlenvX1AXlpHh579/oCXIkGb8P1aG/8z//7L8A8U4CxvTP5b/oPl6hk+/GX7/Y2Rj/sfG+IvhP/PXL7/YOX5ysLL9ALU+mP6C1nF9/8f589/f3xyCHG/+MP5nZGET5/nHxCIoycbE+o+DTZSXBzzFBpr1Z+T7xsTI+ucrI9NP1v9ffzMy/Bb785fxP6inw8LwB7SOmP0v00+m/+x/GdnYOdk5Gf7++f2f9R8T8/+fLyVZfv/n+fWO4S/jv3+sLKDIAy0qY/7PwvD7O/O/n/8Zf0Nav6AxrZ9MoOkW5t8MjKC68w8Tw/8/n38yMbL+5QAN13z78v/rf0Y2hj9//n1lZBX6/o2B6Scoh//5948d1CX6xc3/8wsT1x8GVk52hv8sDF//cbMwgEbgWZl//2di/Pnrz68vr95//HD/NTMDEwfDL6b/v5n+f2f5z/TzH+efP/9BlTYjOwPHb/C6DOYvP35w/uTiZfjy5y8zw2+W/08Y5Zi+/eP5+vf3PyZGhj/gDjXjX0ZG0DT2H6a/oKUdkBj5DXIT438GJvBYOAsTJ2jl1u/Pfxj/szEy/vv258+vX0x/mH+z/PvNzPrx99/v3zmZ2EH1w9//rIy/2JiY//7/zCDIx/YXVKD+Z2BkF2L78oeFgfUPJyMLaBiY4T8TExNoLouFgZP5H+NvdtAajr+MXD/+/PrCwAYaswbNFjKAumhMrEy/WZiYGf58//7pEzfHHybmP3/ZmH8+4xVi/MMk/Pc3qPfB/JeBHbwa6x/jH5b/f/+A5ughSwEX/GX4y8j0h4ERNEP5n5n5Dyg3sr7/+I+JneEHw+9fPxl//QFN6P1l5PrFwAlaHPCT9c8vToYfnP9ZmDm/sTD++/73zx+BP5+//fvNwPSfU4ib7RcDw18WZhZQKQtamczIABqLZOf7//fjuz/MP////fv/LzPjL4aff1hByYuJiZ3xH9P/7wzM7H/AK51A651Y/r2XZGdh+///9y+ufx/5WZj/cDCwMf75zczM9JeZleEfEzPTH3bWXwz//jD8+wepEL8zMv1lYwQtZWL6w8bCzSjBwMjJcIvhNwPHr18sv/4xsn/7/ofxNxPDj++Mv7/xMzH9Zvz95Q/ztz+/mP//Z2TkZvnFxvb733eWf9++gBpsTDz8HCysLMysv1kZGf/9ZmQCrT1nBC1N5WZmZWe4//PjH2aWfyycnCz/WP+yMP3i+vsd1N5m+fX3LwsjqJhjA3X8mTn+/2L/LsjB/O/33x+/WH98ZWdmZOL+ywCaQvrHxMHK8ouJ9Q/zv7/f//z7++83aN8HaOEH+39QomIArVxjZGYAOY/h14/vjIy///z79QtcJv/4wvmT4efPb3///foBanMzM3//zQAaEfjH9J+T8cc/5i9/WdnZGBgYODjZuJn//WQBr/tn+gcaRwatH/zPwMTEwcLMysb45+VPhj9MHOyc///ycrH8+fyHkZmHCdSdYGL5z/GHlY0NtJrq379f/5lYmP6+Y+Ph5GBk/McmzPWDgZnxG8hU0JLQ/4z/QePToA4XaCgONO8FqkdAy41BUQtKPkygmuvv//+/GVl//WFi+cnE+oed+df7Lz8+//n39/dv0Krp33//Mf9nBA+/MjCz/vr/h+Xvd8b/bAz/2Vh/MXLwcLGA2k5Mvzk4QCstGBgY/4LboEz/f7Ew/WTkkvjEyvLn3z9mVpZ/H7k5PnxlY/rH/fM3C/tPxv+s/1jZ2Dj+Mv5n+Q1a7sLC/O//n88/uPjYfn56J8T9/d8fUIoFL7njYPr1998fpn8sb34x/vjNwPyTAdzW+v2b+R8Twx9QL+wPK2hZF2hpLNsvUPHK/oON8evbTz///Pv389efPwwsDH+ZmJkZ/vz5y/P3BxPTX5G/DN9Y/rD//S/N9PMTAyPLP85/oPL2PxsP0w9QR5ThP2hVKAMTy///zJx/QP22X/xv/jH9+fnlv/jPfz/+fGRkYvj38wsDx1/QMAoL499fjMwcoMXE7P+Z2f79YWT49+P3T2GWbx+/87H+ZuD6BdpFAZ5K+/vnP8NPRnaGf39+/2H7+//7L1CM/OJgYAItQ/v3n+Hfb5Bp/0C7Rv4yMv1mZf7z+cO7z6AJ4l+//oEq1r9soMU/jCzMP/l5Pvz9/4mDUeTfj98sP38zf/3H9JdbhZ+ZhfEHC78gKwsXK8Ov3///gdYsglZ0MzExg0byGdkEmH4y/v7N+PsXA8u3Twy8DIz/fzJxcf1kYGT+C9odxMTKxPiLjZX1H/v//38YWf6yMP1+K83w/RMXExcDG8e//+CRaSZQ8mb4x/T779+/P1j+/2b8B67Z//xjBA0+sP4GFdugxXKgVTmgThzb348/Xv9kYP73C7RY8y/bv/9MrD+Z/oGWoDF9/8HF/QO8wvUrC+d3LnbwglEBXl6WXz+ZmX5/+8fKw8DOz8nynxm8YPPP3z8/fv9mZvj/n0WAgYn57z/Gv1/4+T8xghqRv/mZeP4xsv9n/vv3DxcXw09mRkZ2tj+Mf5iYuUArZf79//9eiO3bT46/nF/Zf4PG0hhBTXkGUHudBdTh///nD2hLCgMDw//v3IygxQugUWLQ6jlW0Ko/0AzYt/e//rP8/Pv/PxsTy0sO1n9Mf9j+MbJy/mRiYP3PwMzIL8n0nfkDG+/vX0xc4OFiJhFuNpYfPzn+/mdnZf7NxPDv/z9W0Bo40Gq5/yx/WX78+vbz090f7AwczExsf15/YPj7+xfLP6bPLD9//GVkZmP7w831k5XjPxMoATP+B23NYGT785OR4SsLH/NPln8cTGygReugde3/mFiYfjMygZbKMTP8+A3qVjMwVP7/zvWTleXvvz//mViZ//9mZWZi+/efkZnp68ef/5i4fzBwcP3/+0X8698//xh+M7Gx/gcNYf9l//+flY2R9YeQMONPli8Mfzj/M/1jF+Rh+8/KzAzqjzKD1vX8/c0GKl+YQIuiGf4zs/xmYPzLxfD5/192Ds5v7/79/QOqlv6BVpayMzOxMbIyfGNg+c0GWicHGjz/9Rc0qMIMakW+F+D/z/SX+xfrf0ZmZkZw14EJ1EoBZdlfvxj+gBfp/mPg/svy78e/P/9YObmYGbj4mLkZ/v5j4vzy4c//v3/+8XJ//fPjN/uvvz8ZWNhBA0z/2L/9//vrP8vvz59/cTOzfuPh5b/3mYmF9S8rF+f/v0ygPh7DX9BUNOO/P6x/Qf3nP6AtlaygziYTEyOL6JPvjGzMPIwfGH/84mEGLahjYGDnZvz1h+MfGwP7n99/RBh/MYOmx0Ej1N84OUAtkH+v+PgYf3N+/Mv65w8bK/P/f6A9E/8Y/rOAwv/PX5a/oDzy//dHBm6QXaD9H6CZdVbG/z8+/3/26Q8DG9vf//9/sP1g+ff9DwcH4/9vfxg4//9hlvz0g+8T+6+fHH+/8v36zMX0X/D7N56ff7jY2dmZGP4wMrCx/GVkZP/7k/03C6gxzcT4l4mZgfPHX2aW36zsrMJMX34w8/BJfvzHwMzL+h3Ukv/79/v/3/8ZWJh///zLxvyegY2FiZWRhesfaMzmNysnAxergBjrT/ZPf5lAS/nAUw+gSo/lBxMTaADjK2iVLgNoQcfff4x//jOAZlCZQEus/zIyc739yczG+PsX499/f5nZv/4ErZL69fP//09sjFwsPxg4GET+MP9i4vj3l4X5J4O0kvCNb8z/mIS4mEHDaH+ZGZlZQZ21/8wM/34xg3ZrMDD++8/F8I8DFPaf+Ln+s7GK8X9+y/n6EzPnv9//mJj+fWdgZvzByvSdjYud8TfjP2bmn/+YQXN7//4y/f/P8f/vh088LL84vjL9Z2di+QEquf78//eH4SPD7//M/5hYf4DWTDP8Z2L+zg5aYcjIBtqp+o+R+Tcj47+f7Kw//zExgZay/WEV5H4FamT+/8XI+4Xzz7ff/3/+EAT1oP78Z1H/z6Aq9JhJ4C/LPz5ehn+szOz//rOycTAz/2Jh/PP3LyvTH9COUFCX4R8jKyPDP24mUBP0/x8GdhXRb18/f2D4+48RhFn+MzP8ZxJlA7UKfv9n+P2f6T8bBxsLuGMCqrk+SjL+Z+D8DRoPAxVMoOBnYvr/kpEN1OH/Cx6g+/ef4e9v0BIQVtA0KGj5PKhkY2L/ChogZ2X8x8bw5/9/jp8/f//59ouZi/fXO8b/LBzM75kZ//9m+svwQvDXUUnOP+zsfGKCDAxM7Dw/fvFxM/9nAK2AZ2UB7Q77y8z6HzRgDxp2/cb565vAW2YGFtY/H4UYPjAy//4PKtb+gvYLsP7iYGH+/JORgR+0zpgBNDj/n5GL8T8LA2ht55d/jP8Y2X8xMoF2RTKA+vSgAQ1GRkbO/39+M/xlZAF58S/DN3YmZgYGUM8aZDUjaFAbNPf4i4HlJ2jh8ZfvDKBajJnj/7+PoG7pXwaO7wzcv/7/YwXthfzCLMXGwMsv8Oc3IysXMzM7ZJcDMxs3DxM7aDUP838mBmbQ+BI4Lf4EzVuw8zD9//yL9S0H59//3/+zsrEw/fn/m5fl4wfQJNMXdlC4MrL++vHrG58AJyMTA6gp/4MbNKrxH9Qf/Mv4HzQ2++fnn/8soI4TA2hghIHh3z8G5h//2ZlZGNnYQXuCWP+wsf74+YeZnYGB5S8D27cv3z79A7X3/7H8/PvvD+sfFoa/f3+ycvz8zszBwMbw+ysrN8u/n0wCgpzfQd0gBmZmkKMZQWNtjGygsZn/f1mZQJYw/Gf/8+3pV6YPf9l4eH8w/OHlfC/5joGZ7R8jH+MP0Gr1H0y/QW1+5h9/mDkZ/jIzgubufn4Tlvj9l/EPy79f3Iz/mFh//wXtqQB1Av79+/WHgRU0f/efmeUrqInyDVTkM7BzgDo0oNFzUKZh+MPICpoY/8L29w9oKS8DKLkz/Pj/n4GNmYGFhYmRke3ndwYODgb237+4eDh+szH8Z/zxj4WNhfk7GxsDIwdoF+JfJmZWlv9MoP7zP2aWP0zMLL9ABRMj1z8Ozs+f/vEyP3x1/wsrLzsb+//Pf0AJ4Q8rqFXIwfSdmeknAyt4tJzl3/fPnIKg6eN/f0BznEygHRQg9/7/y/jn3z8GVsb/oB1Rf/8zsDDE/P7PzswpwMPMysrJxgDq6jIyMnz/w/yPme3zl19ff/xh4GT9/f/zn98MrCL/v3/89xc0icLExMTG8Of7Xw4WDmZ2xt8sHNy//v5mZWBk/MPyh5kLVGX9Ae0PB00HMIFyMWjmgAE0ucLM/I+D6d+PH38Zf/8S/SoCXtj88+9vRjbQPqN/jPxsf38xsDH8YmX5w/AHVDgxcbJ+FGQHzaB9Y2AA9wj/Mf1i/wvaCwxq+4CaVqARQdZPoEYjE2hnBSMrK/N/BkYWbj5GFpanr//8+/MZtNXg4y8G9v+MIL+C2m8snP/+vWVgZmH4zfyLhf3nz/9CBhdY2Vm4ePiEvoHSFDMj0x+Wv38Yf35nYQM1wv+wgMre/6Cxh7//mBj/gHYdsXP+ev/tHzvrj+98SrKPH3///wO0rPn/f2ZQc5QJtIaQiQW03ZCJiYmF8T8n5z/Gf5x/2ATFfrL+BI2cgJZM/Gf5D9rx/+8PaGMSaEEg47/PLAyMbCysXODeFAsLE8tfdlDuYfzHzPQdtCrvFzc7aGb6918Gxt+/fv5l5hX9KP3zz7+foA11//6y8WkZcnxhZGZn4/gq9AM0BMjI/JeF8Tcby2/Gv2xMoLkD0PzSP0am///+/fn/+/e3P7/+M/37x/jvBzPbv7dfWHkFr/xkYWL5x/AbvD+IhY3xGwMraKEpAzPTv99///1nY/rDzPxKnIfp318NUC/4Iwszwx82Fsa//37+ZmL6AZqzZfrzl/HP198sDEysfxg4QNvX/oEyHjNoMPcvM9O3r78Y2f8xfGX9y/T7DyMr008GZlbWfwz/f7H/YuBg+/frEy8jI6+Qjho/1zeGDx///vv5koebiZn5Pwsb238mRlaOf79+s4FSMXhYhAG0nPfHn18/v3/8/o/l3++f77/ygNZ0MTz9wvbzF9NfUN5iYnnPxgVa/sT2jgU0VvGPie07Mws7w+9/zEwf2Dn+Mf0Dzewx/2UEbUH9///XX6Y/H34xMv378+P3p2+vPvwFrX8GbUFgZWVjBu3XYWYD9cP//f/4m/kvM8cftu//mJn+/WUWfgnaP/bnLxMjIysr46+fnEz8Imzv/3xm+ifA++ULE8N/RgEh0C40Vq5/oMKAlZGRnYkRtOWG4R+ommb+x8TO8Pfv718/vvzl+/r92/cPn5j+ybP8ZWPh+/vvJQMfIyvDfxZuVvZvLBzMzIKguej/v/8xcLKxgZY9/2X4+ZWD4d9/8EQZaD8EyMx/DIygwUnQKMWvH9/+MP4DrSNkY2Ni4WBjYWZlBM3l/WFk+v3x2y9Ghv9/mdlYWP+zsTP8/fFF/NtfDtDU50cmRtA+C07Ovw9//hF885r7JQO3GOv/d1+ZGZgYGZk5WJj+g6rjvyzMjOxsPKB1CaBJM1BHjfPvT8b/zKws3/98+fbtDaMw0302fr7vf3m+iLCBKg+Gn6z/f/0D7UQW+fUbtEAbNKXG9vs/C+Pf3wxfufmYQGPW/8Ajun//MP5jBvXq/zL++/P797cfoGF1FgZWZmZWBtCQMRsTA8NfTkYGJuYP334ygbaHMXB9ZWT+C9r4+vXXD3YOBhbGX5yszJ/+/P/xU/rT9/98HJz/P//7wczF9eoTaFsHaGKeBXQgB8M/0LZIBlBxAhoLAO1U/vvjNwPLj2+/uBmfv2X4w87IIvSJmZPt25v/vz7ysP34zc7C+u/nn98MoO3b70GtFdDCPYH/f9lBjcT//3//+Mf6k4X9N2iLJAMzaETnH2jm7jfT3z8/vn4Hrab7ywJaTMzKwsTMxgqacPzL9vc/65/PoO4hK9O/P78FvjD//P+H4S/j5z8/vzH8/fePg42B+x/Dv99vWdg5BPnkhP/x/vn25/fP7xysLIycTKxMHN/BoxL/QQ0L0NkB/8A7fBn+//3Hyf6D+Svj/z/MTP/ZXskK/wfF7OMvX3/8//uNgeHHbw6Bf1/+g8aqGBn+M//jAFUYf0FHd/xl4vjJBFnuw8TMAFo08J/lz79/oAL5PzNo/d0/JtBGwL8s5Wxs7Dw8nKCBFhbQcsa/TEzfvv/+D9pO9+/fP0aOv3/+//7H8ZP1358fjAx//v/9zsTK+JeJ4RsL++9vbJz/WH9+Z+EQ+v0OFBZMzAzszBz/WVhBO2/+gHaHMzL9/f+X8f9/JlB1zMTMJsD96wfvnx9MYp/fsLH8Z/rzA7RT5C8jG/NPZtB2BmYmxp9sbP9ZwC0K0M4+pr+gvbCcDIyMv0F9ZhbQLiVGBpCJDMw/GBlYfoN2w4I2LIFmm9hZmNjYmJnZmBn/Mwoy/GNjefL2OyOoRfP7N8N/Rnbm/z9+M37l+sH469+/nwxMP3+Buog//7Ox/PzLzfLzA/Pbb2z/foPWEDFwsIA2q7GChv7/g/MrAxPjX9B4OBMD4z9G5l9/QRsxOBl4/jEy/H/FzM/MwfJXmOUz6EiV//9Yfv/9zwYaogKNrTKwM/9hYfzzn/k/I9f//0ysoLbnn19MTH9YmP6CkhoLZAUeqLfG8pORlfU3KEpYmFmZWUDlHWi5Iwtoq/eXX6CO0P+//xmZQaMgv//+Yfz6i+HXDzaer2wMv9mZ/zL+Z+dgY+D9wiIm+vsV1/ffX57842ZlYOPm+Pnv+3/Qfp3/bP9Bx9j8/w8aZwRtemYG7Wph/gVagfL7GxMT539Wpv9/uIT+fmdg4mJ6x8r4n5EJVKywfmJjAe19ZP77598fTiZOZtAyQFDHgIONTVSB4w/Hx/+szH///f39i+Ev0x9G5r8MrKCNpP/+/GJm+gva5sPCDG71MzByMP9mY/797zcoi4DmRf7/+f/3LwvX928/QTOE377+/cvAw8T0l+UbM6Mcgww/zw8G3uesP99+EPrFycHD/BO0aug/Hygh/2NnAqUChv9M/0Bb7xhBvXAmZtDW6x8/f///95eL6zXL71/fXz5iFGf6CWrQMjH/Y2P9xwxagv/vP9OPP6CdrAx/2RhBhy0xMDJxMoNy9B9m0J4rRpb//34y/v33/9dPtv9/mX7/+vrr6zvGfyzszEysoEMnWEGR+g80qvXr798///6DBoZBM48Mv7/8Z//95/ev/7+Z//xn/cLyh5mdk+HJH24+XtG/v/99/PZJQOwTt5gIw0cG0K6a/z/ZGBlBLNB8FnhkE1Q9gTs5rP9BLRZ2RhbGb9xiH//++AjqUn1l4Gb89/s/N/svVuHvLH/+sbL///qfmZ2V5effP6Dl35xsTEz/Gf+y/v/N/Ps3CwuoEfqfiY3xG9Pfn0zs/1l//2X7ycTKzPz2FwsTCysz83/Q5hDQyRNMbP++/wJNbzIx/vrL8uvf7z+/vn3//YflH9O/v/8ZWFn/gsZgf/xhYXjIzCLEz/KLh//1X67nX/kYf4GCm+k/aDyD7R8D08+/rCysLMxMDH9Bo6sMoBj/wwjacMrB+vc/2+/fgqpff/xh4RL4zcryi53lN/uPD0zM/F/+cTD+//GH9yf3X1BLhvH/NzYmdtC0I8M/Jra/rD//gQZmGJn+/v37h+HvH9CI1H/QCTlcP1lYQItT2FhAC7xZGJlBvV8mlk8//oFGBP/+Z/j94yczy6+vP35//ff/119GZsY/7P8/fv3HwfDnF+M/rr+/GX//43zO9p/3/VdFRdB+639//zAzM/1iBPU+GBn/Qk7aYgbV+qDs/efPj69fQZN9rH+Y2L4yi/14+usvKxvHb1YOJg7G76xCbF/f/+Bk/fbzx3+mX6x/fzL8+8/F+OsP23cmHtD+SdDsE2gBGwNojyjT7z//QUuxf4MWdjIzgTajsP0H5XNQsQca/Wdl+sfx98ffP79+M4GmQH/+/MfE+I3x/1/QDor/DP85GJk43rFzsX3/8+83M4OiptyXD+zPf/5g/MnB/p/tPyP737+go5FACxRAy73/gTYU/WUGjWWC4huUif/8/ffrJ8OPP/9/sPxjFHjxj/Ep329WHk7wLhsGBoFfDEzMLN//M35j/PWP5f9fpm/MrMwMv5hYhEC7glmYmEH9KlCAgZYngHgMDMxMDGz/WH+B99yycLCwcoJmJEB7u5kZmX//+PXz9y9mkG/+M/37wcLwCzRe+JeZ9Tfjzz8MfxU+/f72k+k3iwAzF8Pbjz//M/z8y872+/svdtCwBSvzfzZQTw80twjaQ/ifGTQLBjqUgIHxH+ioEdAE8x+2v785mP4xcss8+vL3DyfHHwYOzv8Mn/9+/v6Tn+ErC/+3v99YQOU0M/NvJhZmpj8/QdOIjAwsoCWKjP//gUZ6QIt0GFl+MrCwMf8EnX3ACDmdBTwsCNr4y83GxPzt+asPP3/8ZfrD8IeJ+ccfhi8sv36C5pv//vrLzPiH4Tvbvz/MnCxc7G9E/n9+/Ivpz28u9n+MvMIcoOqCHZLJmVlBNRR4Kzmoev7DzPgPhBkZWXn+/2T+9wd87gjDPyaRP4//vxFmZ/jM8J2NU+AP8zde5p+gE4tY//3+wQxyJDNovykLA8M3VnZm0CJSRmYGRvBOVwbQUQigmgJ0kgYjMyMjM/uPfyyMoLAELRhjBp0M8vP7n18/Qaswfn/9/ff3r7/cfz//ZGTnZH/GxfX3N7/wxzec5r8/83D/5OJiYn33l5Ph7xsGNg4eVvCKVGbQ+QSgSpzx3z8GUCf0HyhaQBtUQTtRQKfQsIEmiX/9YmH+x/LrD6fIj/df3n9lkGL6++MHMzOb2O+/vH/+sDD/5f0BmuoCDYL+Y2Nn4mRhZWHjEpOU+v2f4+8/UBMQtFrsL8MfRtCeRdAxG///gWY6WJjARxcxMv1nBo2Hf/n++Sdoscnf/0z//zKysr8TkvrIygCat2RiZfnxjkGa/xUj7/9fH7n5GP7/Yvr1m5H77U/QZkIm0NJPZgbQxsr//5lBx5X8BR+vAKrhQY0hRqZ/TH/+/mcCrR5iYvzP9f8nM6Moy/dfoP067DzMzCzfn8p/ZmZm+cYk+OYPM+sfkI9Z/oG2//9h+vWZU5jlPwNo7BVUhTP8//Pjz7//jL/YmRh///75heE/aGUlaKsLIwsoH/1nZ/7F/PPL588/GEATYcycvxn+///N95dV6ON35v88v/4yvfvw/8+7+7zC33/8/cL9//83FuYfnKK3vijJC3CwsDOwg84fYPjPysYEOl3hP2j0B7R6jwE0msbICFpDwgxaKMzMyvqf6fcf9n//mVn4FTjOfWR8w8TGwcXHyvD4N9NnTg5GZrafoDMHOZiYPzGzgXZY/v//+5MEKxNoSSGoBGT8y8DMDBqCgBwl9vffj+8/Pn39940FtNWd+T/DXyb2v0x/Pn36+gs0d8zACJom/fOf9d/vf1xc/9+8Ev7yj/cfMzPjbxGB/yxsbxm+Mn38xyUmzMHJwMjBxc7EyghK1X8ZGVhANQcz6HAhUB8XtAoAtGMAtCABtFT8z1/QKUG//zH/ZmD99e/LT27ZXxfeMbH+/fXvCzfTd2bGvz/+/mLj/vOfgZnj339mLlABDtq8zcD4RZT533/QThZQFfEP1A37x8AGOkfgLxPjP9C637+sLKDTKBhYmP9wMv5le/Xh8/dff0DnlTD8YmBkYgWdIfCPgVmI+dUnTg6W7z/+/mHm4mZg+vWPQfzHP35REVYWBh4ufl5WkJGgDsT//0yglf+gg5lAo1Kg8z9ArUBG0KDN388M/5l/M/1iZmD+wwBe88L2/ddvyf9X34OSCfNvZhZGVr63nzn+83KDeib/GJlBMyUM//7//cXE/OOLOOM/tu+gZi/oUBbWP6COIvOv/6D9Hv9BKer/fxZm0DwbCxMjJwPb31cfvn/78fcv6JAjVpZ/LEzf/7P+//aX6S8vz53/nxj+//vNxcL5/+tbFl7Wj59+CfMyfuP88obl929QP+rPfxbmX/8ZGVlADU/QUjbQGCZo5vX/PxZQhDOAFm3+YPj1k/Xvr79/GNmYvnAx8Hz6xSjNfvjfH1CdxvSNlZuBn4nhx3smbk7WP7+YGTj/g8pvSJn39Q8TaJ8rE8MfJkZQrLCCDn5hAg16MoEOemFkYGZhYwMNuP1jYWJgefn529ffv3/8Y+AAc//8ZPr7m4n17x+Gb3ySr95ysApxsPzj/f3lLwMD4282do4//zi4vkoICDIzMvxlYPv/+S9oEwwH029G0LFITKz/WNhBnct/TKAlOAz/GZlB/bJ/f5iYWEETr385///5wcTy8/+vP5wc7FwcnL8/M7yR4OD4xfqXA7RSiRe0VJgFtKqG/RfD338/fzCCxj1Bh1cwgLpJzKB+BjuoTmBi+QueomT5z8jJxszAycz67/KLl98/f//NyMLwg/MfaED+NxPHr3/MTCxfvnMJ/fzzk1WZ78sn1m8c3KxszL/YWNm4/rP++MnF/Z+J+e9f0GjaT0ZWZqZf7Ix//jP+/cfxn5kZdKgUIyOorcj47xcLI+hQuH+gQXoWTrY/Xxj/M3H+/fvnm4DQF1A7nJOZm1n6x2M2pr+sX/+ys4IOwWBgZf7HyMTyHxQ+oFURf1n+MP9nAh1FwsT4j+kvy9//oErzPyO4lAQtqQMde8HMxPri45efP38wcv/48Z/5L2hPKBPbzx////3884WB8+3HL2y8gowfPgoxfpPh+P/ti8DPn/wcjEzvv394LcXFx/iP+c9/ln+/QVNMoCllUGcVtEgJNC7O+IeZ6R8zM9MfRkbwqoXffxhZ/rHzsrxl+M8J2q/yVf3GJ7Z/bK9+iTC++/vvJyvHdybGvz+Y2RgYQKM3TMyMnOzsbDxSGgx/mH6AOi6gagHUu2JiAu1+YvnFwAgaVAGdu/L3P+iUAsbfH779YPjFwAwa8gFtwWb+x/zzO9u/3wzMQv9+fGQSEfj58/XLD+r/RH68+8bwReQ7nxgT8/sPX7+yMPwA7UUBNZ1ZQQeS/WcBraz/z/wfvLQNtP/h3z9Wxj+sPxmZGJlYQO1RFoY/73gYeP///cXGyPT7D7/c7Z9MX/79+8zIwcj6n5GJ6w8TqJ/KDpohAVW0DMxsoPWubH9ZQLsvQCN9v3/9ZQD3MECLB0BDv6AJXdAxRowMzP9Yvn36/u8v62/QuV0MDL+Z/vwEHV31m5WVg/HTNzZWLrYv779//Mj4SPTz8+9/f7EyCMuw/3v29A0jr4owM+Nf0HE4f0F5AFSJ/AedkAM+egd0GCto8Tn40KH/DP9/ff/2g/U/aF3HX5Z/bL+Y2FhZWZ+ySrLcfP9ZUOCHoNDP/7+YOf9ws/xlBJVL31n/sLAwsTH8+8fO+us7H+j8ECbIljNwTcXA9P/3j2/ff//8+PHLzx9nWFg5mP+zMDP9f/cJtNQBtKbhH6iU+QVKi8wMvxl//frGycb87+P39184+T7/Zfj6mpHh54/fzG+ZGN9+/gsamQGtNGX584+ZAbQ0kJnhD2jwiYGRFTSeBG7kgfrZzL8Y/vz++fMvM/t3ht9soCncr6CTZDi4WL+w/2D/f/7bdy5+ztdfBHh+fOdkZP33+R8zKy/rX8a/zP//soLr1B+MoF2F/0EDM6DzF0FeAR3BA9qYDJq2/8/AwsgGakKwfnn5i+nvn1+gE5RAy9H+gCrkX395/zAysQr9/Mry6d9/Rl6edzzfHouzfQXNrQmx3WP/+uXfX5b///78YfrzkwN0TtAf8BlNf0HnvzCDj0gDDTb+ZmZl+P/v7/+/f0Dn1v1h/PH/DxPv3z+gCpyDhfW/yM9vXxXZL/34yPXp38/X/IKgJXF/2X7+/AsqftlAzUPQ6XFMf/4zgw5rAy2WZQCN+P5jAi2Y/A06go0B1G4EVcXMoN1QH76A1iWDxov+/fkFOi4IdKDT349snEz/f/1l+c/2k5P/84cff/+wPP/F+v+vAD/706c8LEwMDOw/vrKDtraBlnuDtlqD8uN/UCiAp3dAA0gM//8x///7+8/7v7///Pv+8x/TX2b2f/8YOf/9Zmb884uFjYGV66264JU3X8VYGD9/YeRk+QXas8fIwPCF/S8jOysbaI8C098/f0DpE5I1QIuAmBj+gaz8958ZVFb+Y2QAbfVhZWD68+nL5z/g1d+giUnQYoi/zL+Zeb+xM/78z8L6jfHHt5/M336y/fr39/c/RhYRQZ7P93+DpndBp8myc7KBMiUzaPIOnJBBWy2ZQYOC/0CDUYyMf1n+M7L8/v0XtCADtIyK7S8LO/OPf+wMDH9Apfjv90yM4mwnGH9y8TP8+/Wfh/n3rz8crN9/MP1lYfjz8z/vf2ZmZua/vzkZ/4MGJxjA5QcjqIv4/xeoe8LKyMDM/JuBhf0/DwPjp6svvn3/8/vPH4ZfoJPkWP4xsPz/zsjBzv7jOwvD1++/WD99Z+X8/fvf35+guScugf8/nn9m/sfG/Ps/KwcnDzsr01/Wv6D5xN9sbKD9cv+ZmP78Zf3HCFpPwsQIGnL99w+k9w9o6TWoqOJg+/cXsnSOhZv5Pyvz2z/MjDxCvJ9//GBmBe3/+POHgeMnMwsrA+P/v39Ah3Ow//3BycjICFrG9gd00Bfo7BbQKC0zaM8R6Cw2Bpb/rAzM/z9+Zfz1/e8v5v9ff///9es/I6gRwcfz6zUzE8OfX19//WP7x8394RfLFxbQIAW3GMeP5+8ZePlZ2f/+EhDiYWXhYPwF6ouABiD/s4Hy/B9mRlY20IpoFlCmY/jB9A/UjmRk/svE/o+BGTRkBoqhf/+ZGZi4/jLx8wu/5JL9+J2T88+/H9/ZfoKqO+7/DGzc/1iYODg5+dhZOIX+/wIFCGRYhJEZtJIbPGHKyMjECBr/YQKtBGVk+vnp7+/foFEgBuZfv0AL5P8xsrP//fLtM8svNp6f335zcvF/ePH33w/QZiAGfnE+pjffeJmEeDiZOT+yc7ODNo4xM7Aw/WP+D1pQAdpC8ZuR899fFsY/jMygtXnMv/7++wcq7/+x/PsH6qEz/fnNzgxKIkyMP7gYv/3lFeN+rnD1JxMjz/9/v379+sHJ9fc3Axvjz/9//7GwMf/5w87IJMgH6raCtuD++/vvDzOoxgOV8KC11P9B+w1BB9iw//j6+ee3X7/+MP1n/QnaI8vwhxk0e/zlF9v3v6xffjMyM3z7/p31Hwsf7wdJ/u/sbP++/xQXZGZlZ/4nLCMKyumgA77+MYGO8GIHlTPMjCx/f7KBjjBjBJ2XxcTE+At03C4zw/8/IH+BDpj6/YvrB2jGkYH5Mwf7r7d87BLvNe6+Zv//l/8FIwcTw0+23784QQe8Mfxg5vrByM7498dP7t+svxmYQQe+/QPVm39AA5m/fv38+fXr98/fTjKwsLAy/Hz7+eevH79+snCBsjLDP1Ymzv9fPv/68f8bO8fPT8yMTD9+/BdiZGdk5GIV4GFjYf7NJcHB+f0fCx8DFwsTJycjwy9W0CF8zGxMzEy/QSOyzD/ZmZhBc+ugJej//jD//MbMxcDE8J8FVJ6Bmkgsf35xcPz6z8j65z9oBvU7AzejBBvLrfeM//jFfoDqASa2fz9Bhy2xcLB+Z2H8+fvX9+9MrL/ANTlosuk/818Gpl9/wCvHQOeGsjIwsDCxff/69cf3X6AzD35/Ba2LZmRk/v6d8ddP0DHFn3/+AJ1bwczz7x8DowCj4J8PvDz/2EGDnMysXPx//nMw//sBWl/FCeofgg7SBA09//4FOov5LwuokmEHdeYYWf7+As20MzGDtpMyg9YmMXznYPsFWmb9i5GL9fuXv8xcDByij99//s7FwfANdKgDz0fQ0Zp/Gf6yfvsn8OsH698/rH//giaomJjZfjMwgQOAGXRAGCMTE+sPBtDKpd8Mv379/AtKeuBzUll/f2P++ec/aKME01fQCC7H/5/c/xhYuDkYQYf4cjCATi1jAO1j/c7MzPj/JxvTX+Z/v0AD5aAS9z8T459ff1l/Mf9h/AkaF//FCFrK9pPl+4c/oJ1Vf0HtOtDc09/fjKwsP5mZf////Y2T58f3/3ws7BwsbCwfv7PycP389eMfCzNobQ7DVxZW0PYSBrbfv1lZf4GWe4D6iqCF1qBT1/4xsjCy/GcAra9lYf73+fPXnz9//fv//weoxfnr72/QyTcMzH+/fP/9F3ReKRMf23thfhaWf/9ZvnCxsoMGQdl//WNmY2Di+vMb1NVhAi3U+c/0H9Qo/f+H8TfoEPO/LP9BJT5omJTxH2jvD2hdB2itEGgFNyfDb2Y2xv//WX7/Z2L48+crL9e/b99ZGf8x8fxne/jjDxsb6KhA/p//OdgY/v77ysnF8IXj6zfQxpq/oNUQzH8YWEFDAkwsv0HH7jOz/wQtX2T5d/3pl2efv4P2s/39x/T7N6iR9YeR+fe3X7/Y/7D++88nLcT/SJGPgxHUS2IELX5k+sP07z83MzMb01/QIXqgxi1o8SdoJQzjX8a//3/9YGVnBR30+Z/hPwsoNYDOUwUtAWIA7dj6y8zAxviX4xfzfwaG36AzJxkYuFi//frxl4n5549fP3+y/P7zg5WNg/EHEw8z04//zAw//jH8Z/n184PEP1CWYgCN7/9jAm2aYWBiYf/7h5EVdAobAwPLp09ff/79+xc07fDv//c/f34zMoN9wy3GwH+ZWUBRgoeZiZ0ddDoRw1/QogXQBlmub6wcf1hAGz5Ap1mCwuMPA6j8A800gQ7uZfrNwwzqm4OOhGX4+4/5/3fwom3QfPFfln//WJkhZ0ox/gcNOjIy/AStn/zAzQmaZfvJ9v8f428Wll/sHMygKeifrCwsP5nY//z9+eMPG+h4hv/Mf0DDWKB9JgyMrEygZYfMoI3HLG+//v715+///39//Gb7/ef3fxZOXq7bQvx8nJwC3CrCbKC5JtCKQNCyRCaGPwx/QAeLsHCDhqhAI6PMf0FjoaDk8v8Xy39G0N7XH6Da4h8zeGETaMbt31+G33///vn59/+vv6DTqf8xMIO6lL///v3P/OM3KKL/Mv1mZvn37f/vPwzsHCx/GJh+/2bl/s74h4GBmQW0to6Li4uFnUOc5d/Pf6z///0CLQlh+gs654rhN6iTwsrKCtrnBNpz//fvz79/f31h/8XIwcfPx8nCx87MwsbHySALPuHyPwMnaE4BdB7sR9C6SNCCGoa/rP+Y/4JW2PxjZPwLuvbiHyPr/9//mP5+/87M+Ofvv3/MoLKRgYn57x9GkDe+MzD/BZ3cyfiTiZ3h9x9Qe5zpNyPzrx/s//8wMv75xsD57Tvjvz9/mbj//wWNzzFzMv5n/f+Hme3Lb+Zfv38y//75QeYny2/QWYF//jGBEur/339+//754w/Dby7Q2naWr7/+/Pn/5/+fb/9E3vCJCvJwMINijoWDi4MRPLwKWgIKamAwgg47/P0PtMLrL2jNNxMDEyvDn/+gkQZWFtCZKlyg0QXGP7/B6x1AyzL+M/wGHXL4nxl0WuaP/xygRYd/Gf6x/PnOysjC8JOB9e8fNo4voH2doLM/f3Oxf/3H8es3F9u37/////8swMgAWoL5iwvUT/vz5xfzKxHW3yygUViWXwzMf5lYf7GAApCZiYn968/FDAwsv0ATdr++/+QTF3kozMfCwsQMuliAi52Z/T+oVw86Avs/qKkJ6hp/4/77n/n/d2aO76wMzAzM7KA1Y6CTLkAp6D+osf/n+x9m5v/MfCKcoNYJaDPdf2bQMZygJZ6MzP/+/GP4x/zv+x9mvl//fjMy/PsDOvDqNxPjt38c/z9yMXxjYGLj/cAs9PXzn78sH9mZf/9hZvjNxg06s5nhz+/P76T+sPz+z8j0F9Tw+cfyh5mZ5RcjaMCBBbS7gOXXz5+/fn1nkxPnYedmYwKdr8P6n5Gdi/Ef2x9G0FIbhl8s/1lYGP8zgtrkbH8ZmH8ysjD+ZP3P8o8BtCHrDzsbyL3MLEyMvxl//WRh+s/OCmpuMzKw/AO1MBn//vj1+8eXj1/YmEFr7UEbjX7/Zfj2j+0v0y+Gf7/+sYOmLFl/f+Jh/vbl//+vv9lAJ4rxfvnJ9PMPFwvnv6+Mv7jYWP/9/P2b9ds7MXCh+Z+RGbRF5B8LaBoY1PBlYQadwszy4/Pvb8wKIgI8bP+Z2dhYQa20j/xcoLVKoDoENEMA6seAToNgBG11Z2Ji4mBg/AUqiv6BygxWZgZGFtZf/xj//+H49Z/hF+Pf/4ygoSqWf6BRE5Zf//+x/P3F8Pf7j6+/2UCTSb9//2NgZvz56wcDK2iy4T/Lv3/MPxk5fr7n/f/n799/P/8IMXxm4vjz8zvHP9AhMqz/QcULKyvLD9Yfn75z/WZkYGIETZGAxjoYGRhZ/jCzsPxiYQcd7v1BRu4ZN8N/Nk5Opg9MrMx/mZgYeFlAXVTQTjVQ4QBahAEazWP6z8gK6o1wMDL+5QJNpv1n//+XhfkvExsraJIYdNwT20cG0EFlbAyMv1hAG7tA69oYGP7/YGH8+/f7L15Qexu0aZoRVNkzgia4/rGx/QYdpfn5J/uPvwy/QYdqfmPn+vJPiOXTG1bm39+YOBkY/jD9ZOZi/8fw5/MHbpAfWEH9P1DbGbREn/nfH2Y28LH/LPoSrDz/fnCAVr4zgiZOQJ77y/CPmRlUnoM2BzCBWpGg3WZ/QcUvaJsqwycuRgbGv//Y/v79z8DEyskCmsn/+4WJ85/Qb9DZmf//sjP8Ac2t/we1tP+ysP76zcTA9OMfH2iD/u8/P5mZ/vz/AyrjGBl+sjOCDiNk4Pj/+zsjG/svUHOf8f/vf3///f/B+peNG7Ti6O8v7j+/OX4zfn8jwgI6lAu0AYwBVJKDdvMxMoDOiAE5Wgl0EDjo/gYm0EQgaBsR00/QUifQGYX/mVlBJyGDZplZGBjY/4N6TYz/mZh/gZz57y8LqEwHNT1Y/rAwgM6HBrVOf4Ga7394/jKCZ5B+gY46ANVBzAz/v/0S/f//x/c///8w/Gb4/5OV6d9fJuYvjEx/vzIxcfxiZvrEyM3I8Zvl71921s+szII/v/39/peD+x8LJzM7BxMTI7uAJPtfcG7/x/CPEXQOFCjdM7H8ZgZfBcHCCNo3ygq6FQMcGqATwkEdYtAYzn/2fwxMoGkU0BlQ4NYNaASGkZGBG9SlAG0v/gmaJGT89wc0osn8j5n5FxMDaO8GqFwGrdsAL+VkZf7HyfnhPyht/OViAS2m+M/wm5H57w82pt9//rD9+cvMxvznA+iEhm+/GdjZP3H9Zv3HwMz19j8zw++fDKw8zCycjKA53H+fPwiDp75+/fsPujLg31/QEjsm0BoLJlCMMIGOmAHtV2FgAi3HBk1jgLZ3gbby/mUB5WFQ4w6UxUC9O0h3k4EHtInp/+/fP0GrJplB3XwOBrZ/TCws/5iZQVfCMIG71KDdQqA9lsyMzN8/MzJ++//vG9O/P8z/f/xj+cnI8oPx039Ghj/vWX4zcP3+85X353/QcZl/Gf////eL48cv0MHWv5gZWH7yMDCysTCz/GP5/+vDV57/LKBDQBhBbVFQ55nh38/ff7/+bgZ55O9/8Dgr6HhjUL0HqgQZQfekgEpbUJUIGor+B14OzcgM6gWCNoGD8v3ff385WP4w/f/96x8r+y8uZtBsOGgvL2jKh5WDBdSEB53Zy8jG9JuJnZOLjefPbwaGv29B6weY2b89+vP/7z9mFnbmd1y/f377z8b87devfwy/mH/9///1LxvX359/uDn/gw6EYPzO/oWRn43hDwsj6/dPvKDD5UH7EBiYwRtDmJhZwBuoQR4BzSaBlqqC5ktBQ0eg8U6QMtA9VKBTnUAtKtA0PWhUlAF0yAEzaIbgP2gN4b8/3H9+/Wfm/M/MAj6SgIGZ8Q8raFMIC2gUFbS6j+UPK8s/JoYPfzlBjfr/TH/Z2RkYmVlZQOdLMIEafGxneJlYuJhY2e7xfP7D+vvPr+8Mv/+wf/vP+f8X418Oob9/mZjY2ViZQHsHGBj+f/nF8Y/5P9Nv0HnQDEzgBVSM4DFHkEdAa6VB+3ZBiQ100uZf0HQDaEE20z9QIxvUwQPFEmjBC2gDAiNoKf5f0O0XTIz/OX6zMLH+//8PdIggC9s/RpZvbAz/foGWc4HOKgftdmcBLdtg5WT6Cwq4/385pJlA61T+/VMH5bJ//xkZLUFLGRhBe5P/gRZf/TnOyvDjF9vfn7w///z5zsXKwMDFwscOXnfyn5Hhx3d20OH8oGlq0FIXBgYm0OkRoJ4kaHP8f1C0MYA2BIAG0/8yMoMmhUBLp8DjaaBSC7RsF5TS/oMSF/Mfxv8Mf1n+/gPtS2YGbUEFBTYjOwszCyNolBN04haolgOtWwZ1U0CFJQfDL9CpcX//Mv1mY2NmBd1xALqs499/UEEMOmadgZEftD/n319Gs3//Prz/9O0L238WRobvn3nYWBm+M7P9+fuT8R/nv7/fuUHZD7Qzhhk03QS6bwA0aAM6WosFFHt/mZj+/v3NCJqgYfzHwPwHvP34719G0MHa4NstGBlAu3mZQNvY/4IOewWdcgFadgraKwnuIbExgY4pZwR1GP4wgvb8gsYAWUCz4KAxYCYWdsY/oF3Cv9mZmNg4//3/z//3HxPoxBsmBtD4JKjsA63z+PefQew/s+yP7x9eX2FlYOVi+Pv1mwA389d/HD9B46ugk1s5wBOToDky0CggaDEY6w/w/C7LBzbm/0ws//6A5vr+gA6ZZPjH9I/5H+gk+v8/GUDRxgRKBiygDgDTf1A3EbQRHjS7B7KXAdxm/M3KygZKWUx//oGW5jD/5/gL8iJoQIWJgeEf458/bL+Z/v3//+sP85/vrL852f5z/Gb4D9pF8JED3E5mAbXqQEULqHxl5uQTVX7x8eff3/+ZGbhZONh+/PvFwMzACRqCBB2XzgSahwLlVlCR/vf/378QjzzkVPzDyMb4l/kXy19Q/wp0A9P/f6Dw+w0aRgHNlYPai3+YwUu4/jGDtruB0iALaP/cf9Cx+3+Z2EC3WIEuh2IGzYMxMrL8ZfwH2sMDWmsKGof/+YXz139mpr8MoF7yT9B0MRMT8x9QE4MPtDLhDwMDI6i5BGqL/fvPyARKJW7v3r559+vXt/+gsvcHB8v/P7/5eYT4ORl///wL2gL8ixF0bA5IN+hWEVBm//SVg5WJEzTDx/IftCUGlIr+MoISF8NvZlA+ALUPQVNBoIMPGMH3BoI26YBGZUEntYArTGZW0FlIIEcw/GdgBdWioHVzbKB9qv8Y/v38+YrrCxPoyBTQ9A37vz9f/3OArur7z8AEWjkIOp8X1CcGLQNg/M8L2qPFwMwkLvLz+8cPb1///Pv5F2gz9j/W37/+/f/9h+sv8z/mXwygAP0BWmTy5+enHx3gPMLM9JKNGTTbAy7XGEET8Jygg8JAA5NMf0A2gLb//wbPuIBqeNBhIf//MjD/YwJ1bUEZ5D/ozDqmP6CTExhA+ZkBtHEWNObCCLqo4Nevr18+fufiZAUdr8TExATaBcT47x/L3/8gM0DdXhbQQQk/GBhBU8l/WZmYQaN6fxg4uPhFfnz/uvkT97//LByM////+fnzz9+f3KD2IPMfRkbQHv3/f5mY/0FSFgNoxw0j0//ffxn+vwVtPgEtVgC1YBn//vkHOgMdtFb7P9Nf0F4jUCkMaqf9/80KGj8BnVUHqlz+///3jxWc1UGb/RhBZR5opzHoyheWf78/f/n8m4H50w8B0EKD76CVpf+4OFn//WZh/Pf/H/M/NlCfjAV0+Aqoo8AA8icHI8Nfpv9fmP4zcfD++WX6+MP3P9yMvzl/sfz5/oPzGy8DqMPPBN7tzcQA6miCwhKUtP4zgS7jYAKdPfgXdGbMX2aGz6Axk78MTN9By6RAh7KwMIC2Dv9nZgCtl2D6/+8XEygXgI6hBB2u+B80p8MG2ggBOqgSXHcxQooVUIPtDyPbt6+gE8b/fmFiZ/z7i5WXgwM0wfcHNOoIqaQZmUE7SEE32IAuIWFkAJ1g+g90iw0zC7uRyutnL7/wMrL///f314+//36ygxbn/WeAbM76DxpHAXWrQEOmoBoWfBcHA+i0fGYGxj/gI7lYQEeBgsYk/zOzg8ZxQU3N/39BRQEj03/GP6DDHP+C+p3/wVdfMPz9/4eJCdRTYGJk+sP8jxm0JY+F8S8DJ+OPP6wsT/+ycf/5zfKTgYONk5Od6T8TJ+iiEVDR+4+RDTTa+gfcOgPdyAE+5oSZAVTugbwmyCsu9+bhW8bvnP9AiYvtN89vUG+D8Q8j6A4SZtDe7S+gLMLAAloPxcgCuh4AtIKBheUfAyvDv3+MLP/YwKNUoMFoBtCytH+Mf0DtJ9DZnQzM/0EDlaDzIkCHq4DCnYnxHxNoiwDojHTQRRygg6tAWx5Bm1P+/RP4zcLN84f582deJl4e0JJHhn+gK7BA1jAxgibr/oA8A7pfBbylFHRpCzNoyQuoSczwl51P5Nm7D78ZQccasf0GnQjG9BvUswANR/7+8OvFB4hHmJiZQC085v+gIS9QHQ66jgl0CQ/T3/+MDJDb61hA25sYmP+ygOabQatAQJU+aLU1E6ieAe1T+A+aFf/HAArYv/+ZGUAzJaCk/o+R9R9oGYgEExv7X/Z/rH/Y2EDjqqAbQtj+gPYaggIElP9AnQWQWSwMoMPVQCcEgi4t/AOqhv/9/Sco+P7YdxbQROTvH7+Z2X79Yf7J8J+BmeXfj0+fP36GxghoZpyZAdzF/scEOocM1ChhYfrD8ge0vRd0YhrodiDQPUygqQPQoiFQLQ+ui0F9KtAFUn9BF5T8YQQtr/j/F9RCZvrHAN41xAaqLkAr5dn+gVam8YIORgL1a0A1CGjX6D9mhj+gGdRf4DP8QJPBoNbeP0Y20JFvoEtx/v8FLQplZvr7X/QXJzs/LyMXD9Ofn///fwMNifz58fn7uze/vkBjBNTVYADlfNAsFgtoKOk/6PRlsAho+zmo1QUaMAEVJf+Y2MHjf39A47ygYgY0/g6KWOZ//1hAi7wZQQuFQAtKQdtnQG3hv3/+/mUHtc6YoXcsgeYUWP7/BdV5DKz//4Pu82Jg/v8b5DXQWBEjI+s/FlAH8D8jIygvgO6tYGT7y8Gp/OHL75+/mb/x/2UBHU/199f3758+fnv//sPXTydBLmAA3f0AKu////8HipX//0D5gfEf+OIz0DYs0FKP/wzQ5Psf0iIB1figJT9MoAWMoAv9QK5jApXn/xiYQAsamRgYwdUbaI8j6Azgf2ygxurff4xcDODBHNBSedAM+j/Q6RCgsxVA7WyGf+C1OKCdZX9BN/UxsoCWAIEW2Pxl+s8uI/jh3d/vDF9BJw+DZlb//Pn148c30MZI0CUToOIXFIagRTCgigZUybEwgvIgqM/x5z+oCQTyGWihMKipAVq9CDoZArQi+g/obh3Q2ADIzyAFjKA7J36DSjK236CDoZj/M/0DDTWC5gpBd6eAtgiBjjYDtcJAozSgvgBo+d4vcLUG6fYwgK7I+Q86tgHUtwOdhQTaOsXy5/9vUX5O3i9fGLm/coNiivEPqFb//vPfn99/v4MjBDS4C1o5AFrF8ZcJXBqBuqyMoC2AoJwOaiSzgDb5gKIbdFcX+BYs0LI40ITvP9BpdaAjxZlAuwxAG6dZwasXWUH7Rf7+Ay1IBq36ZGBh+sPA9Oc/E+h4TNAagn+Mv0GzT6BKCHS3GzMoVhn+/mX49/8/6P4P0BVbv0F3ZoDczMTEysXFwvCHQ0hcXvT3pw8/QdX2/39/fv368//3r99fv32CeIQFdMIvqPD9z/oHNP4ISjzM//8xg3I46I4tZmYGJjbQUBl4VRyotgPVEf8YQI150K7C/6AxCNB6dFCzjPE/IyvbPxbQaWOQ8RpQO/Hvb2bwPS+gPe2gyuYPC+gcRtCAKaiPBlqRyMjwi+nPTyZQ7+Xvvz+/2P6zMv5nZQCdCssAmgoCbfH+/5OZne0fK9O7L1/YfjKBDjb59eP7j9+/v739/gPqEVZ2ZkbQLCUjA8tfRvDuJtDEMqhDCPIjM2jRKyMbA/Mv0K0EoEb0XyZGFlDBBmpbMYHSOBNo0gq0VPE/4++/oOP+QAMMjKArBv+CxjZBIzcMoHzNArq4kYmRgfkvG2gZFajGBNVCvxlAZ+/8Al2QASolQOf+gJZ6gSomJshK2P//QD0j0O2JPDJMXz8K/Gb6/evn99/fv//8/e3VF6g/QMtQGFiYQIedMDKwM4NO5AF1MzgY/4J2tYP2gIJuDfwLWskLSoWgZfwgXzCDWo2g6VTQvCYTaHqC6TcDB2jBMGiJCOgyJtDVekx/QUPgjKz//oAWn4K22INOTgH1EUDX4YGqPQbQSTz/Gdj+sIFWFTGCWvmgVhDoyFJwacLwDxRrjCx/wAtrGBi4ZC98+vX/97/v3xm+ffn55+vLr6DTkMBRAr7wCDTmCV4yyALa5gxaYQlqJoP6CEygCpuZFXwQDajZDdp4+fcvqLhn+f2HFRSioIEwVobff0HNT1DpBdrr9Qu0GR+8MoAZtCvoLxO438Dymxl0IOl/VtB4KmiCkQF0LQ/oQEFQGxB0sMf//79/g7b7MoGXR4IW2oIOfWP4A2ong9IgCyMzr9Rn9t///nz/+fXHr8+vv4H2J4D9AbqJ5j8zqNMD6r0ygYagQX2836Cro5hBrct/LP+/s/0HN0CYGcBnBICniEHHoDH8ZwQtwWX+y/4X5HCGP8wsoIH5P785wOkCNNT4//dvUIEMWrUB2vwB6tb+BW20/As62Q90wd3/3yyQa7FAJ0WDrjBkBO0i+veflRF0xg1o5ReoUGZi/s/y6y94glviD8fnvy/evP/95d3rr39+g04uh3oEtKoONBQMWg8OqvxAm4IgC0uYmf+BxilAw6b/mEGLSkBrMkB1Cwuo7c0MWh7MCjrsmvnrfzYG0DlwoCYpA+QyVtCaLdBWyJ+gXQbgNRGMTH/+MLCCVzwyMICH4kG3ozNx/AdNRjKAro9iAo24/P/7i+HPn3+sTP+YQdsrQDUmaLcg6LIb0OQ0I9e/H1//f/74kuX1Z9CSNAZGhtdQjzAxs4CulWYE3ZMHOgoA1KxlYv7/n5n1x3/QiijQQc7MTAxMf0C7xECV7T9m1r9MvyC1ABOoAmX9x8YCKl5Aezv//WH6y87yg4WZCdSPYmBiA60mB10KCBpxAq07/cfAzvLrNyto9yvIjH+s/3+C1saBV6D+Z2b8zcLACZqa/cUAOjuAgRE0NQo6CZYJNAX+7x/oFBGGr79fvfry8wdoPAFaIYC8wgLqM/0D3W8NCmbG/0xM/8DXRzP8+wda3gPK5X/YGP+w/mdkAB1yxgi6PPc3I2gojon1PwvjHyZWFtA2ZVZG1v//WP4wMHKAtvCBBjgZ/4KOkmFm+f0TVHix/GIAXTbLyMDMys7EyPLrH/O/v6AhwD8/WBh+gBpvoPKPAbQBmgW0goj9369/4L6JIGg1NeikINAQwZ+fv/4Iff/76sfH/6w/Wf6Dr8MGj7uBPQK6rQm03IUZtDwV3KhmBCUoUJHEBLoTEJTt2RiZ/rEw/AEdeQvqCrP9+8P2l/n3H9b/f0BNwL/coJlYRtBtiAysjKAF9aCzJECjgX9Ap62Ad8yDzmD9x/TnNzM7B3jBPfP/f7+ZGEBdw39Mv75yMjH//8MEOhyHGbRJnIEbtBSLEXQN30fQGVOgdi0/47+fXz///PPnC8MHFrZ/v1lB55qB+2UgX4CaKJygdAPqOf77wwZa/wgK57+gcpgFVHiChkZY/oDv8QS14kC1A6iryfqfEbRNhoGJAbREF7TfAxT1rKz//jJxMTD9+8HEyMAKSvYM/36ABjMYQWOKoAEuZsb/oLTP9vv/X1DB/xd0GODffxy/QcmWAbQOj/kfaLnX/29/frKwMLD+//MVVACw/GPi+sjw59uPX7//vf/LyQY6m58JdFUPM2hsDeaRv+yQNTtMrKCTSJjYmMH9J5bfoB4haAc4aFaegQXUEwcV6Qx//4OOhmdh+A5u7oKOsWJi/fcHNMPIwPKT+S/DH+Z/jIygFT8soIFn0J72P0xsPxj/M/9h/M0AWtX15+9/RtDoF2gyjIEFdCrGv1+gyUzQCU2Mf/4y/P7/7+e7/wxMv34yszKAMitoiIzxCwPjn38soGu+mVlAEHTqDgvoXl6oN0BH+IOOBGQC5WLIyDO4c8HwF3SzIiPLn/+soOYmOJQYGP6xgBbY/wbNCDGx/gffuAGKXFDpA9oBBSok//z5x/iXmZnrD2ibMTM7qAL69R+Ul/6BbuX9z/j/D+N/0Kzof1B7ihm0aeH/X7Y/TKA9TKBZD9Bxu7+/ff3LzPoHNI/EycD0G7SKG3S4Gaj2YmP8DjorjQG03AF09iQoG8F8wsL4/zdo8RsjaEQTFN3g1bPg3gj4Ts3/4AUuzKAOz3dQwgANt/4Fne/L9Jed6S8TaIMLK2j5AgMrqM3KwAhqYzH8/ws6dh9UjTGw/QVtYANd8czAADmiEjTc948BtPcT7Jl//8Anj7IxgTb9/GX59fvTF/B4IxMTw8+/nKBGPWj4E+Ts/0wMf/+ysX9l/Pj3z48/f379+fMNNOYD9Qm4mQs6NogJdC4bKNGBFruB9r///8MCSrWgbdwMjEygVYr/mcG9jb9MTKBzw0GbzP79Z/7DyvnuHxPH/7//wSetsvwDH+jIzPqPke0f8+8///79At0nyPoPNIUDcizrLyZGUIuAkYkVtFcJNPMCGhj8A0osf5n+/PgEOjQIdFnmv7///jFxgrbIgg/UZWL6w8TEBqq8Qf1hFua/rP9YWD9++fEG5pHfLEx/QYf6gMZkwBv6QCPPf5mZmX6ygXPef6bfrL9Z/rCCzoYBOYCBifE36DCR34zMoMYjaJiTG3TPHxNofBxUXf7/DbquloGB5Q9oedcf0AQD62/Q+BFoLQfoJB7QAcD/2UF39TL9ZAGd7gPaE/vnJwMryw/mH4xszFwMf0HDeKDh9p/soPYlqCkOGin4x8jyk/Ev61/mf6Bj+kD1BsitMI+AZxhAc8mgtW5MzKAbyphBvd5/rMygNQb/QMfAMf5i+M36H9R8ZWD585eFFTTHABoEBm2nY2EBnX7B/AfUXf3LyARqzILilZHlDwMzI+i0bNDSABYG0PoBBmYmJkbwNSyghcig/VOsDCyM/0FbWViYfv37yc7wm+k/x3/QZYssoBVFfxn+/Gf7ycACviIN1HhlYvn/m5npD2hZA2j1EahbhqhH/jKCtkaBTINsvmRgAd0zxAC69ew/K+iuo7+gQ+dAbbJ/IFv/MbL+BV3YAtpHBzrIFTTuwPaPlZHpFyvDbzbQqo8/oFb3H9C9MgwsoEldFlYGdqaff0EnqP4HTXv/A+0nZGYE3yXE+Bd0Vy9o/xED83/Q4cugHuI/ZravoBHXf7//gc4jAM2tgyYGQX08ZgbGn4yge9RB50AygvYigcZzwHECuoKSkfE/CzOo+Qrq8YNngUDHkjD8ZQH1SkEdKnC7F1TXgAZPQJ0QFvBIEejw0/8s/3//Z///D3RkDnggiYn5GysTaB8E6OJM0JaO3xx/GBhZQf180N7h/0x/QPef//8Dmsr+/4cBdEkeM+goIHbQQnYWJs5/LKCzq77/Z2TkZPr+5zsLaL4fNLgIakEz/WNm/fUfdGICKzPrX4afzOBRN7A/GFjYmRhAozagG8iYOEFNYNDOYdCSUCZWFkZW0KwGqLP4G1R6MoI2RYJ79Ezg4ytBIwks4AX8HL9AB8X8B53swMjI+oftHwvL/7+g5ReMTKDN46C9Q6BBSkYWpv8MHL9Bm1RAx3SCym7mfyygdczMf7/+/cXDxcrCzPyXCbTCj40BdPLlDwZG0Lpy0OGD/0D3kIO6X6AFxEwMrAzgXhBo0RvUI0ygzaEMLP9+szGxgTY8srCCVj0wgEsoRvY/oHtV/jEzg1pKoAlZ8DW+LKAGNQuoM8fIzMrACNqCAzoigoH5L+jyVUaOH/9BZ+mD7udl/fOXmRm8Nh40LPwPdCoyaKkN6IZphl8MTP+ZWH7/AB3+wcrIwPkTdJMf6Agq0JgSB6je/P+fgRe0dgp0zg3oemDQDj5QSQHuSIDuxAX1CCDeALWqWFj/gcoBcEIFNSXAB1yAzwZgAbdSGEDLsX6DZkz+//8Dqp2ZQd1pRlbQrC0rwx+GvxwsrL/+/gNVRqD9bUyMzBysrKzM7Cy/WBg4QL0KkJtANxSDGjRM/0BBDFr7BVq1xwA6ZfPnH9DiVgYOHjbwutHfLAwsnIyg/TlMLKAzIxhAmRZ0qTeowckIui7n79/vP8BdN9AZVjB/gO5YB10n/YcZdG0BaFyRgYkRNNgAWqIPihrQsaWgec2/oDzPwMDwB3Sgxw8WRsbfTKBDBpn//f4HWlMObtqCjqACNUVAzTVmxj+g01rAB3b9ZQRtaPz9F7Q6699fhn+soBPvfoGmk5mY/rMwgI6BYmJk4gVV7//Bkw2gcUlQi5L1x6+/oGPHQIcMgDpmv0EDB6BjhRmZGL9/+/Lp46fPz2E+YWFkZmJl/c/CBDofn5EZNHvzD3RKDWjtK6jbzAhy3d///0ClCuiUzv+gJbWgxeMMv0DjoKCJBvBV06Az7EDjpP8Y/jP9YQWN9INuP//DxPzrPwMbaEacheEPqJoD3RIGmsQDnVzOCAoXBtASNNBV5ky8P0Cj+wz/QOM9oItQ//3+8+cXBwMLC+jKZgZmUEEImvgAzVAzgsaKGUBzLDB/MLCAbg9j/gc6hfAvGwtoDeI/UBcXNFMBOvMJNJzM8ocRtCsVNKMOOuGEBXyEGDMLM+iUzj9soHb9P2am/6ALCkFHjP0DXWH8lxE0EMQMuoIHdCDRf9Ais79sfxh+gQpCRtBBpKBOD2ih00+GX+D5eCZQcxRcKIAWPoOu0vz9++dv0AmMf3+wg8YuQKOaoC4Tw3+2X+AN36DKEF6LgFZiMzH8+83CzAo67YcRNPrzHzQJDRrWYQTVVYyg6ugfM+h82L9MoPMP/jEw/oVc4fIflLvZGFh+gY7rBS8qZQJ1WUEjnaBb6UAz6X9Bpxkw/2EEbTlkBB1aA7r9F3QTBSh4QQN4oCz7m/UXaL86aDs+aPAWtOnoJ+OfH6CB4X+gvZWgziaoMwCaIvvH8h90pwhoNxd4thUeIQygRRZM/1lArXtQGcAIOnUX1CRgYvwLWmDwF9RFBA3hgxwKWhr4GzyOBqrSQUtvmJl+MzGw/wFNOjKBRrUYQQtMWP6A9k6zgCbwQGPyDKB106DxPGbQvS//QItIQG1gpv8//3OC9wyDDwEBLU9iBs3kgdZaM/1h/PUP3Cv795sdfCwZqO8OWj33iwm0zAq00B2U3pAihIGF+z/oLkpWJhbQKCsDE8tv0DzRP9BkGzNoTQIj6JAe0GQ6aPYHvLIU1DEATSIzMP39AzqG8T8T6M5C0BFxTKz/GX/9/8/EAjoqkoGB5RvYNNABwv/+M7CA9soyMnCAkhroNNU/f0HdRzYWpn+//rAw/PvOwMTCzMLA+BM0gc7A+vsvyCOs7IwcrODLakGxB9oZBJpeZgFdcABalg/eJAaLExZmRlZG1n+gIoiNFXR/GwsjuNAF31IBCiLQ7ZigHRNMoBOgGZjA7QsW0PnooL1nf/+xghrnoDFppl+geSQW5j8//oE6hwz///3hZPsFGqkC3VgMWsgIGpRhAZ1cwvTv+2/Gv6BZJ0ZGLiZGRpYvzD9+MLOAZrFA68z/Mn1l/vn3/z9mFgZ28HoI0MYwSEIC3WH/ixl0gTv4QmfQ8BLcI6ANcMygxMTAzAqKqj+g4hw0gg46pxG0j4jlP+h8or/MrP/Ao4ig2SjQgOR/hn+soOUcoMoftMsYNJEKmmH8z8j46zcXC2ho5zcr+Ko50NF24NMzmUBHEf8FbTz6y8L0G7TREbScGXSE1Y/fzOCCC7Sg59/PH39AGwaY//1hYPn3l/kPE2htK2i8FbSIgvEP879voAYeqGHLBDr2A+oTFlDnEDwLwAhaksnACLqpHrRCBNQdBI23/QO1J0HXYbH+YmT595OV9T8Hw392UPeC6Q8DqIXAApqiY/oH2jsA6hYy/GUEdz1A5xX9/fX9K6iXwMb0B+wIZtC5t6D1PQyMDKx//4E2PoN2J7Kw/WJiYGNjZ2Bk/8Pw/S8L82/QhoW/DKz/fjMw/fnNAapdwOPsjODZZWb2Tz9//vz29cPXz5+fQb3BwMDCxsAGuo8FdK8laHkWyx/QDCAj6CIa0MYARlD/BVRZMDKBbif7x/gbNDjFBOqpgopi0DzEX1D37x9oGfAf0KwQEyMDByto7wdo/cZ/BjZGJqY///6B7pZnZgINZfxlYvzDxv3rDyMH45//TMxsoElvVr7/v1g4mf/9Y2IAT/Iy/uL6B7Lo/x8W0MZM0HAW6Kx0UIH7B3J1JmjKDBQvcH8wgE7jYWFgZgWdzwiSAbXrwB7/ywLaPA4ZpgCdHMAACnlGRqbfoPU0oDYRaCz0719QMwLUOP8LOheZ5d8/tn8soIqI+e8vZtB4DjMTy1/QySKg0wL/gxtSjL+YGJi4QOen/P37HXRaMeguQmbQAnlGZtCBZ6DTViFJ9/9/VnbQpTjMoOLqP6jS+QcaW+f6BSqzGZlA6wNAzWyoX1jYGJnYGJlYmH+DFi+AiifQkVKQZhmo7w0KRdDqdEYG0NIhJnbG3/9BC63BTT9Q74OJiQk0XwiaCWD5zwxZvQ46goyR5R/Lb9BCzT8s/9gZ/jL8ZfsLWrYC6pf9Y/7LChqOZWQFLUMADfIyMf1l/wcaDf8HWljPyPKf/T/odBUG0MgZ6D4bFtA6eUbQGO1/0M0KoLgGTRCBzieGeoOBgeUBaEMjIwOTKgMrI2hsG7TnETRWD+ppgqbZQAPSoH4N6DQo0IAd6z/wznTQQCAT4x9Q35wJNJf6n+E/K+sfJo6/zH8YWRh+s4LWmrD+ZGYGL+n4z/QfNPMCWrf5jwl0UAao5QY+b4b5P2gj5j/QVTSgrgHojl3Q8Tag+z7+g46MYwN1ccFjcYyQKa1/jKAT4UFxBOogg2s5iF9YXCE0wzHQKc8MDIxaoDM7QAebgCbZQKtKGUHHbILuSAEtpwTdSAUaUgGVDKC6CaQQ1JL7z/b330/QlBPzL1BPgwm0zecfA8tv0MALIzPoDhBQLfAXdO0GaK4RNJjP+ucfSJYN1NEAnbUB6jMyMzN8ZwH1QL//AU38g2Kb+R/D3/+gIYj/zL///fv/j+EHMxvLH9D8PxMz6MBBqPNBqxcgTCsIxXAJ1Mz6z8CoCZpbYfj//xd4iAU0BgC6aYzx/x9QRwTUFwTleNCYAnSIArSkgglcyoKWyP4DbwBnY/oDugwFdBswaMMD6HQDBlbQqi1QTQBqHTD+YwCteAOdpMMMWtsKOk3tDyPDz59/QVs1QAtL/jEy/GNh+Am6eAc0Hs7E9P83uAQAJS/IymWou0Ejs1AmhNKDUAxXGRhA7QImDfBqCIb//74wgBZfMLGAxtVB3S7QYmZQ9x+0aPE/aOYJ1IgGjXn+Ay1xA/WOwIUYaFyd8R9oAd5/FgbGf6BrEEA9KtAcAnSul5UBtFv7JwsrqGJiBR0iy/KP+f/vP78ZGDn/gYLpN+hSJ9AaFVA9xwA6xQyU/0FDPmABiIPhnXcIF5O8Ahb6//+/KmhUipHh92/QSBp4ug80hAgqskFD+P//M/0GDa6w/vn3H3TNI2jWHJTufv0HnTMBWpMG6lCAxqVBbNBqqb8Mv0FHhDCBboEEtdtAKz9B63l+g1Yl//v/69svUGcGtCgM1FoBFb2Mf3+Djvdj/Pz15/fPX758/fTuKthxYAIjRsCiSIQOlH3xFgNo6Ivhnxp4Yh10JAk4NYBSIChVgHaWg6ofZtDIDwtoEhq8aJCDAXTmB2hKDLQwDXT4LGgeHzS7+OsvO2iilBE0sgA624YRdGIGKNv8Bi1D/vMPdJE8eLkoAwPbrz+gzidosIPx/2/QaBVoSQLIZqjjQKPxCCZ+lj5M+izowO3//xk1QR1V0Gw7A2gTO/iwGqZ/zH/AWy1AE4mg9ZYgd7H+Y2IC3VfLyAS6tQC0GYWJCVRTsTAx/gOtZvzLDBqfAa08Yvz5H7QnhAF07gUjB2jUm5Hpz3/Q5d4cP1lAJv8DxTFo5fAv0DIYcLsK5ipEZoeLEGIYQxVcBp2xwsjEqA6aNgJlJ9Biz//MoGkl0NIlUA4D2fsbNKDzkxl0fAS4igXt9IJUz0z/2EAVPugiM9CAHXjQC1w2gE73BEUy4z9QO/cPqEBgAdUboPVcoM4qqN0FuvIClNygjgH17OFMEhm6UPVXQAMc//8waILW4oCmU0GH14J6lwz///4BTeYx/QTtKGAATbmCxsr+Mv1mZ/r/hxm0A4WB9Reo6Q0+AYXhHyhGQPdI/GdgZfz5m5kVtC3uHwPn339/wRvOGH+D5oJA9wCBkgDo9CCkip2MGIG6H07BMxFong+U2FVB6zlAjWBmcAMNdEAGKPxAYQqaLgVdvAea3wZ1GJn+cf5lYP4N2q0MGgQEnXIB6r4wgLY1/Wf/A1qIDlrIAFowwPYfdJk2aJIf1OhhALd7kUsqgpkd7mBCDHgmugqqs/8z/tf8B1r8BDrJCNRK+Mv0B3RwDWhgDDwh9peZgeMPeEITdKzOP1ADGdRdA+w/qGMF2snC/J3h339m0IIW0JH2oOXHoKX54I06zKyg0WVQwwnJTdTzCNxQbSjrKmiVE+P/fzqgdQugHg4DaCsK85//oFlkcNefgeE/Exv4SBLGfwws/5j/glp0IFX//v1n4vr7i+U3aErxP2haluHvDwbQWO9vZiZGJtDpdOBT9KA2gSgAmHwnNTACgpMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_clahe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Original Grayscale\", img)\n",
    "\n",
    "# Step 4: Wait for key press and close windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image in grayscale\n",
    "image = cv2.imread('//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/'+data['file_path'][1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply CLAHE\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
    "clahe_image = clahe.apply(image)\n",
    "\n",
    "# Plot original and CLAHE-enhanced image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"CLAHE Image\")\n",
    "plt.imshow(clahe_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image in grayscale\n",
    "image = cv2.imread('//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/'+data['file_path'][1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Convert to float32 for precision\n",
    "image_float = image.astype(np.float32)\n",
    "\n",
    "# Compute mean and std per image\n",
    "mean = np.mean(image_float)\n",
    "std = np.std(image_float)\n",
    "\n",
    "# Avoid division by zero\n",
    "if std == 0:\n",
    "    std = 1e-8\n",
    "\n",
    "# Normalize to zero mean and unit variance\n",
    "normalized_image = (image_float - mean) / std\n",
    "\n",
    "\n",
    "# Plot original and normalized image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Normaliza Image\")\n",
    "plt.imshow(normalized_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyLungMask:\n",
    "    def __init__(self, left_rle, right_rle, heart_rle, margin_radius=20, original_shape=(1024, 1024), image_shape=(512, 512)):\n",
    "        self.left_rle = left_rle\n",
    "        self.right_rle = right_rle\n",
    "        self.heart_rle = heart_rle\n",
    "        self.margin_radius = margin_radius\n",
    "        self.original_shape = original_shape\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    def decode_rle(self, rle_str):\n",
    "        if isinstance(rle_str, pd.Series):\n",
    "            rle_str = rle_str.iloc[0]\n",
    "        if pd.isna(rle_str):\n",
    "            return np.zeros(self.original_shape, dtype=np.uint8)\n",
    "\n",
    "        s = np.fromiter(map(int, rle_str.strip().split()), dtype=np.int32)\n",
    "        starts = s[0::2] - 1  # 1-based indexing\n",
    "        lengths = s[1::2]\n",
    "        ends = starts + lengths\n",
    "\n",
    "        mask = np.zeros(self.original_shape[0] * self.original_shape[1], dtype=np.uint8)\n",
    "\n",
    "        # Efficient fill using advanced indexing\n",
    "        for start, end in zip(starts, ends):\n",
    "            mask[start:end] = 1\n",
    "\n",
    "        return mask.reshape(self.original_shape)\n",
    "\n",
    "\n",
    "    def dilate_mask(self, mask):\n",
    "        selem = disk(self.margin_radius)\n",
    "        return binary_dilation(mask, structure=selem).astype(np.uint8)\n",
    "\n",
    "    def resize_mask(self, mask):\n",
    "        mask_img = Image.fromarray(mask.astype(np.uint8) * 255)\n",
    "        mask_resized = mask_img.resize((self.image_shape[1], self.image_shape[0]), resample=Image.NEAREST)\n",
    "        return np.array(mask_resized) // 255\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = F.to_pil_image(image)\n",
    "\n",
    "        image_resized = image.resize(self.image_shape[::-1], Image.BILINEAR)\n",
    "        image_np = np.array(image_resized)\n",
    "\n",
    "        left_mask = self.decode_rle(self.left_rle)\n",
    "        right_mask = self.decode_rle(self.right_rle)\n",
    "        heart_mask = self.decode_rle(self.heart_rle)\n",
    "        \n",
    "\n",
    "        left_mask = self.dilate_mask(left_mask)\n",
    "        right_mask = self.dilate_mask(right_mask)\n",
    "        heart_mask = self.dilate_mask(heart_mask)\n",
    "\n",
    "        combined_mask = left_mask + right_mask + heart_mask\n",
    "        combined_mask = np.clip(combined_mask, 0, 1)\n",
    "\n",
    "        combined_mask = self.resize_mask(combined_mask)\n",
    "        masked_image = image_np * combined_mask\n",
    "\n",
    "        return Image.fromarray(masked_image.astype(np.uint8))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\main\\mimic\\dataset\\train_mask_clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Select 10 random rows\n",
    "sampled_data = data.sample(n=10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Plotting setup\n",
    "fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(12, 40))\n",
    "fig.suptitle(\"Original Image | Mask | Mask with Margin\", fontsize=16, y=1.02)\n",
    "\n",
    "for i, row in sampled_data.iterrows():\n",
    "    try:\n",
    "        # Load image\n",
    "        path = os.path.join('//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/', row['file_path'])\n",
    "        image = Image.open(path).convert('L')\n",
    "\n",
    "        # Generate masks\n",
    "        left_rle = row['Left Lung']\n",
    "        right_rle = row['Right Lung']\n",
    "        heart_rle = row['Heart']\n",
    "        masker_margin = ApplyLungMask(left_rle, right_rle, heart_rle, margin_radius=50)\n",
    "        masker_nomargin = ApplyLungMask(left_rle, right_rle, heart_rle, margin_radius=0)\n",
    "\n",
    "        mask_image = masker_nomargin(image)\n",
    "        mask_margin_image = masker_margin(image)\n",
    "\n",
    "        # Plot Original Image\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title('Original Image')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Plot Mask\n",
    "        axes[i, 1].imshow(mask_image, cmap='gray')\n",
    "        axes[i, 1].set_title('Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # Plot Mask with Margin\n",
    "        axes[i, 2].imshow(mask_margin_image, cmap='gray')\n",
    "        axes[i, 2].set_title('Mask with Margin')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {row['file_path']}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = pd.DataFrame(data, columns=['subject_id', 'study_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths['study_id'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths['subject_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define your data transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a standard size (e.g., 224x224)\n",
    "    transforms.ToTensor(),          # Convert image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "# Path to your MIMIC-CXR-JPG dataset\n",
    "dataset_dir = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/'\n",
    "\n",
    "# Collect all the image file paths\n",
    "image_paths = [os.path.join(dataset_dir, fname) for fname in merge if fname.endswith('.jpg')]\n",
    "\n",
    "# Initialize lists to collect pixel values\n",
    "all_pixels = []\n",
    "\n",
    "# Load images and collect pixel data\n",
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = transform(img)  # Apply the transformations\n",
    "    all_pixels.append(img)\n",
    "\n",
    "# Convert all collected pixel data into a single tensor\n",
    "all_pixels = torch.stack(all_pixels)\n",
    "\n",
    "# Compute the mean and std\n",
    "mean = torch.mean(all_pixels, dim=(0, 2, 3))  # Mean across batch, height, and width\n",
    "std = torch.std(all_pixels, dim=(0, 2, 3))    # Std across batch, height, and width\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunksize = 10000\n",
    "all_mask_data = []\n",
    "\n",
    "file_path = r\"\\\\gaia\\imageData\\deep_learning\\input\\data\\chexmask\\chexmask-database-a-large-scale-dataset-of-anatomical-segmentation-masks-for-chest-x-ray-images-1.0.0\\Preprocessed\\MIMIC-CXR-JPG.csv\"\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    all_mask_data.extend(chunk[['dicom_id', 'Left Lung', 'Right Lung', 'Landmarks']].to_dict('records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df = pd.DataFrame(all_mask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"\\\\gaia\\imageData\\deep_learning\\input\\data\\chexmask\\chexmask-database-a-large-scale-dataset-of-anatomical-segmentation-masks-for-chest-x-ray-images-1.0.0\\Preprocessed\\MIMIC-CXR-JPG.csv\"\n",
    "chunksize = 1000\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    mask_data = chunk\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data['Landmarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['file_path'].str.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_id = []\n",
    "for path in data['file_path'].str.split('/'):\n",
    "     dicom_id.append(path[-1][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_df = pd.DataFrame(dicom_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat([data, dicom_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.rename(columns={0:'dicom_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dicom_id = mask_df[mask_df['dicom_id'].isin(dicom_id)]['dicom_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_contain_datset = new_data[new_data['dicom_id'].isin(mask_dicom_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mask_dataset = pd.merge(mask_contain_datset, mask_df, how='inner', on='dicom_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mask_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation\n",
    "from skimage.morphology import disk\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def decode_rle(rle_str, shape=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Decode RLE string to binary mask.\n",
    "    \"\"\"\n",
    "    s = list(map(int, rle_str.strip().split()))\n",
    "    starts, lengths = s[0::2], s[1::2]\n",
    "    flat_mask = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for start, length in zip(starts, lengths):\n",
    "        flat_mask[start:start + length] = 1\n",
    "    return flat_mask.reshape(shape)\n",
    "\n",
    "\n",
    "def dilate_mask(mask, margin_radius):\n",
    "    selem = disk(margin_radius)\n",
    "    return binary_dilation(mask, structure=selem).astype(np.uint8)\n",
    "\n",
    "def resize_mask(mask, new_shape):\n",
    "    \"\"\"\n",
    "    Resize mask using nearest neighbor (no interpolation).\n",
    "    \"\"\"\n",
    "    mask_img = Image.fromarray(mask.astype(np.uint8) * 255)\n",
    "    mask_resized = mask_img.resize((new_shape[1], new_shape[0]), resample=Image.NEAREST)\n",
    "    return np.array(mask_resized) // 255  # back to binary\n",
    "\n",
    "\n",
    "def rescale_landmarks(landmarks, from_shape, to_shape):\n",
    "    \"\"\"\n",
    "    Rescale landmark coordinates from one image shape to another.\n",
    "    \"\"\"\n",
    "    scale_x = to_shape[1] / from_shape[1]\n",
    "    scale_y = to_shape[0] / from_shape[0]\n",
    "    return [(int(x * scale_x), int(y * scale_y)) for x, y in landmarks]\n",
    "\n",
    "\n",
    "\n",
    "def parse_landmarks(landmark_str):\n",
    "    try:\n",
    "        coords = list(map(int, landmark_str.strip().split(',')))\n",
    "        if len(coords) % 2 != 0:\n",
    "            raise ValueError(\"Uneven number of coordinates in landmark string\")\n",
    "        landmarks = [(coords[i], coords[i + 1]) for i in range(0, len(coords), 2)]\n",
    "        return landmarks\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse landmarks:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "mask_df = data\n",
    "row = mask_df.iloc[11]\n",
    "\n",
    "# === Step 3: Load image ===\n",
    "image_path = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/' + row['file_path']\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "print(image.shape)\n",
    "h, w = image.shape\n",
    "original_shape = (1024, 1024)\n",
    "actual_shape = image.shape \n",
    "\n",
    "left_mask = decode_rle(row['Left Lung'], shape=(1024, 1024))\n",
    "right_mask = decode_rle(row['Right Lung'], shape=(1024, 1024))\n",
    "\n",
    "left_mask_dilated = dilate_mask(left_mask, 60)   # smaller margin\n",
    "right_mask_dilated = dilate_mask(right_mask, 60) # bigger margin\n",
    "\n",
    "mask_combined = np.clip(left_mask_dilated + right_mask_dilated, 0, 1)\n",
    "mask_resized = resize_mask(mask_combined, image.shape)\n",
    "masked_image = image * mask_resized\n",
    "\n",
    "# Apply the mask to image\n",
    "masked_image = image * mask_resized\n",
    "\n",
    "\n",
    "# Plot everything\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image with Landmarks\")\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Masked Image (Lungs)\")\n",
    "plt.imshow(masked_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "        \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, base_dir='//gaia/imageData/deep_learning/output/Sutariya/chexpert/'):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.base_dir, self.image_paths[idx])\n",
    "        image = Image.open(path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32) \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def prepare_dataloaders(image_paths, labels, shuffle=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Lambda(lambda i: i.repeat(3, 1, 1) if i.shape[0] == 1 else i),\n",
    "        transforms.Lambda(lambda i: i/255),\n",
    "        transforms.Normalize(mean=[0.5062, 0.5062, 0.5062], std=[0.2873, 0.2873, 0.2873]), # Adapt to own standard deviation and mean to Chexpert\n",
    "        transforms.Lambda(lambda i: i.to(torch.float32)),\n",
    "        transforms.RandomResizedCrop((224,224), scale=(0.6, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(contrast=(0.7, 1.2)) # Randomly change the brightness, contrast, saturation and hue of an image\n",
    "    ])\n",
    "\n",
    "    dataset = MyDataset(image_paths, labels, transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda i: i.repeat(3, 1, 1) if i.shape[0] == 1 else i),\n",
    "    transforms.Normalize(mean=[0.5062]*3, std=[0.2873]*3),\n",
    "    transforms.RandomResizedCrop(\n",
    "        (200, 200),\n",
    "        scale=(0.9, 1.0),               \n",
    "        ratio=(0.9, 1.1),              \n",
    "        interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomVerticalFlip(0.3),\n",
    "    transforms.RandomRotation(degrees=10)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_chexpert = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\chexpert\\train_clean_dataset.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
    "data = MyDataset(training_dataset_chexpert['Path'], training_dataset_chexpert[labels].values, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = MyDataset(training_dataset_chexpert['Path'], training_dataset_chexpert[labels].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbl = data.__getitem__(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, lbl1 = data1.__getitem__(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in train_loader:\n",
    "     print(img, label)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_chexpert['Pneumonia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\chexpert\\valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_positive_sample(group):\n",
    "\n",
    "    disease_columns = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture'\n",
    "    ]\n",
    "    \n",
    "    group['positive_count'] = group[disease_columns].sum(axis=1)\n",
    "    \n",
    "    positive_cases = group[group['positive_count'] > 0]\n",
    "    \n",
    "    if not positive_cases.empty:\n",
    "\n",
    "        selected_sample = positive_cases.loc[positive_cases['positive_count'].idxmax()]\n",
    "    else:\n",
    "        selected_sample = group.sample(n=1).iloc[0]\n",
    "    \n",
    "    return selected_sample\n",
    "\n",
    "\n",
    "# Select the single subject_id per patient which has most positive disease \n",
    "def sampling_datasets(training_dataset):\n",
    "\n",
    "    training_dataset = training_dataset.groupby('subject_id', group_keys=False).apply(select_most_positive_sample)\n",
    "    training_dataset.drop(columns=['positive_count'], inplace=True, errors='ignore')\n",
    "    \n",
    "    return training_dataset\n",
    "\n",
    "\n",
    "def cleaning_datasets(traning_dataset):\n",
    "\n",
    "    traning_dataset[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']] = (traning_dataset[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']].fillna(0.0) == 1.0).astype(int)  # In The limits of fair medical imaging paper they treat uncertain label as negative and fill NA with 0.\n",
    "\n",
    "    #Select only Frontal View \n",
    "    traning_dataset = traning_dataset[traning_dataset['Frontal/Lateral'] == 'Frontal']\n",
    "\n",
    "    return traning_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframe(training_data, demographic_data):\n",
    "    path = training_data['Path']\n",
    "    patientid = []\n",
    "    for i in path:\n",
    "        id = i.split(sep='/')[2]\n",
    "        id = id.replace(\"patient\", \"\")\n",
    "        patientid.append(float(id))\n",
    "\n",
    "    temp_patient = pd.DataFrame(patientid,columns=['patient_id'])\n",
    "    training_data = training_data.reset_index(drop=True)\n",
    "    training_data['subject_id'] = temp_patient['patient_id']\n",
    "    training_data_merge = training_data.merge(demographic_data, on='subject_id')\n",
    "    return training_data_merge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\chexpert\\demographics_CXP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = merge_dataframe(training_data, demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_merge_data = cleaning_datasets(merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_merge_data = sampling_datasets(clean_merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_merge_data['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ethnic_group = sampling_merge_data.groupby('ethnicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_data = {}\n",
    "for group, data in train_ethnic_group:\n",
    "     group_by_data[group] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = 60 \n",
    "ethnicity_col = 'Ethnicity'\n",
    "diseases = [\n",
    "    \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\",\n",
    "    \"Enlarged Cardiomediastinum\", \"Fracture\", \"Lung Lesion\",\n",
    "    \"Lung Opacity\", \"No Finding\", \"Pleural Effusion\", \"Pleural Other\",\n",
    "    \"Pneumonia\", \"Pneumothorax\", 'Support Devices'\n",
    "]\n",
    "ethnic_groups = [\"Non-Hispanic/Non-Latino\", \"Hispanic/Latino\"]\n",
    "\n",
    "df = sampling_merge_data\n",
    "\n",
    "# Helper to sample for test set\n",
    "def sample_test(df_group, disease, n_samples=N):\n",
    "    positives = df_group[df_group[disease] == 1]\n",
    "\n",
    "    sampled_pos = positives.sample(n=n_samples, random_state=42)\n",
    "\n",
    "    return sampled_pos\n",
    "\n",
    "# Sample the test set\n",
    "test_dfs = []\n",
    "\n",
    "for ethnicity in ethnic_groups:\n",
    "    df_ethnicity = group_by_data[ethnicity]\n",
    "    \n",
    "    for disease in diseases:\n",
    "        sampled_test = sample_test(df_ethnicity, disease, N)\n",
    "        test_dfs.append(sampled_test)\n",
    "\n",
    "# Combine test samples\n",
    "final_test_df = pd.concat(test_dfs).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Save\n",
    "final_test_df.to_csv('test_split.csv', index=False)\n",
    "\n",
    "print(\"✅ Done! Test shape:\", final_test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\",\n",
    "    \"Enlarged Cardiomediastinum\", \"Fracture\", \"Lung Lesion\",\n",
    "    \"Lung Opacity\", \"No Finding\", \"Pleural Effusion\", \"Pleural Other\",\n",
    "    \"Pneumonia\", \"Pneumothorax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Support Devices'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['No Finding'].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test[test['Fracture'] == 1].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['Fracture'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test[test['Fracture'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_merge_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['Sampled_Ethnicity', 'Sampled_Disease'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_data = {}\n",
    "group_data = test.groupby('ethnicity')\n",
    "for g, d in group_data:\n",
    "     group_by_data[g] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices'\n",
    "]\n",
    "\n",
    "feature_counts = []\n",
    "\n",
    "for feature in features:\n",
    "    counts = group_by_data['Non-Hispanic/Non-Latino'][feature].value_counts()\n",
    "    for value, count in counts.items():\n",
    "        feature_counts.append({\n",
    "            'Feature': feature,\n",
    "            'Value': value,\n",
    "            'Count': count,\n",
    "            'ratio': np.round(count / len(group_by_data['Non-Hispanic/Non-Latino']),2)\n",
    "        })\n",
    "\n",
    "summary_non_hispanic_df = pd.DataFrame(feature_counts)\n",
    "\n",
    "summary_non_hispanic_df = summary_non_hispanic_df.sort_values(by=['Feature', 'Count']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_non_hispanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dataset = pd.read_csv(r\"E:\\Thesis\\CXR_Preprocessing\\cxr_preprocessing\\sampling_data_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r'E:\\Thesis\\CXR_Preprocessing\\cxr_preprocessing\\test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['subject_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_data = sampling_dataset[~sampling_dataset['subject_id'].isin(test['subject_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampling_dataset) - len(traning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices'\n",
    "]\n",
    "\n",
    "feature_counts = []\n",
    "\n",
    "for feature in features:\n",
    "    counts = group_by_data['Hispanic/Latino'][feature].value_counts()\n",
    "    for value, count in counts.items():\n",
    "        feature_counts.append({\n",
    "            'Feature': feature,\n",
    "            'Value': value,\n",
    "            'Count': count,\n",
    "            'ratio': np.round(count / len(group_by_data['Hispanic/Latino']),2)\n",
    "        })\n",
    "\n",
    "summary_hispanic_df = pd.DataFrame(feature_counts)\n",
    "\n",
    "summary_hispanic_df = summary_hispanic_df.sort_values(by=['Feature', 'Count']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hispanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_non_hispanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_data['Non-Hispanic/Non-Latino'].value_counts(subset=[\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.read_csv(r\"E:\\Thesis\\CXR_Preprocessing\\sampling_data_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = np.random.randint(len(training_data_merge['Path']), size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for i in range(len(random_numbers)):\n",
    "     img = cv2.imread(r'..//..//datasets/' + str(training_data_merge['Path'][random_numbers[i]]), cv2.IMREAD_GRAYSCALE)\n",
    "     imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in imgs:\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    adaptive_img = clahe.apply(i)\n",
    "    \n",
    "    images = [i ,adaptive_img]\n",
    "    titles = ['Original Image',  'Adaptive Hist']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))  \n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(titles[i])\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].hist(image.ravel(), bins=256)\n",
    "        axes[i, 1].set_title('Histogram')\n",
    "\n",
    "        axes[i, 2].hist(image.ravel(), bins=256, cumulative=True)\n",
    "        axes[i, 2].set_title('Cumulative Distribution')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in imgs:\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    adaptive_img = clahe.apply(i)\n",
    "\n",
    "\n",
    "    images = [i ,adaptive_img]\n",
    "    titles = ['Original Image',  'Adaptive Hist']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))  \n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(titles[i])\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].hist(image.ravel(), bins=256)\n",
    "        axes[i, 1].set_title('Histogram')\n",
    "\n",
    "        axes[i, 2].hist(image.ravel(), bins=256, cumulative=True)\n",
    "        axes[i, 2].set_title('Cumulative Distribution')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_hist_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_hist_img = cv2.equalizeHist(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "ada_hist_img = clahe.apply(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "# Plot original histogram and cumulative histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(eq_hist_img, cmap='gray')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(), bins=255, range=(1,255))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(eq_hist_img.ravel()*255, bins=255,  range=(1,255))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(ada_hist_img.ravel()*255, bins=255,  range=(1,255))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ada_hist_img, cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(), bins=256, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(ada_hist_img.ravel(), bins=256, cumulative=True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(eq_hist_img.ravel(), bins=256, cumulative=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset = pd.read_csv('../../mimic_cxr/mimic_cxr_jpg_with_demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_datasets(traning_dataset):\n",
    "\n",
    "    traning_dataset[['Atelectasis', 'Cardiomegaly',\n",
    "       'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture',\n",
    "       'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n",
    "       'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']] = (traning_dataset[['Atelectasis', 'Cardiomegaly',\n",
    "       'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture',\n",
    "       'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n",
    "       'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']].fillna(0.0) == 1.0).astype(int) # In The limits of fair medical imaging paper they treat uncertain label as negative and fill NA with 0.\n",
    "\n",
    "    return traning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_most_positive_sample(group):\n",
    "    disease_columns = [\n",
    "        'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "        'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', \n",
    "        'Lung Opacity', 'No Finding', 'Pleural Effusion', \n",
    "        'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n",
    "    ]\n",
    "    \n",
    "    # Count positive cases\n",
    "    group['positive_count'] = group[disease_columns].sum(axis=1)\n",
    "\n",
    "    # Select cases with at least one positive disease\n",
    "    positive_cases = group[group['positive_count'] > 0]\n",
    "    \n",
    "    if not positive_cases.empty:\n",
    "        selected_sample = positive_cases.loc[[positive_cases['positive_count'].idxmax()]].copy()\n",
    "    else:\n",
    "        selected_sample = group.sample(n=1)\n",
    "\n",
    "    return selected_sample\n",
    "\n",
    "# Select one subject_id per patient with the most positive diseases\n",
    "def sampling_datasets(training_dataset):\n",
    "    training_dataset = training_dataset.groupby('subject_id', group_keys=False).apply(select_most_positive_sample)\n",
    "    \n",
    "    # Drop helper column\n",
    "    training_dataset.drop(columns=['positive_count'], inplace=True, errors='ignore')\n",
    "\n",
    "    return training_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset = cleaning_datasets(mimic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_mimic_dataset = sampling_datasets(mimic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id, study_id = sampling_mimic_dataset['subject_id'], sampling_mimic_dataset['study_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "with open('IMAGE_FILENAMES.txt', 'r') as f:\n",
    "     paths = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = []\n",
    "for i in paths:\n",
    "     clean_path.append(i[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][0:-1][11:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][0:-1][21:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "for path in clean_path:\n",
    "    \n",
    "    patient_id = path[11:19]\n",
    "    study_id = path[21:29]\n",
    "    data.append((patient_id, study_id, path))\n",
    "\n",
    "df_paths = pd.DataFrame(data, columns=['subject_id', 'study_id', 'file_path'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths['subject_id'] = df_paths['subject_id'].astype(str)\n",
    "df_paths['study_id'] = df_paths['study_id'].astype(str)\n",
    "\n",
    "# Ensure mimic_dataset has the same data types\n",
    "sampling_mimic_dataset['subject_id'] = sampling_mimic_dataset['subject_id'].astype(str)\n",
    "sampling_mimic_dataset['study_id'] = sampling_mimic_dataset['study_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mimic_dataset = sampling_mimic_dataset.merge(df_paths, on=['subject_id', 'study_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mimic_dataset.drop_duplicates(subset=['subject_id', 'study_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0' + str(merge_mimic_dataset['file_path'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imread(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, img1, img2,img3 ,img4, img5,img6,img7,img8 = plt.imread(r\"E:\\Thesis\\demo.jpg\"),plt.imread(r\"E:\\Thesis\\demo1.jpg\"), plt.imread(r\"E:\\Thesis\\demo (2).jpg\"), plt.imread(r\"E:\\Thesis\\demo (3).jpg\"), plt.imread(r\"E:\\Thesis\\demo (4).jpg\"), plt.imread(r\"E:\\Thesis\\demo (5).jpg\"), plt.imread(r\"E:\\Thesis\\demo (6).jpg\"), plt.imread(r\"E:\\Thesis\\demo (7).jpg\"), plt.imread(r\"E:\\Thesis\\demo (8).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_img, eq_img1, eq_img2, eq_img3,eq_img4,eq_img5,eq_img6,eq_img7,eq_img8 = exposure.equalize_hist(img), exposure.equalize_hist(img1), exposure.equalize_hist(img2), exposure.equalize_hist(img3), exposure.equalize_hist(img4), exposure.equalize_hist(img5), exposure.equalize_hist(img6), exposure.equalize_hist(img7), exposure.equalize_hist(img8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [img, img1, img2, img3, img4, img5, img6, img7, img8]\n",
    "equalized_images = [eq_img, eq_img1, eq_img2, eq_img3, eq_img4, eq_img5, eq_img6, eq_img7, eq_img8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_equalize(image_path):\n",
    "    # Read image in grayscale\n",
    "    original_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image to (224, 224)\n",
    "    resized_img = cv2.resize(original_img, (224,224))\n",
    "    \n",
    "    # Exclude pixels with 0 value for processing\n",
    "    img = resized_img[(resized_img > 0)]\n",
    "    \n",
    "    # Equalize the histogram of the image\n",
    "    hist = exposure.equalize_hist(img,nbins=255)\n",
    "    \n",
    "    return resized_img, hist\n",
    "\n",
    "# Paths for your 9 images\n",
    "image_paths = [\n",
    "    r\"E:\\Thesis\\demo.jpg\",\n",
    "    r\"E:\\Thesis\\demo1.jpg\",\n",
    "    r\"E:\\Thesis\\demo (2).jpg\",\n",
    "    r\"E:\\Thesis\\demo (3).jpg\",\n",
    "    r\"E:\\Thesis\\demo (4).jpg\",\n",
    "    r\"E:\\Thesis\\demo (5).jpg\",\n",
    "    r\"E:\\Thesis\\demo (6).jpg\",\n",
    "    r\"E:\\Thesis\\demo (7).jpg\",\n",
    "    r\"E:\\Thesis\\demo (9).jpg\"\n",
    "]\n",
    "\n",
    "# Prepare lists to store results\n",
    "original_images = []\n",
    "equalized_images = []\n",
    "\n",
    "# Process each image\n",
    "for path in image_paths:\n",
    "    orig, eq = read_and_equalize(path)\n",
    "    original_images.append(orig)\n",
    "    equalized_images.append(eq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, hist = read_and_equalize(r\"C:\\Users\\sutariya\\Downloads\\IM-0035-0001.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_img = exposure.equalize_hist(im,nbins=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(im.ravel(), bins=255, range=(1,255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eq_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eq_img.ravel()*255, bins=255, range=(1,255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot original histogram and cumulative histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(equalized_images[i].ravel(), bins=255, range=(1,255))\n",
    "    plt.title(f\"Histogram of Original Image {i+1} (No 0s)\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(original_images[i].ravel(), bins=255, cumulative=True)\n",
    "    plt.title(f\"Cumulative Histogram of Original Image {i+1} (No 0s)\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Cumulative Frequency\")\n",
    "\n",
    "    # Plot equalized histogram and cumulative histogram\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(equalized_images[i].ravel(), bins=255, range=(1,255))\n",
    "    plt.title(f\"Histogram of Equalized Image {i+1} (No 0s)\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(equalized_images[i].ravel(), bins=255, cumulative=True)\n",
    "    plt.title(f\"Cumulative Histogram of Equalized Image {i+1} (No 0s)\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Cumulative Frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(equalized_images[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot original histogram and cumulative histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_images[i], cmap='gray')\n",
    "    plt.title(f\"Image {i}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(equalized_images[i],cmap='gray')\n",
    "    plt.title(f\"Eq_image {i}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file names\n",
    "chexpert_file = \"../../datasets/mimic-cxr-2.0.0-chexpert.csv\"  # MIMIC-CXR metadata\n",
    "patients_file = \"../../datasets/admissions.csv\"  # MIMIC-IV patient demographics\n",
    "output_file = \"mimic_cxr_jpg_with_demographics.csv\"  # Merged output file\n",
    "\n",
    "# Load the CSV files\n",
    "df_chexpert = pd.read_csv(chexpert_file)\n",
    "df_patients = pd.read_csv(patients_file)\n",
    "\n",
    "# Check for duplicate subject_id in patients dataset\n",
    "df_patients_unique = df_patients[['subject_id', 'race']].drop_duplicates(subset=['subject_id'])\n",
    "\n",
    "# Verify uniqueness\n",
    "assert df_patients_unique.duplicated(subset=['subject_id']).sum() == 0, \"Duplicate subject_id found in patients dataset\"\n",
    "\n",
    "# Merge using 'subject_id' (left join to retain only rows in chexpert file)\n",
    "df_merged = df_chexpert.merge(df_patients_unique, on=\"subject_id\", how=\"left\")\n",
    "\n",
    "# Save the merged dataset\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "# Display the number of rows before and after merging\n",
    "print(f\"Original dataset size: {df_chexpert.shape[0]}\")\n",
    "print(f\"Merged dataset size: {df_merged.shape[0]}\")\n",
    "print(f\"Merged dataset saved as: {output_file}\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv('mimic_cxr_jpg_with_demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset['race'] = mimic_dataset['race'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df_cleaned.isnull().sum())  # Shows missing values per column\n",
    "print(df_cleaned.info())  # Checks data types\n",
    "print(df_cleaned.head())  # Displays sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_merged.dropna(subset=['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cleaned.to_csv(\"mimic_cxr_jpg_with_demographics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.read_csv('sampling_data_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = img[(img > 0) & (img < 255)]\n",
    "    hist = cv2.calcHist([img], [0], None, [254], [1, 255])\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_file = \"histogram_count_data.h5\"\n",
    "batch_size = 3000\n",
    "\n",
    "# Open HDF5 file for writing with variable-length data\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    dt = h5py.vlen_dtype(np.int16)  # Variable-length data type\n",
    "    f.create_dataset('hist', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "\n",
    "# Process and store amplitude spectra in batches\n",
    "for i in range(0, len(training_data_merge), batch_size):\n",
    "    batch_paths = training_data_merge['Path'][i:i + batch_size]\n",
    "\n",
    "    # Compute amplitude spectra for batch\n",
    "    batch_hist = [get_histogram(r'..//..//datasets/' + str(p)) for p in batch_paths]\n",
    "\n",
    "    # Append batch to HDF5 file\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        current_size = f['hist'].shape[0]\n",
    "        f['hist'].resize(current_size + len(batch_hist), axis=0)\n",
    "        f['hist'][current_size:] = batch_hist\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(len(training_data_merge) // batch_size) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files_by_race_group(file_path, output_dir, dataframe,  batch_size=1000, is_hist=True):\n",
    "    race_labels = dataframe['race'].values\n",
    "    race_dict = {}\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        for i in range(0, len(dataframe), batch_size):\n",
    "            batch_race_labels = race_labels[i:i + batch_size]\n",
    "            batch_data = f['hist'][i:i + batch_size] if is_hist else f['amplitude_spectrum'][i:i + batch_size]\n",
    "            for data, race in zip(batch_data, batch_race_labels):\n",
    "                if race not in race_dict:\n",
    "                    race_dict[race] = []\n",
    "                race_dict[race].append(data)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for race, data in race_dict.items():\n",
    "        race_file = os.path.join(output_dir, f\"{race}.h5\")\n",
    "        num_samples = len(data)\n",
    "        feature_size = len(data[0])\n",
    "\n",
    "        with h5py.File(race_file, 'w') as f:\n",
    "            dset = f.create_dataset(\n",
    "                \"hist\" if is_hist else \"amplitude_spectrum\",\n",
    "                shape=(0, feature_size),\n",
    "                maxshape=(None, feature_size),\n",
    "                dtype= np.int16 if is_hist else np.float32,\n",
    "                compression=\"gzip\"\n",
    "            )\n",
    "\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                batch = np.array(data[i : i + batch_size], dtype=np.int16 if is_hist else np.float32)\n",
    "                dset.resize(dset.shape[0] + batch.shape[0], axis=0)\n",
    "                dset[-batch.shape[0] :] = batch  \n",
    "                print(f\"Saved batch {i // batch_size + 1} for {race} ({dset.shape[0]} samples total)\")\n",
    "\n",
    "        print(f\"{race} data saved in {race_file} (Total samples: {num_samples})\")\n",
    "\n",
    "    print(\"Race-wise data saved in files successfully!\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(h5_file_path, batch_size=2000, is_hist=True):\n",
    "     mean = 0\n",
    "     with h5py.File(h5_file_path, 'r') as f:\n",
    "          num_samples = len(f['hist']) if is_hist else len(f['amplitude_spectrum'])\n",
    "          if num_samples < batch_size:\n",
    "               batch_size = num_samples\n",
    "          num_batch = num_samples / batch_size\n",
    "          for i in range(0, num_samples, batch_size):\n",
    "               batch = np.array(f['hist'][i : i + batch_size], dtype=np.float32) if is_hist else np.array(f['amplitude_spectrum'][i : i + batch_size], dtype=np.float32)\n",
    "               mean += np.mean(batch, axis=0)\n",
    "     \n",
    "     mean /= num_batch\n",
    "     return mean\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_difference(hist_A, hist_B):\n",
    "    return (hist_B - hist_A) / (hist_A + 1e-6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_race_group('histogram_count_data.h5', 'histograms', training_data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('histograms/Native American.h5', 'r') as f:\n",
    "     hist = f['hist'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hist:\n",
    "     print(i[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hist:\n",
    "     print(i[251:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_hist_mean = calculate_mean('histograms/Asian.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_hist_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_hist_mean = calculate_mean('histograms/White.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asian_specific = calculate_mean('histograms/Pacific Islander.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_hist_mean = calculate_mean('histograms/Other.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_hist_mean = calculate_mean('histograms/Black.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = relative_difference(white_hist_mean, Asian_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_american = calculate_mean('histograms/Native American.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = relative_difference(Asian_specific, unknown_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_mean = calculate_mean('histograms/Unknown.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_american"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(other_hist_mean, color='r', linewidth=2, alpha=0.4)\n",
    "plt.plot(white_hist_mean, color='y', linewidth=2, alpha=0.4)\n",
    "plt.plot(asian_hist_mean, color='g', linewidth=2, alpha=0.4)\n",
    "plt.plot(black_hist_mean, color='b', linewidth=2, alpha=0.4)\n",
    "plt.plot(native_american, color='purple', linewidth=2, alpha=0.4)\n",
    "plt.plot(Asian_specific, color='brown', linewidth=2, alpha=0.4)\n",
    "plt.plot(unknown_mean, color='white', linewidth=2, alpha=0.4)\n",
    "plt.xlabel(\"Pixel Intensity (0-255)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Grayscale Histogram\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_difference(relative_diff, title=\"Relative Difference in Histograms\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(relative_diff, label=\"Relative Difference\")\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.xlabel(\"Histogram Bin\")\n",
    "    plt.ylabel(\"Relative Difference\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eq_hist(path):\n",
    "     img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "     img = cv2.resize(img, (224,224))\n",
    "     img = img[(img > 0) & (img < 255)]\n",
    "     eq_img = exposure.equalize_hist(img, nbins=254)\n",
    "     hist, bins = np.histogram(eq_img, bins=254, range=(0, 1))\n",
    "     return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_file = \"equalize_hist_data.h5\"\n",
    "batch_size = 3000\n",
    "\n",
    "# Open HDF5 file for writing with variable-length data\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    dt = h5py.vlen_dtype(np.int16)  # Variable-length data type\n",
    "    f.create_dataset('hist', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "\n",
    "# Process and store amplitude spectra in batches\n",
    "for i in range(0, len(training_data_merge), batch_size):\n",
    "    batch_paths = training_data_merge['Path'][i:i + batch_size]\n",
    "\n",
    "    # Compute amplitude spectra for batch\n",
    "    batch_hist = [apply_eq_hist(r'..//..//datasets/' + str(p)) for p in batch_paths]\n",
    "\n",
    "    # Append batch to HDF5 file\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        current_size = f['hist'].shape[0]\n",
    "        f['hist'].resize(current_size + len(batch_hist), axis=0)\n",
    "        f['hist'][current_size:] = batch_hist\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(len(training_data_merge) // batch_size) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_race_group(\"equalize_hist_data.h5\", 'equalize_histograms', training_data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_adaptive_equalize_hist(path):\n",
    "     img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "     img = cv2.resize(img, (224,224))\n",
    "     img = img[(img > 0) & (img < 255)]\n",
    "     eq_img = exposure.equalize_adapthist(img, nbins=254)\n",
    "     hist, bins = np.histogram(eq_img, bins=254, range=(0, 1))\n",
    "     return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_file = \"equalize_adaptive_hist_data.h5\"\n",
    "batch_size = 3000\n",
    "\n",
    "# Open HDF5 file for writing with variable-length data\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    dt = h5py.vlen_dtype(np.int16)  # Variable-length data type\n",
    "    f.create_dataset('hist', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "\n",
    "# Process and store amplitude spectra in batches\n",
    "for i in range(0, len(training_data_merge), batch_size):\n",
    "    batch_paths = training_data_merge['Path'][i:i + batch_size]\n",
    "\n",
    "    # Compute amplitude spectra for batch\n",
    "    batch_hist = [apply_adaptive_equalize_hist(r'..//..//datasets/' + str(p)) for p in batch_paths]\n",
    "\n",
    "    # Append batch to HDF5 file\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        current_size = f['hist'].shape[0]\n",
    "        f['hist'].resize(current_size + len(batch_hist), axis=0)\n",
    "        f['hist'][current_size:] = batch_hist\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(len(training_data_merge) // batch_size) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_race_group(\"equalize_adaptive_hist_data.h5\", 'equalize_adaptive_histograms', training_data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_eq_mean, white_eq_mean = calculate_mean('equalize_histograms/Asian.h5'), calculate_mean('equalize_histograms/White.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_ada_eq_mean, white_ada_eq_mean = calculate_mean('equalize_adaptive_histograms/Asian.h5'), calculate_mean('equalize_adaptive_histograms/White.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_ada_eq_mean, black_eq_mean = calculate_mean('equalize_adaptive_histograms/Black.h5'), calculate_mean('equalize_histograms/Black.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_hist_mean = calculate_mean('histograms/Black.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_eq_diff = relative_difference(white_ada_eq_mean, black_ada_eq_mean)\n",
    "diff  = relative_difference(asian_hist_mean, black_hist_mean)\n",
    "eq_diff  = relative_difference(white_eq_mean, black_eq_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(eq_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(ada_eq_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = np.random.randint(len(training_data_merge['Path']), size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(len(random_numbers)):\n",
    "     img = cv2.imread(r'..//..//datasets/' + str(training_data_merge['Path'][random_numbers[i]]), cv2.IMREAD_GRAYSCALE)\n",
    "     images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "\n",
    "# Set the background color of the entire figure\n",
    "fig.patch.set_facecolor('#ADD8E6')  # Light blue background (use any valid color code)\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each subplot and display an image\n",
    "for i in range(100):\n",
    "    mask=images[i]\n",
    "    axes[i].imshow(mask, cmap='gray')\n",
    "    axes[i].axis('off')  # Hide axis labels\n",
    "\n",
    "    # Set background color for each subplot\n",
    "    axes[i].set_facecolor('#F5F5DC')  # Beige background for each subplot\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "\n",
    "# Set the background color of the entire figure\n",
    "fig.patch.set_facecolor('#ADD8E6')  # Light blue background (use any valid color code)\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each subplot and display an image\n",
    "for i in range(100):\n",
    "    mask = np.isin(images[i], [0,1,2,3,4,5,6,7,8,255])\n",
    "    axes[i].imshow(mask, cmap='gray')\n",
    "    axes[i].axis('off')  # Hide axis labels\n",
    "\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_ada_eq_mean = calculate_mean(\"equalize_adaptive_histograms/Black.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_ada_eq_mean, other_hist_mean, other_eq_mean = calculate_mean('equalize_adaptive_histograms/Other.h5'), calculate_mean('histograms/Other.h5'), calculate_mean('equalize_histograms/Other.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_hist_mean = (asian_hist_mean + black_hist_mean + white_hist_mean+other_hist_mean) / 4\n",
    "avg_eq_mean = (asian_eq_mean + black_eq_mean + white_eq_mean+other_eq_mean) / 4\n",
    "avg_ada_eq_mean = (asian_ada_eq_mean + black_ada_eq_mean + white_ada_eq_mean+other_ada_eq_mean) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('equalize_histograms/Other.h5', 'r') as f:\n",
    "     hist = f['hist'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))  # Wider figure for better spacing\n",
    "\n",
    "# Subplot 1: Histogram Means\n",
    "plt.subplot(1, 3, 1)  # (rows, columns, index)\n",
    "plt.plot(asian_hist_mean, color='r', linewidth=2, alpha=0.5, label='Asian Hist')\n",
    "plt.plot(black_hist_mean, color='b', linewidth=2, alpha=0.5, label='Black Hist')\n",
    "plt.plot(white_hist_mean, color='y', linewidth=2, alpha=0.5, label='White Hist')\n",
    "plt.plot(other_hist_mean, color='g', linewidth=2, alpha=0.5, label='Other Hist')\n",
    "plt.plot(avg_hist_mean, color='#000', linewidth=2, alpha=0.5, label='Average Hist')\n",
    "plt.title(\"Histogram Means\")\n",
    "plt.xlabel(\"Pixel Intensity (0-255)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Equalized Means\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(asian_eq_mean, color='r', linewidth=2, alpha=0.5, label='Asian EQ')\n",
    "plt.plot(black_eq_mean, color='b', linewidth=2, alpha=0.5, label='Black EQ')\n",
    "plt.plot(white_eq_mean, color='y', linewidth=2, alpha=0.5, label='White EQ')\n",
    "plt.plot(other_hist_mean, color='g', linewidth=2, alpha=0.5, label='Other EQ')\n",
    "plt.plot(avg_eq_mean, color='#000', linewidth=2, alpha=0.5, label='Average EQ')\n",
    "plt.title(\"Equalized Means\")\n",
    "plt.xlabel(\"Pixel Intensity (0-255)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 3: Adaptive Equalized Means\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(asian_ada_eq_mean, color='r', linewidth=2, alpha=0.5, label='Asian ADA EQ')\n",
    "plt.plot(black_ada_eq_mean, color='b', linewidth=2, alpha=0.5, label='Black ADA EQ')\n",
    "plt.plot(white_ada_eq_mean, color='y', linewidth=2, alpha=0.5, label='White ADA EQ')\n",
    "plt.plot(other_ada_eq_mean, color='g', linewidth=2, alpha=0.5, label='Other ADA EQ')\n",
    "plt.plot(avg_ada_eq_mean, color='#000', linewidth=2, alpha=0.5, label='Average ADA EQ')\n",
    "plt.title(\"Adaptive Equalized Means\")\n",
    "plt.xlabel(\"Pixel Intensity (0-255)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_diff = relative_difference(other_eq_mean, asian_eq_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(eq_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amplitude_spectrum(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (224,224))\n",
    "    tukey_window = tukey(image.shape[0], alpha=0.5)\n",
    "    tukey_2d = np.outer(tukey_window, tukey_window)\n",
    "    tukey_image = image * tukey_2d\n",
    "    tukey_image = (tukey_image - tukey_image.min()) / (tukey_image.max() - tukey_image.min())\n",
    "    tukey_image = tukey_image - tukey_image.mean()\n",
    "\n",
    "    f_image = np.fft.fft2(tukey_image)\n",
    "    f_shift = np.fft.fftshift(f_image)\n",
    "    amplitude_spectrum = np.abs(f_shift).astype(np.float32)\n",
    "    return amplitude_spectrum.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_file = \"amplitude_data.h5\"\n",
    "batch_size = 4000\n",
    "\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    f.create_dataset('amplitude_spectrum', shape=(0, 50176), maxshape=(None, 50176),\n",
    "                     dtype=np.float32, compression=\"gzip\", chunks=True)\n",
    "\n",
    "for i in range(0, len(training_data_merge), batch_size):\n",
    "    batch_paths = training_data_merge['Path'][i:i + batch_size]\n",
    "\n",
    "    batch_spectra = [get_amplitude_spectrum(r'..//..//datasets/' + str(p)) for p in batch_paths]\n",
    "\n",
    "    batch_spectra = np.vstack(batch_spectra)\n",
    "\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        f['amplitude_spectrum'].resize(f['amplitude_spectrum'].shape[0] + batch_spectra.shape[0], axis=0)\n",
    "        f['amplitude_spectrum'][-batch_spectra.shape[0]:] = batch_spectra\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(len(training_data_merge) // batch_size) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_race_group('amplitude_data.h5', 'spectrums', training_data_merge, 2000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_spec_mean, black_spec_mean = calculate_mean('spectrums/Asian.h5', is_hist=False), calculate_mean('spectrums/Black.h5', is_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_spec_mean , white_spec_mean = calculate_mean('spectrums/Other.h5', is_hist=False), calculate_mean('spectrums/White.h5', is_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_diff = relative_difference(other_spec_mean, black_spec_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(spec_diff.reshape(224,224), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "def pca_reduce_large(x_hdf5, y_hdf5, n_components, batchsize=None, normalize=True):\n",
    "    with h5py.File(x_hdf5, 'r') as x_file:\n",
    "        nfeat = x_file['amplitude_spectrum'].shape[1] \n",
    "        xlen = x_file['amplitude_spectrum'].shape[0]\n",
    "\n",
    "    with h5py.File(y_hdf5, 'r') as y_file:\n",
    "        assert y_file['amplitude_spectrum'].shape[1] == nfeat\n",
    "        ylen = y_file['amplitude_spectrum'].shape[0]\n",
    "\n",
    "    if batchsize is None:\n",
    "        batchsize = min(min(xlen, ylen), int(1e9 / 4 / (224 * 112) / 4))\n",
    "\n",
    "    target_feat_size = 224 * 112  \n",
    "\n",
    "    pca = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "    if normalize:\n",
    "        avg = np.zeros(target_feat_size)\n",
    "        var = np.zeros(target_feat_size)\n",
    "        \n",
    "        with h5py.File(x_hdf5, 'r') as x_file:\n",
    "            for chunk_idx in range(0, xlen, batchsize):\n",
    "                chunk = x_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "                avg += np.mean(chunk, axis=0) * chunk.shape[0] / (xlen + ylen)\n",
    "                var += np.var(chunk, axis=0) * chunk.shape[0] / (xlen + ylen)\n",
    "\n",
    "        with h5py.File(y_hdf5, 'r') as y_file:\n",
    "            for chunk_idx in range(0, ylen, batchsize):\n",
    "                chunk = y_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "                avg += np.mean(chunk, axis=0) * chunk.shape[0] / (xlen + ylen)\n",
    "                var += np.var(chunk, axis=0) * chunk.shape[0] / (xlen + ylen)\n",
    "        \n",
    "        std = np.sqrt(var)\n",
    "\n",
    "    with h5py.File(x_hdf5, 'r') as x_file:\n",
    "        for chunk_idx in range(0, xlen, batchsize):\n",
    "            chunk = x_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "            if normalize:\n",
    "                chunk = (chunk - avg) / std\n",
    "            pca.partial_fit(chunk)\n",
    "\n",
    "    with h5py.File(y_hdf5, 'r') as y_file:\n",
    "        for chunk_idx in range(0, ylen, batchsize):\n",
    "            chunk = y_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "            if normalize:\n",
    "                chunk = (chunk - avg) / std\n",
    "            pca.partial_fit(chunk)\n",
    "\n",
    "    x_transformed = np.zeros((xlen, n_components))\n",
    "    y_transformed = np.zeros((ylen, n_components))\n",
    "\n",
    "    with h5py.File(x_hdf5, 'r') as x_file:\n",
    "        for chunk_idx in range(0, xlen, batchsize):\n",
    "            chunk = x_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "            if normalize:\n",
    "                chunk = (chunk - avg) / std\n",
    "            x_transformed[chunk_idx:chunk_idx+batchsize, :] = pca.transform(chunk)\n",
    "\n",
    "    with h5py.File(y_hdf5, 'r') as y_file:\n",
    "        for chunk_idx in range(0, ylen, batchsize):\n",
    "            chunk = y_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "            if normalize:\n",
    "                chunk = (chunk - avg) / std\n",
    "            y_transformed[chunk_idx:chunk_idx+batchsize, :] = pca.transform(chunk)\n",
    "\n",
    "    print(f'Dim-reduced using incremental PCA from {nfeat} to {x_transformed.shape[1]} features to explain {pca.explained_variance_ratio_.sum():.4f}% of observed variance.')  \n",
    "\n",
    "    return x_transformed, y_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_transformed, asian_transformed = pca_reduce_large(\"race_wise_hdf5/Black.h5\", \"race_wise_hdf5/Asian.h5\", n_components=100, batchsize=3000, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True, fastmath=True)\n",
    "def mean_axis_0(x):\n",
    "    mean = np.zeros(x.shape[1], dtype=x.dtype)\n",
    "    x = x / x.shape[0]\n",
    "    for i in prange(x.shape[0]):\n",
    "        mean += x[i, :]\n",
    "\n",
    "    return mean \n",
    "\n",
    "\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def direct_covariance(x, mean_x):\n",
    "    n, p = x.shape\n",
    "    cov = np.zeros((p, p))\n",
    "    \n",
    "    for i in prange(p):\n",
    "        for j in range(i + 1):  # Use symmetry for efficiency\n",
    "            sum_val = 0.0\n",
    "            #for k in range(n):\n",
    "            #    sum_val += (x[k, i] - mean_x[i]) * (x[k, j] - mean_x[j])\n",
    "            #cov[i, j] = sum_val / n\n",
    "            cov[i, j] = np.sum((x[:, i] - mean_x[i]) * (x[:, j] - mean_x[j]))\n",
    "            if i != j:\n",
    "                cov[j, i] = cov[i, j]  # Fill the symmetric part\n",
    "    \n",
    "    return cov\n",
    "\n",
    "\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def cov(x, mean_x):\n",
    "    for ii in prange(x.shape[0]):\n",
    "        x[ii, :] -= mean_x\n",
    "    c = np.dot(x.T, x)  # this does recognize and exploit that the result will be symmetrical, see https://stackoverflow.com/a/43454451/2207840\n",
    "    c *= np.true_divide(1, x.shape[0] - 1)\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def hotelling_t2(x, y, bessel=True):\n",
    "    r\"\"\"hotelling_t2.\n",
    "\n",
    "    Compute the Hotelling (T2) test statistic.\n",
    "\n",
    "    It is the multivariate extension of the Student's t-test.\n",
    "    Test the null hypothesis that two multivariate samples have the same underlying\n",
    "    probability distribution, when specifying samples for x and y. The number of samples do not have\n",
    "    to be the same, but the number of features does have to be equal.\n",
    "\n",
    "    Equation:\n",
    "\n",
    "    Hotelling's t-squared statistic is defined as:\n",
    "\n",
    "    .. math::\n",
    "        T^2 = n (\\\\bar{x} - {\\mu})^{T} S^{-1} (\\\\bar{x} - {\\mu})\n",
    "\n",
    "    Where S is the pooled covariance matrix and ᵀ represents the transpose.\n",
    "\n",
    "    The two sample t-squared statistic is defined as:\n",
    "\n",
    "    .. math::\n",
    "        T^2 = (\\\\bar{x} - \\\\bar{y})^{T} [S(\\\\frac1 n_x +\\\\frac 1 n_y)]^{-1} (\\\\bar{x}̄ - \\\\bar{y})\n",
    "\n",
    "    References:\n",
    "        - Hotelling, Harold. (1931). The Generalization of Student's Ratio. Ann. Math. Statist. 2, no. 3, 360--378.\n",
    "          doi:10.1214/aoms/1177732979. https://projecteuclid.org/euclid.aoms/1177732979\n",
    "\n",
    "        - Hotelling, Harold. (1955) Les Rapports entre les Methodes Statistiques recentes portant sur des Variables Multiples\n",
    "          et l'Analyse Factorielle. 107-119.\n",
    "          In: L'Analyse Factorielle et ses Applications. Centre National de la Recherche Scientifique, Paris.\n",
    "\n",
    "        - Anderson T.W. (1992) Introduction to Hotelling (1931) The Generalization of Student’s Ratio.\n",
    "          In: Kotz S., Johnson N.L. (eds) Breakthroughs in Statistics.\n",
    "          Springer Series in Statistics (Perspectives in Statistics). Springer, New York, NY\n",
    "\n",
    "    :param x: array-like, samples of observations\n",
    "    :param y: array-like, samples of observations\n",
    "    :param bessel: bool, apply bessel correction (default)\n",
    "    :return:\n",
    "        statistic: float,\n",
    "            the t2 statistic\n",
    "        #f_value: float,\n",
    "        #    the f value\n",
    "        #p_value: float,\n",
    "        #    the p value\n",
    "        #s: 2d array,\n",
    "        #    the pooled variance\n",
    "    \"\"\"  # noqa: W605\n",
    "\n",
    "    nx, ny = x.shape[0], y.shape[0]\n",
    "\n",
    "    n1 = nx - 1 if bessel else nx\n",
    "    n2 = ny - 1 if bessel else ny\n",
    "\n",
    "    x_bar = mean_axis_0(x)\n",
    "    y_bar = mean_axis_0(y)\n",
    "    diff_bar = x_bar - y_bar\n",
    "\n",
    "    p = x.shape[1:]\n",
    "    p = p[0] if p else 1\n",
    "\n",
    "    #s1 = n1 * np.cov(x, rowvar=False).astype(np.float32)\n",
    "    #s2 = n2 * np.cov(y, rowvar=False).astype(np.float32)\n",
    "    #s1 = n1 * direct_covariance(x, x_bar)\n",
    "    #s2 = n2 * direct_covariance(y, y_bar)\n",
    "    s1 = n1 * cov(x, x_bar)\n",
    "    s2 = n2 * cov(y, y_bar)\n",
    "\n",
    "    pooled_cov = (s1 + s2) / (n1 + n2)\n",
    "\n",
    "    #if p > n1 + n2 - 2:\n",
    "        # For high-dimensional data, add regularization and compute pseudoinverse\n",
    "    #    inv_pooled_cov = np.linalg.pinv(pooled_cov + 1e-6 * np.eye(p))\n",
    "    #else:\n",
    "        #inv_pooled_cov = np.linalg.inv(pooled_cov)\n",
    "    #    inv_pooled_cov = np.linalg.solve(pooled_cov, np.identity(p))\n",
    "\n",
    "    #t2_stat = nx * ny / (nx + ny) * (diff_bar.T @ inv_pooled_cov @ diff_bar)\n",
    "\n",
    "    z = np.linalg.solve(pooled_cov, diff_bar)\n",
    "    t2_stat = nx * ny / (nx + ny) * (diff_bar.T @ z)\n",
    "    \n",
    "    return t2_stat\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def compute_null_distribution(xy, xlen, n_permut, test_statistic):\n",
    "    null_distribution = np.zeros((n_permut,))\n",
    "\n",
    "    for ii in prange(n_permut):\n",
    "        indices = np.random.permutation(xy.shape[0])\n",
    "\n",
    "        perm_x = xy[indices[:xlen], :]\n",
    "        perm_y = xy[indices[xlen:], :]\n",
    "\n",
    "        null_distribution[ii] = test_statistic(perm_x, perm_y)\n",
    "\n",
    "    return null_distribution\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def euclidean_distance(x, y):\n",
    "    \"\"\"Calculate Euclidean distance between mean spectra\"\"\"\n",
    "    mean1 = mean_axis_0(x)\n",
    "    mean2 = mean_axis_0(y)\n",
    "    return np.sqrt(np.sum((mean1 - mean2)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_differences(x, y, normalize=True, n_permut=999, test_statistic='hotelling'):\n",
    "\n",
    "    assert x.shape[1] == y.shape[1], \"x and y can have different numbers of samples but must have the same number of features\"\n",
    "\n",
    "    xlen, ylen = x.shape[0], y.shape[0]\n",
    "    nfeat = x.shape[1]\n",
    "\n",
    "    assert xlen > 1\n",
    "    assert ylen > 1\n",
    "\n",
    "    if nfeat > 1000 and test_statistic == 'hotelling':\n",
    "        warn(\"Dimensionality reduction is strongly advised for the hotelling statistic with very large feature space; your memory will explode.\")\n",
    "\n",
    "    x = x.astype(np.float32, copy=False)\n",
    "    y = y.astype(np.float32, copy=False)\n",
    "    xy = np.vstack((x, y))\n",
    "\n",
    "    if normalize:\n",
    "        print('Start normalization')\n",
    "        xy = (xy - np.mean(xy, axis=0)) / np.std(xy, axis=0)\n",
    "        x = xy[:xlen, :]\n",
    "        y = xy[xlen:, :]\n",
    "\n",
    "    if test_statistic == 'hotelling':\n",
    "        #test_statistic = lambda x, y: hotelling_t2(x, y)[0]\n",
    "        test_statistic = hotelling_t2\n",
    "    elif test_statistic == \"euclidean\":\n",
    "        test_statistic = euclidean_distance\n",
    "    else:\n",
    "        assert callable(test_statistic), \"test_statistic must be a callable with two arguments (the two samples) that returns a single number, the value of the statistic.\"\n",
    "\n",
    "    print('Calculating observed stat')\n",
    "    observed_stat = test_statistic(x, y)\n",
    "\n",
    "    print('Calculating null distribution')\n",
    "    null_distribution = compute_null_distribution(xy, xlen, n_permut, test_statistic)\n",
    "\n",
    "    pval = np.mean(np.abs(null_distribution) >= np.abs(observed_stat))\n",
    "\n",
    "    return pval, observed_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "print('Start')\n",
    "start = time()\n",
    "pval, stat = test_differences(asian_transformed, black_transformed, normalize=False)  # normalization not necessary since we already did that + PCA\n",
    "end = time()\n",
    "print(f\"Runtime with hotelling: {end-start:.2f}. Pval, stat:\")\n",
    "print([pval, stat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']] = training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']].replace(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx] / 255.0\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.read_csv('sampling_data_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_group = training_data_merge.groupby('race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = pd.DataFrame(race_group['Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_ethnicity(data, ethnicity_column):\n",
    "    \n",
    "    ethnicity_groups = {}\n",
    "\n",
    "    # Group by ethnicity\n",
    "    for ethnicity_name, group_data in data.groupby(ethnicity_column):\n",
    "        ethnicity_groups[ethnicity_name] = (group_data)\n",
    "\n",
    "    return ethnicity_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = training_data_merge[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_by_ethnicity(training_data_merge, 'ethnicity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Hispanic/Latino']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.ethnicity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups_data = {}\n",
    "for name, group_data in training_data_merge.groupby('ethnicity'):\n",
    "    images = []\n",
    "    paths = tqdm(group_data['Path'], desc=\"Loading images\")\n",
    "    for p in paths:\n",
    "        full_path = '//gaia/imageData/deep_learning/output/Sutariya/chexpert' + '/' + str(p)\n",
    "        img = read_image(full_path)\n",
    "        images.append(img)\n",
    "        paths.set_postfix({'Loaded': len(group_data['Path'])})\n",
    "    data_labels = group_data[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']].values\n",
    "    groups_data[name] = (images, torch.tensor(data_labels, dtype=torch.long))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_data['Non-Hispanic/Non-Latino'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_images['Non-Hispanic/Non-Latino'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnic = training_data_merge.ethnicity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_ethnic_images_labels(dataset, path, device):\n",
    "    \"\"\"\n",
    "    Loads images and processes ethnic labels.\n",
    "\n",
    "    Args:\n",
    "    - training_data_merge (DataFrame): Contains image paths and diagnostic labels.\n",
    "\n",
    "    Returns:\n",
    "    - data_images (List[Tensor]): List of image tensors.\n",
    "    - data_labels (Tensor): Tensor of diagnostic class labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        data_images = []\n",
    "        paths = tqdm(dataset['Path'], desc=\"Loading images\")\n",
    "        for path in paths:\n",
    "            full_path = '/deep_learning/output/Sutariya/chexpert' + '/' + str(path)\n",
    "            img = read_image(full_path)\n",
    "            data_images.append(img)\n",
    "            paths.set_postfix({'Loaded': len(data_images)})\n",
    "        torch.save(data_images, path)\n",
    "    else:\n",
    "        data_images = torch.load(path, map_location=device, weights_only=True)\n",
    "\n",
    "    data_labels = dataset[['Non-Hispanic/Non-Latino', 'Hispanic/Latino', 'Unknown',\n",
    "       'Patient Refused']].values\n",
    "    data_labels = torch.tensor(data_labels, dtype=torch.float32)\n",
    "    \n",
    "    return data_images, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\chexpert\\valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images = []\n",
    "for path in training_data_merge['Path']:\n",
    "     full_path = '../../datasets' + '/' + str(path)\n",
    "     img = read_image(full_path)\n",
    "     data_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data_merge['No Finding'].values\n",
    "data_labels = torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Lambda(lambda i: i/255),\n",
    "    transforms.Lambda(lambda i: i.to(torch.float32)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data_images,labels,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in dataloader:\n",
    "     print(i.shape)\n",
    "     print(l.shape)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_diagnostic_images_labels(training_data_merge):\n",
    "    data_images = []\n",
    "    paths = tqdm(training_data_merge['Path'], desc=\"Loading images\")\n",
    "    for path in paths:\n",
    "        full_path = '../../datasets' + '/' + str(path)\n",
    "        img = read_image(full_path)\n",
    "        data_images.append(img)\n",
    "        paths.set_postfix({'Loaded': len(data_images)})\n",
    "\n",
    "    data_labels = training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture']].values\n",
    "    data_labels = torch.tensor(data_labels, dtype=torch.float32)\n",
    "    data_labels = torch.argmax(data_labels, dim=1)\n",
    "\n",
    "    return data_images, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(data_images, labels,shuffle=True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop((256,256), scale=(0.7, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.Lambda(lambda i: i/255),\n",
    "        transforms.Lambda(lambda i: i.to(torch.float32)),\n",
    "        transforms.Normalize(mean=[0.50,0.50,0.50], std=[0.28,0.28,0.28])\n",
    "    ])\n",
    "\n",
    "    dataset = MyDataset(data_images,labels,transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=32, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv('../../datasets/train_clean_dataset.csv')\n",
    "validation_dataset = pd.read_csv('../../datasets/validation_clean_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_images,train_labels = store_diagnostic_images_labels(training_dataset)\n",
    "val_data_images, val_lables = store_diagnostic_images_labels(validation_dataset)\n",
    "train_loader = prepare_dataloaders(train_data_images,train_labels, shuffle=True)\n",
    "val_loader = prepare_dataloaders(val_data_images, val_lables,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store pixel values\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "for images, _ in dataloader:\n",
    "    # images shape: (batch_size, 1, 256 * 256)\n",
    "    batch_samples = images.size(0)  # Batch size\n",
    "    images = images.view(batch_samples, -1)  # Flatten pixels\n",
    "    mean += images.mean(dim=1).sum()\n",
    "    std += images.std(dim=1).sum()\n",
    "    num_samples += batch_samples\n",
    "\n",
    "# Final mean and std\n",
    "mean /= num_samples\n",
    "std /= num_samples\n",
    "\n",
    "print(f\"Mean: {mean.item():.4f}, Std: {std.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_races = training_data_merge['race'].value_counts().index[:5]\n",
    "training_data_merge['race'] = training_data_merge['race'].where(training_data_merge['race'].isin(top_5_races))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.get_dummies(training_data_merge, columns=['race'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data_merge[['race_Asian', 'race_Black', 'race_Other', 'race_Unknown', 'race_White']].values\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.argmax(labels, dim=1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Lambda(lambda x: x.to(torch.float32)),\n",
    "    transforms.Lambda(lambda i: i.repeat(3, 1, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet_Model(nn.Module):\n",
    "     def __init__(self, weights, out_feature):\n",
    "          super().__init__()\n",
    "          self.weight = weights\n",
    "          self.out_feature = out_feature\n",
    "          self.encoder = torchvision.models.densenet121(weights=weights)\n",
    "          self.layer1 = nn.Linear(1000, 120)\n",
    "          self.clf = nn.Linear(120, out_feature)\n",
    "\n",
    "     \n",
    "     def encode(self, x):\n",
    "          return self.encoder(x)\n",
    "\n",
    "     def forward(self, x):\n",
    "          z = self.encode(x)\n",
    "          z = self.layer1(z)\n",
    "          return self.clf(z)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet_Model(weights=None, out_feature=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torchvision.models.densenet121(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data_images,labels,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset)) \n",
    "val_size = int(0.10 * len(dataset))  \n",
    "test_size = len(dataset) - train_size - val_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, img in train_loader:\n",
    "     print(label.shape, img.shape)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_roc_auc(y_true, y_scores, log=True, log_name=\"roc_auc_curve\"):\n",
    "\n",
    "    y_scores = np.array(y_scores)\n",
    "    classes = np.unique(y_true) \n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    \n",
    "    total_roc = 0\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    \n",
    "    for i, class_label in enumerate(classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        total_roc += roc_auc\n",
    "        ax.plot(fpr, tpr, lw=2, label=f\"Class {class_label} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "    # Plot settings\n",
    "    ax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"Multi-Class ROC Curve\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Log to wandb\n",
    "    if log:\n",
    "        wandb.log({log_name: wandb.Image(fig)})\n",
    "    else:\n",
    "        print(f\"{log_name} : {total_roc/len(classes)}\")\n",
    "\n",
    "    # Close figure to free memory\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, train_loader, val_loader, num_epochs=10, device=None, is_binary=False):\n",
    "    model = model.to(device)\n",
    "\n",
    "    all_train_labels, all_train_preds = [], []\n",
    "    all_val_labels, all_val_preds = [], []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    early_stopper = EarlyStopper(patience=3)\n",
    "    criterion = nn.BCEWithLogitsLoss() if is_binary else nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\", leave=False)\n",
    "        for inputs, tr_labels in train_loop:\n",
    "            inputs, tr_labels = inputs.to(device), tr_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).to(device)\n",
    "\n",
    "            # Convert predictions & labels\n",
    "            if is_binary:\n",
    "                tr_labels = tr_labels.unsqueeze(dim=1)\n",
    "                tr_preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            else:\n",
    "                tr_preds = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            all_train_labels.extend(tr_labels.cpu().numpy())\n",
    "            all_train_preds.extend(tr_preds)\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, tr_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, vl_labels in val_loop:\n",
    "                inputs, vl_labels = inputs.to(device), vl_labels.to(device)\n",
    "\n",
    "                outputs = model(inputs).to(device)\n",
    "                if is_binary:\n",
    "                    vl_labels = vl_labels.unsqueeze(dim=1)\n",
    "                    vl_preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "                else:\n",
    "                    vl_preds = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "                all_val_labels.extend(vl_labels.cpu().numpy())\n",
    "                all_val_preds.extend(vl_preds)\n",
    "\n",
    "                loss = criterion(outputs, vl_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "        # Log metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        log_roc_auc(all_train_labels, all_train_preds, log=True, log_name='Training ROC-AUC')\n",
    "        log_roc_auc(all_val_labels, all_val_preds, log=True, log_name='Validation ROC-AUC')\n",
    "\n",
    "    # Final ROC-AUC Logging\n",
    "    log_roc_auc(all_train_labels, all_train_preds, log=False, log_name='Training ROC-AUC')\n",
    "    log_roc_auc(all_val_labels, all_val_preds, log=False, log_name='Validation ROC-AUC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('no_finding_model_weights.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_model = DenseNet_Model(weights=weights,out_feature=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in race_model.encoder.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training(race_model,train_loader,val_loader,num_epochs=10,is_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(race_model.state_dict(), 'race_finding_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = DenseNet_Model(weights=None, out_feature=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('race_finding_model_weights.pth',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_state_dict(weights,strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(test_loader, model, device, is_binary=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_test_labels, all_test_preds = [], []\n",
    "\n",
    "    loader = tqdm(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if is_binary:\n",
    "                labels = labels.view(-1, 1)\n",
    "                predicted = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            else:\n",
    "                predicted = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "            all_test_preds.extend(predicted)\n",
    "\n",
    "    log_roc_auc(all_test_labels, all_test_preds, log=True, log_name='Testing ROC-AUC')\n",
    "    log_roc_auc(all_test_labels, all_test_preds, log=False, log_name='Testing ROC-AUC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model(test_loader,test_model,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
