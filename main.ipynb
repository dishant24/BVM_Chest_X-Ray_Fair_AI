{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import binary_dilation\n",
    "from skimage.morphology import disk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\main\\chexpert\\dataset\\train_clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_races = data['race'].value_counts().index[:5]\n",
    "labels = top_races.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "     'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "     'Pneumothorax', 'Pleural Effusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No Finding',\n",
       " 'Enlarged Cardiomediastinum',\n",
       " 'Cardiomegaly',\n",
       " 'Lung Opacity',\n",
       " 'Lung Lesion',\n",
       " 'Edema',\n",
       " 'Consolidation',\n",
       " 'Pneumonia',\n",
       " 'Atelectasis',\n",
       " 'Pneumothorax',\n",
       " 'Pleural Effusion']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "labels.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demographic_data_path = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/admissions.csv.gz'\n",
    "all_dataset_path = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/mimic-cxr-2.0.0-chexpert.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_demographic_data(training_data, demographic_data):\n",
    "     \n",
    "     # Load the CSV files\n",
    "     df_chexpert = pd.read_csv(training_data, compression='gzip')\n",
    "     df_patients = pd.read_csv(demographic_data, compression='gzip')\n",
    "\n",
    "     # Check for duplicate subject_id in patients dataset\n",
    "     df_patients_unique = df_patients[['subject_id', 'race']].drop_duplicates(subset=['subject_id'])\n",
    "\n",
    "     # Verify uniqueness\n",
    "     assert df_patients_unique.duplicated(subset=['subject_id']).sum() == 0, \"Duplicate subject_id found in patients dataset\"\n",
    "\n",
    "     # Merge using 'subject_id' \n",
    "     df_merged = df_chexpert.merge(df_patients_unique, on=\"subject_id\", how=\"left\")\n",
    "     df_cleaned = df_merged.dropna(subset=['race'])\n",
    "\n",
    "     return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_merge = add_demographic_data(all_dataset_path, demographic_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_positive_sample(group):\n",
    "\n",
    "    disease_columns = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture'\n",
    "    ]\n",
    "    \n",
    "    group['positive_count'] = group[disease_columns].sum(axis=1)\n",
    "    \n",
    "    positive_cases = group[group['positive_count'] > 0]\n",
    "    \n",
    "    if not positive_cases.empty:\n",
    "\n",
    "        selected_sample = positive_cases.loc[positive_cases['positive_count'].idxmax()]\n",
    "    else:\n",
    "        selected_sample = group.sample(n=1).iloc[0]\n",
    "    \n",
    "    return selected_sample\n",
    "\n",
    "def sampling_datasets(training_dataset):\n",
    "\n",
    "    training_dataset = training_dataset.groupby('subject_id', group_keys=False).apply(select_most_positive_sample)\n",
    "    training_dataset.drop(columns=['positive_count'], inplace=True, errors='ignore')\n",
    "    training_dataset.reset_index(drop=True)\n",
    "    \n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_datasets(traning_dataset, is_chexpert=True):\n",
    "\n",
    "    traning_dataset[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']] = (traning_dataset[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']].fillna(0.0) == 1.0).astype(int)  # In The limits of fair medical imaging paper they treat uncertain label as negative and fill NA with 0.\n",
    "\n",
    "    \n",
    "    #Select only Frontal View \n",
    "    if is_chexpert:\n",
    "        traning_dataset = traning_dataset[traning_dataset['Frontal/Lateral'] == 'Frontal']\n",
    "\n",
    "    return traning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sutariya\\AppData\\Local\\Temp\\ipykernel_14836\\4273310855.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  training_dataset = training_dataset.groupby('subject_id', group_keys=False).apply(select_most_positive_sample)\n"
     ]
    }
   ],
   "source": [
    "total_data_clean = cleaning_datasets(total_data_merge, False)\n",
    "sampling_total_dataset = sampling_datasets(total_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test(df_group, disease, n_samples=0):\n",
    "    positives = df_group[df_group[disease] == 1]\n",
    "    print(f'Disease:{disease} have {len(positives)} values.')\n",
    "    sampled_pos = positives.sample(n=n_samples, random_state=42)\n",
    "\n",
    "    return sampled_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_data(dataset, N, split_by='race'):\n",
    "    test_dfs = []\n",
    "    diseases = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion']\n",
    "    # Select only top 5 values of the column\n",
    "    groups = dataset[split_by].value_counts().index[:5].values\n",
    "    print(groups)\n",
    "    df = dataset.copy()\n",
    "    train_group = df.groupby(split_by)\n",
    "    group_by_data = {}\n",
    "    for group, data in train_group:\n",
    "        group_by_data[group] = data\n",
    "    for group in groups:\n",
    "        df_group = group_by_data[group]\n",
    "        for disease in diseases:\n",
    "          print(f'Computed for {disease}...')\n",
    "          if len(df_group[disease]) < 20:\n",
    "              sampled_test = sample_test(df_group, disease, len(df_group[disease]))  \n",
    "          else:\n",
    "              sampled_test = sample_test(df_group, disease, N)  \n",
    "          test_dfs.append(sampled_test)\n",
    "\n",
    "    # Combine test samples\n",
    "    final_test_df = pd.concat(test_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    train_df = df[~df['subject_id'].isin(final_test_df['subject_id'].unique())].reset_index(drop=True) \n",
    "\n",
    "    # Save\n",
    "    final_test_df.to_csv('test_dataset.csv', index=False)\n",
    "    train_df.to_csv('train_dataset.csv', index=False)\n",
    "\n",
    "    print(\"✅ Done! Test shape:\", final_test_df.shape, train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset = sampling_total_dataset.drop(['Pleural Other', 'Fracture', 'Support Devices'], axis=1)\n",
    "sampling_total_dataset.loc[sampling_total_dataset['race'].str.startswith('WHITE'), 'race'] = 'WHITE'\n",
    "sampling_total_dataset.loc[sampling_total_dataset['race'].str.startswith('BLACK'), 'race'] = 'BLACK'\n",
    "sampling_total_dataset.loc[sampling_total_dataset['race'].str.startswith('ASIAN'), 'race'] = 'ASIAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000032</th>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000764</th>\n",
       "      <td>10000764</td>\n",
       "      <td>57375967</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000935</th>\n",
       "      <td>10000935</td>\n",
       "      <td>50578979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>BLACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000980</th>\n",
       "      <td>10000980</td>\n",
       "      <td>58206436</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BLACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001176</th>\n",
       "      <td>10001176</td>\n",
       "      <td>53186264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999156</th>\n",
       "      <td>19999156</td>\n",
       "      <td>50847545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999287</th>\n",
       "      <td>19999287</td>\n",
       "      <td>50574077</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BLACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999442</th>\n",
       "      <td>19999442</td>\n",
       "      <td>58497551</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999733</th>\n",
       "      <td>19999733</td>\n",
       "      <td>57132437</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999987</th>\n",
       "      <td>19999987</td>\n",
       "      <td>55368167</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51851 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject_id  study_id  Atelectasis  Cardiomegaly  Consolidation  \\\n",
       "subject_id                                                                   \n",
       "10000032      10000032  50414267            0             0              0   \n",
       "10000764      10000764  57375967            0             0              1   \n",
       "10000935      10000935  50578979            0             0              0   \n",
       "10000980      10000980  58206436            0             1              0   \n",
       "10001176      10001176  53186264            0             0              0   \n",
       "...                ...       ...          ...           ...            ...   \n",
       "19999156      19999156  50847545            0             0              0   \n",
       "19999287      19999287  50574077            1             0              1   \n",
       "19999442      19999442  58497551            1             0              0   \n",
       "19999733      19999733  57132437            0             0              0   \n",
       "19999987      19999987  55368167            1             0              0   \n",
       "\n",
       "            Edema  Enlarged Cardiomediastinum  Lung Lesion  Lung Opacity  \\\n",
       "subject_id                                                                 \n",
       "10000032        0                           0            0             0   \n",
       "10000764        0                           0            0             0   \n",
       "10000935        0                           0            0             0   \n",
       "10000980        1                           0            0             0   \n",
       "10001176        0                           0            0             1   \n",
       "...           ...                         ...          ...           ...   \n",
       "19999156        0                           0            0             0   \n",
       "19999287        0                           0            0             1   \n",
       "19999442        0                           0            0             1   \n",
       "19999733        0                           0            0             0   \n",
       "19999987        0                           0            0             0   \n",
       "\n",
       "            No Finding  Pleural Effusion  Pneumonia  Pneumothorax     race  \n",
       "subject_id                                                                  \n",
       "10000032             1                 0          0             0    WHITE  \n",
       "10000764             0                 0          0             0    WHITE  \n",
       "10000935             0                 1          1             0    BLACK  \n",
       "10000980             0                 1          0             0    BLACK  \n",
       "10001176             0                 0          0             0    WHITE  \n",
       "...                ...               ...        ...           ...      ...  \n",
       "19999156             1                 0          0             0    WHITE  \n",
       "19999287             0                 0          0             0    BLACK  \n",
       "19999442             0                 0          0             0    WHITE  \n",
       "19999733             1                 0          0             0    WHITE  \n",
       "19999987             0                 0          0             0  UNKNOWN  \n",
       "\n",
       "[51851 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WHITE' 'BLACK' 'OTHER' 'ASIAN' 'UNKNOWN']\n",
      "Computed for No Finding...\n",
      "Disease:No Finding have 12710 values.\n",
      "Computed for Enlarged Cardiomediastinum...\n",
      "Disease:Enlarged Cardiomediastinum have 1664 values.\n",
      "Computed for Cardiomegaly...\n",
      "Disease:Cardiomegaly have 7655 values.\n",
      "Computed for Lung Opacity...\n",
      "Disease:Lung Opacity have 9296 values.\n",
      "Computed for Lung Lesion...\n",
      "Disease:Lung Lesion have 1513 values.\n",
      "Computed for Edema...\n",
      "Disease:Edema have 4327 values.\n",
      "Computed for Consolidation...\n",
      "Disease:Consolidation have 1905 values.\n",
      "Computed for Pneumonia...\n",
      "Disease:Pneumonia have 3971 values.\n",
      "Computed for Atelectasis...\n",
      "Disease:Atelectasis have 8330 values.\n",
      "Computed for Pneumothorax...\n",
      "Disease:Pneumothorax have 1150 values.\n",
      "Computed for Pleural Effusion...\n",
      "Disease:Pleural Effusion have 8170 values.\n",
      "Computed for No Finding...\n",
      "Disease:No Finding have 4296 values.\n",
      "Computed for Enlarged Cardiomediastinum...\n",
      "Disease:Enlarged Cardiomediastinum have 317 values.\n",
      "Computed for Cardiomegaly...\n",
      "Disease:Cardiomegaly have 1859 values.\n",
      "Computed for Lung Opacity...\n",
      "Disease:Lung Opacity have 1927 values.\n",
      "Computed for Lung Lesion...\n",
      "Disease:Lung Lesion have 299 values.\n",
      "Computed for Edema...\n",
      "Disease:Edema have 848 values.\n",
      "Computed for Consolidation...\n",
      "Disease:Consolidation have 408 values.\n",
      "Computed for Pneumonia...\n",
      "Disease:Pneumonia have 904 values.\n",
      "Computed for Atelectasis...\n",
      "Disease:Atelectasis have 1608 values.\n",
      "Computed for Pneumothorax...\n",
      "Disease:Pneumothorax have 147 values.\n",
      "Computed for Pleural Effusion...\n",
      "Disease:Pleural Effusion have 1293 values.\n",
      "Computed for No Finding...\n",
      "Disease:No Finding have 901 values.\n",
      "Computed for Enlarged Cardiomediastinum...\n",
      "Disease:Enlarged Cardiomediastinum have 91 values.\n",
      "Computed for Cardiomegaly...\n",
      "Disease:Cardiomegaly have 432 values.\n",
      "Computed for Lung Opacity...\n",
      "Disease:Lung Opacity have 501 values.\n",
      "Computed for Lung Lesion...\n",
      "Disease:Lung Lesion have 91 values.\n",
      "Computed for Edema...\n",
      "Disease:Edema have 220 values.\n",
      "Computed for Consolidation...\n",
      "Disease:Consolidation have 104 values.\n",
      "Computed for Pneumonia...\n",
      "Disease:Pneumonia have 227 values.\n",
      "Computed for Atelectasis...\n",
      "Disease:Atelectasis have 486 values.\n",
      "Computed for Pneumothorax...\n",
      "Disease:Pneumothorax have 64 values.\n",
      "Computed for Pleural Effusion...\n",
      "Disease:Pleural Effusion have 414 values.\n",
      "Computed for No Finding...\n",
      "Disease:No Finding have 806 values.\n",
      "Computed for Enlarged Cardiomediastinum...\n",
      "Disease:Enlarged Cardiomediastinum have 84 values.\n",
      "Computed for Cardiomegaly...\n",
      "Disease:Cardiomegaly have 393 values.\n",
      "Computed for Lung Opacity...\n",
      "Disease:Lung Opacity have 489 values.\n",
      "Computed for Lung Lesion...\n",
      "Disease:Lung Lesion have 122 values.\n",
      "Computed for Edema...\n",
      "Disease:Edema have 207 values.\n",
      "Computed for Consolidation...\n",
      "Disease:Consolidation have 93 values.\n",
      "Computed for Pneumonia...\n",
      "Disease:Pneumonia have 199 values.\n",
      "Computed for Atelectasis...\n",
      "Disease:Atelectasis have 390 values.\n",
      "Computed for Pneumothorax...\n",
      "Disease:Pneumothorax have 72 values.\n",
      "Computed for Pleural Effusion...\n",
      "Disease:Pleural Effusion have 389 values.\n",
      "Computed for No Finding...\n",
      "Disease:No Finding have 383 values.\n",
      "Computed for Enlarged Cardiomediastinum...\n",
      "Disease:Enlarged Cardiomediastinum have 123 values.\n",
      "Computed for Cardiomegaly...\n",
      "Disease:Cardiomegaly have 525 values.\n",
      "Computed for Lung Opacity...\n",
      "Disease:Lung Opacity have 667 values.\n",
      "Computed for Lung Lesion...\n",
      "Disease:Lung Lesion have 42 values.\n",
      "Computed for Edema...\n",
      "Disease:Edema have 366 values.\n",
      "Computed for Consolidation...\n",
      "Disease:Consolidation have 167 values.\n",
      "Computed for Pneumonia...\n",
      "Disease:Pneumonia have 235 values.\n",
      "Computed for Atelectasis...\n",
      "Disease:Atelectasis have 606 values.\n",
      "Computed for Pneumothorax...\n",
      "Disease:Pneumothorax have 108 values.\n",
      "Computed for Pleural Effusion...\n",
      "Disease:Pleural Effusion have 605 values.\n",
      "✅ Done! Test shape: (1051, 14) (50800, 14)\n"
     ]
    }
   ],
   "source": [
    "split_train_test_data(sampling_total_dataset, 20, 'race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r\"E:\\Thesis\\CXR_Preprocessing\\cxr_preprocessing\\test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1046    0\n",
       "1047    0\n",
       "1048    0\n",
       "1049    1\n",
       "1050    0\n",
       "Name: Enlarged Cardiomediastinum, Length: 1051, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Enlarged Cardiomediastinum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset[sampling_total_dataset['race'].str.startswith(('WHITE', 'BLACK', 'ASIAN'))]['race'].map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset.loc[sampling_total_dataset['race'].str.startswith('WHITE'), 'race'] = 'WHITE'\n",
    "sampling_total_dataset.loc[sampling_total_dataset['race'].str.startswith('BLACK'), 'race'] = 'BLACK'\n",
    "sampling_total_dataset.loc[sampling_total_dataset['race'].str.startswith('ASIAN'), 'race'] = 'ASIAN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(r\"\\\\gaia\\imageData\\public\\MIMIC-CXR\\physionet.org\\files\\mimic-cxr-jpg\\2.1.0\\mimic-cxr-2.0.0-metadata.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_data[['subject_id', 'study_id', 'ViewPosition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_data.reset_index(drop=True)\n",
    "sampling_total_dataset = sampling_total_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset = pd.merge(sampling_total_dataset, meta_data, how='inner', on=['subject_id', 'study_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset = sampling_total_dataset.drop_duplicates(['subject_id', 'study_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset = sampling_total_dataset[sampling_total_dataset.ViewPosition.isin(['AP','PA'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset['Pleural Other'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_test(df_group, disease, n_samples=0):\n",
    "    positives = df_group[df_group[disease] == 1]\n",
    "    print(f'Disease:{disease} have {len(positives)} values.')\n",
    "    sampled_pos = positives.sample(n=n_samples, random_state=42)\n",
    "\n",
    "    return sampled_pos\n",
    "\n",
    "def split_train_test_data(dataset, N, split_by='race'):\n",
    "    test_dfs = []\n",
    "    diseases = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices'\n",
    "    ]\n",
    "    # Select only top 5 values of the column\n",
    "    groups = dataset[split_by].value_counts().index[:5].values\n",
    "    print(groups)\n",
    "    df = dataset.copy()\n",
    "    train_group = df.groupby(split_by)\n",
    "    group_by_data = {}\n",
    "    for group, data in train_group:\n",
    "        group_by_data[group] = data\n",
    "    for group in groups:\n",
    "        df_group = group_by_data[group]\n",
    "        for disease in diseases:\n",
    "            print(f'Computed for {disease}...')\n",
    "            sampled_test = sample_test(df_group, disease, N)\n",
    "            test_dfs.append(sampled_test)\n",
    "\n",
    "    # Combine test samples\n",
    "    final_test_df = pd.concat(test_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    train_df = df[~df['subject_id'].isin(final_test_df['subject_id'].unique())].reset_index(drop=True) \n",
    "\n",
    "    # Save\n",
    "    final_test_df.to_csv('test_dataset.csv', index=False)\n",
    "    train_df.to_csv('train_dataset.csv', index=False)\n",
    "\n",
    "    print(\"✅ Done! Test shape:\", final_test_df.shape, train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_total_dataset['Pleural Other'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = sampling_total_dataset['race'].value_counts().index[:5].values\n",
    "train_group = sampling_total_dataset.groupby('race')\n",
    "group_by_data = {}\n",
    "feature_counts = []\n",
    "for label in labels:\n",
    "    per_disease = {}\n",
    "    for group, data in train_group:\n",
    "        group_by_data[group] = data\n",
    "    for group in groups:\n",
    "        df_group = group_by_data[group]\n",
    "        feature_counts.append({\n",
    "            'disease': label,\n",
    "            'positive': df_group[label].value_counts().values[1],\n",
    "            'race': group\n",
    "        })\n",
    "\n",
    "count_data = pd.DataFrame(feature_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data.to_csv('per_race_disease_positive_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframe(training_data, demographic_data):\n",
    "    path = training_data['Path']\n",
    "    patientid = []\n",
    "    for i in path:\n",
    "        id = i.split(sep='/')[2]\n",
    "        id = id.replace(\"patient\", \"\")\n",
    "        patientid.append(float(id))\n",
    "\n",
    "    temp_patient = pd.DataFrame(patientid,columns=['patient_id'])\n",
    "    training_data = training_data.reset_index(drop=True)\n",
    "    training_data['subject_id'] = temp_patient['patient_id']\n",
    "    training_data_merge = training_data.merge(demographic_data, on='subject_id')\n",
    "    return training_data_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(training_file_path)\n",
    "demographic_data = pd.read_csv(demographic_data_path)\n",
    "training_data_merge = merge_dataframe(training_data, demographic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test(df_group, disease, n_samples=0):\n",
    "    positives = df_group[df_group[disease] == 1]\n",
    "\n",
    "    sampled_pos = positives.sample(n=n_samples, random_state=42)\n",
    "\n",
    "    return sampled_pos\n",
    "\n",
    "def split_train_test_data(dataset, N, split_by='race'):\n",
    "    test_dfs = []\n",
    "    diseases = [\n",
    "        \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\",\n",
    "        \"Enlarged Cardiomediastinum\", \"Fracture\", \"Lung Lesion\",\n",
    "        \"Lung Opacity\", \"No Finding\", \"Pleural Effusion\", \"Pleural Other\",\n",
    "        \"Pneumonia\", \"Pneumothorax\", 'Support Devices'\n",
    "    ]\n",
    "    # Select only top 3 values of the column\n",
    "    groups = dataset[split_by].value_counts().index[:5].values\n",
    "    df = dataset.copy()\n",
    "    train_group = df.groupby(split_by)\n",
    "    group_by_data = {}\n",
    "    for group, data in train_group:\n",
    "        group_by_data[group] = data\n",
    "    for group in groups:\n",
    "        df_ethnicity = group_by_data[group]\n",
    "        for disease in diseases:\n",
    "            sampled_test = sample_test(df_ethnicity, disease, N)\n",
    "            test_dfs.append(sampled_test)\n",
    "\n",
    "    # Combine test samples\n",
    "    final_test_df = pd.concat(test_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    train_df = df[~df['subject_id'].isin(final_test_df['subject_id'].unique())].reset_index(drop=True) \n",
    "\n",
    "    # Save\n",
    "    final_test_df.to_csv('test_dataset.csv', index=False)\n",
    "    train_df.to_csv('train_dataset.csv', index=False)\n",
    "\n",
    "    print(\"✅ Done! Test shape:\", final_test_df.shape, train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_test_data(training_data_merge, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image in grayscale\n",
    "image = cv2.imread('//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/'+data['file_path'][1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply CLAHE\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
    "clahe_image = clahe.apply(image)\n",
    "\n",
    "# Plot original and CLAHE-enhanced image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"CLAHE Image\")\n",
    "plt.imshow(clahe_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image in grayscale\n",
    "image = cv2.imread('//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/'+data['file_path'][1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Convert to float32 for precision\n",
    "image_float = image.astype(np.float32)\n",
    "\n",
    "# Compute mean and std per image\n",
    "mean = np.mean(image_float)\n",
    "std = np.std(image_float)\n",
    "\n",
    "# Avoid division by zero\n",
    "if std == 0:\n",
    "    std = 1e-8\n",
    "\n",
    "# Normalize to zero mean and unit variance\n",
    "normalized_image = (image_float - mean) / std\n",
    "\n",
    "\n",
    "# Plot original and normalized image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Normaliza Image\")\n",
    "plt.imshow(normalized_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyLungMask:\n",
    "    def __init__(self, left_rle, right_rle, heart_rle, margin_radius=20, original_shape=(1024, 1024), image_shape=(512, 512)):\n",
    "        self.left_rle = left_rle\n",
    "        self.right_rle = right_rle\n",
    "        self.heart_rle = heart_rle\n",
    "        self.margin_radius = margin_radius\n",
    "        self.original_shape = original_shape\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    def decode_rle(self, rle_str):\n",
    "        if isinstance(rle_str, pd.Series):\n",
    "            rle_str = rle_str.iloc[0]  \n",
    "        if pd.isna(rle_str):\n",
    "            return np.zeros(self.original_shape, dtype=np.uint8)\n",
    "        \n",
    "        s = list(map(int, rle_str.strip().split()))\n",
    "        starts, lengths = s[0::2], s[1::2]\n",
    "        flat_mask = np.zeros(self.original_shape[0] * self.original_shape[1], dtype=np.uint8)\n",
    "        for start, length in zip(starts, lengths):\n",
    "            flat_mask[start:start + length] = 1\n",
    "        return flat_mask.reshape(self.original_shape)\n",
    "\n",
    "\n",
    "    def dilate_mask(self, mask):\n",
    "        selem = disk(self.margin_radius)\n",
    "        return binary_dilation(mask, structure=selem).astype(np.uint8)\n",
    "\n",
    "    def resize_mask(self, mask):\n",
    "        mask_img = Image.fromarray(mask.astype(np.uint8) * 255)\n",
    "        mask_resized = mask_img.resize((self.image_shape[1], self.image_shape[0]), resample=Image.NEAREST)\n",
    "        return np.array(mask_resized) // 255\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = F.to_pil_image(image)\n",
    "\n",
    "        image_resized = image.resize(self.image_shape[::-1], Image.BILINEAR)\n",
    "        image_np = np.array(image_resized)\n",
    "\n",
    "        left_mask = self.decode_rle(self.left_rle)\n",
    "        right_mask = self.decode_rle(self.right_rle)\n",
    "        heart_mask = self.decode_rle(self.heart_rle)\n",
    "        \n",
    "\n",
    "        left_mask = self.dilate_mask(left_mask)\n",
    "        right_mask = self.dilate_mask(right_mask)\n",
    "        heart_mask = self.dilate_mask(heart_mask)\n",
    "\n",
    "        combined_mask = left_mask + right_mask + heart_mask\n",
    "        combined_mask = np.clip(combined_mask, 0, 1)\n",
    "\n",
    "        combined_mask = self.resize_mask(combined_mask)\n",
    "        masked_image = image_np * combined_mask\n",
    "\n",
    "        return Image.fromarray(masked_image.astype(np.uint8))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\main\\mimic\\dataset\\train_mask_clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Select 10 random rows\n",
    "sampled_data = data.sample(n=10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Plotting setup\n",
    "fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(12, 40))\n",
    "fig.suptitle(\"Original Image | Mask | Mask with Margin\", fontsize=16, y=1.02)\n",
    "\n",
    "for i, row in sampled_data.iterrows():\n",
    "    try:\n",
    "        # Load image\n",
    "        path = os.path.join('//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/', row['file_path'])\n",
    "        image = Image.open(path).convert('L')\n",
    "\n",
    "        # Generate masks\n",
    "        left_rle = row['Left Lung']\n",
    "        right_rle = row['Right Lung']\n",
    "        heart_rle = row['Heart']\n",
    "        masker_margin = ApplyLungMask(left_rle, right_rle, heart_rle, margin_radius=50)\n",
    "        masker_nomargin = ApplyLungMask(left_rle, right_rle, heart_rle, margin_radius=0)\n",
    "\n",
    "        mask_image = masker_nomargin(image)\n",
    "        mask_margin_image = masker_margin(image)\n",
    "\n",
    "        # Plot Original Image\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title('Original Image')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Plot Mask\n",
    "        axes[i, 1].imshow(mask_image, cmap='gray')\n",
    "        axes[i, 1].set_title('Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # Plot Mask with Margin\n",
    "        axes[i, 2].imshow(mask_margin_image, cmap='gray')\n",
    "        axes[i, 2].set_title('Mask with Margin')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {row['file_path']}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = pd.DataFrame(data, columns=['subject_id', 'study_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths['study_id'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths['subject_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\main\\mimic\\dataset\\testing_clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\main\\mimic\\dataset\\train_clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_mimic_jpg = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\main\\mimic\\dataset\\MASK-MIMIC-CXR-JPG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_mimic_jpg = pd.read_csv(r\"\\\\gaia\\imageData\\public\\MIMIC-CXR\\physionet.org\\files\\mimic-cxr-jpg\\2.1.0\\mimic-cxr-2.0.0-metadata.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_mimic_jpg['dicom_id'].isin(metadata_mimic_jpg['dicom_id']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_mimic_jpg['dicom_id'].isin(mask_mimic_jpg['dicom_id']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([data['file_path'], t_data['file_path']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define your data transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a standard size (e.g., 224x224)\n",
    "    transforms.ToTensor(),          # Convert image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "# Path to your MIMIC-CXR-JPG dataset\n",
    "dataset_dir = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/'\n",
    "\n",
    "# Collect all the image file paths\n",
    "image_paths = [os.path.join(dataset_dir, fname) for fname in merge if fname.endswith('.jpg')]\n",
    "\n",
    "# Initialize lists to collect pixel values\n",
    "all_pixels = []\n",
    "\n",
    "# Load images and collect pixel data\n",
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = transform(img)  # Apply the transformations\n",
    "    all_pixels.append(img)\n",
    "\n",
    "# Convert all collected pixel data into a single tensor\n",
    "all_pixels = torch.stack(all_pixels)\n",
    "\n",
    "# Compute the mean and std\n",
    "mean = torch.mean(all_pixels, dim=(0, 2, 3))  # Mean across batch, height, and width\n",
    "std = torch.std(all_pixels, dim=(0, 2, 3))    # Std across batch, height, and width\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunksize = 10000\n",
    "all_mask_data = []\n",
    "\n",
    "file_path = r\"\\\\gaia\\imageData\\deep_learning\\input\\data\\chexmask\\chexmask-database-a-large-scale-dataset-of-anatomical-segmentation-masks-for-chest-x-ray-images-1.0.0\\Preprocessed\\MIMIC-CXR-JPG.csv\"\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    all_mask_data.extend(chunk[['dicom_id', 'Left Lung', 'Right Lung', 'Landmarks']].to_dict('records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df = pd.DataFrame(all_mask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"\\\\gaia\\imageData\\deep_learning\\input\\data\\chexmask\\chexmask-database-a-large-scale-dataset-of-anatomical-segmentation-masks-for-chest-x-ray-images-1.0.0\\Preprocessed\\MIMIC-CXR-JPG.csv\"\n",
    "chunksize = 1000\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    mask_data = chunk\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data['Landmarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['file_path'].str.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_id = []\n",
    "for path in data['file_path'].str.split('/'):\n",
    "     dicom_id.append(path[-1][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_df = pd.DataFrame(dicom_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat([data, dicom_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.rename(columns={0:'dicom_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dicom_id = mask_df[mask_df['dicom_id'].isin(dicom_id)]['dicom_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_contain_datset = new_data[new_data['dicom_id'].isin(mask_dicom_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mask_dataset = pd.merge(mask_contain_datset, mask_df, how='inner', on='dicom_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mask_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation\n",
    "from skimage.morphology import disk\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def decode_rle(rle_str, shape=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Decode RLE string to binary mask.\n",
    "    \"\"\"\n",
    "    s = list(map(int, rle_str.strip().split()))\n",
    "    starts, lengths = s[0::2], s[1::2]\n",
    "    flat_mask = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for start, length in zip(starts, lengths):\n",
    "        flat_mask[start:start + length] = 1\n",
    "    return flat_mask.reshape(shape)\n",
    "\n",
    "\n",
    "def dilate_mask(mask, margin_radius):\n",
    "    selem = disk(margin_radius)\n",
    "    return binary_dilation(mask, structure=selem).astype(np.uint8)\n",
    "\n",
    "def resize_mask(mask, new_shape):\n",
    "    \"\"\"\n",
    "    Resize mask using nearest neighbor (no interpolation).\n",
    "    \"\"\"\n",
    "    mask_img = Image.fromarray(mask.astype(np.uint8) * 255)\n",
    "    mask_resized = mask_img.resize((new_shape[1], new_shape[0]), resample=Image.NEAREST)\n",
    "    return np.array(mask_resized) // 255  # back to binary\n",
    "\n",
    "\n",
    "def rescale_landmarks(landmarks, from_shape, to_shape):\n",
    "    \"\"\"\n",
    "    Rescale landmark coordinates from one image shape to another.\n",
    "    \"\"\"\n",
    "    scale_x = to_shape[1] / from_shape[1]\n",
    "    scale_y = to_shape[0] / from_shape[0]\n",
    "    return [(int(x * scale_x), int(y * scale_y)) for x, y in landmarks]\n",
    "\n",
    "\n",
    "\n",
    "def parse_landmarks(landmark_str):\n",
    "    try:\n",
    "        coords = list(map(int, landmark_str.strip().split(',')))\n",
    "        if len(coords) % 2 != 0:\n",
    "            raise ValueError(\"Uneven number of coordinates in landmark string\")\n",
    "        landmarks = [(coords[i], coords[i + 1]) for i in range(0, len(coords), 2)]\n",
    "        return landmarks\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse landmarks:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "mask_df = data\n",
    "row = mask_df.iloc[11]\n",
    "\n",
    "# === Step 3: Load image ===\n",
    "image_path = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0/' + row['file_path']\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "print(image.shape)\n",
    "h, w = image.shape\n",
    "original_shape = (1024, 1024)\n",
    "actual_shape = image.shape \n",
    "\n",
    "left_mask = decode_rle(row['Left Lung'], shape=(1024, 1024))\n",
    "right_mask = decode_rle(row['Right Lung'], shape=(1024, 1024))\n",
    "\n",
    "left_mask_dilated = dilate_mask(left_mask, 60)   # smaller margin\n",
    "right_mask_dilated = dilate_mask(right_mask, 60) # bigger margin\n",
    "\n",
    "mask_combined = np.clip(left_mask_dilated + right_mask_dilated, 0, 1)\n",
    "mask_resized = resize_mask(mask_combined, image.shape)\n",
    "masked_image = image * mask_resized\n",
    "\n",
    "# Apply the mask to image\n",
    "masked_image = image * mask_resized\n",
    "\n",
    "\n",
    "# Plot everything\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image with Landmarks\")\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Masked Image (Lungs)\")\n",
    "plt.imshow(masked_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "        \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, base_dir='//gaia/imageData/deep_learning/output/Sutariya/chexpert/'):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.base_dir, self.image_paths[idx])\n",
    "        image = Image.open(path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32) \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def prepare_dataloaders(image_paths, labels, shuffle=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Lambda(lambda i: i.repeat(3, 1, 1) if i.shape[0] == 1 else i),\n",
    "        transforms.Lambda(lambda i: i/255),\n",
    "        transforms.Normalize(mean=[0.5062, 0.5062, 0.5062], std=[0.2873, 0.2873, 0.2873]), # Adapt to own standard deviation and mean to Chexpert\n",
    "        transforms.Lambda(lambda i: i.to(torch.float32)),\n",
    "        transforms.RandomResizedCrop((224,224), scale=(0.6, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(contrast=(0.7, 1.2)) # Randomly change the brightness, contrast, saturation and hue of an image\n",
    "    ])\n",
    "\n",
    "    dataset = MyDataset(image_paths, labels, transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda i: i.repeat(3, 1, 1) if i.shape[0] == 1 else i),\n",
    "    transforms.Normalize(mean=[0.5062]*3, std=[0.2873]*3),\n",
    "    transforms.RandomResizedCrop(\n",
    "        (200, 200),\n",
    "        scale=(0.9, 1.0),               \n",
    "        ratio=(0.9, 1.1),              \n",
    "        interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomVerticalFlip(0.3),\n",
    "    transforms.RandomRotation(degrees=10)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_chexpert = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\chexpert\\train_clean_dataset.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
    "data = MyDataset(training_dataset_chexpert['Path'], training_dataset_chexpert[labels].values, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = MyDataset(training_dataset_chexpert['Path'], training_dataset_chexpert[labels].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbl = data.__getitem__(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, lbl1 = data1.__getitem__(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in train_loader:\n",
    "     print(img, label)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_chexpert['Pneumonia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\chexpert\\valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_positive_sample(group):\n",
    "\n",
    "    disease_columns = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture'\n",
    "    ]\n",
    "    \n",
    "    group['positive_count'] = group[disease_columns].sum(axis=1)\n",
    "    \n",
    "    positive_cases = group[group['positive_count'] > 0]\n",
    "    \n",
    "    if not positive_cases.empty:\n",
    "\n",
    "        selected_sample = positive_cases.loc[positive_cases['positive_count'].idxmax()]\n",
    "    else:\n",
    "        selected_sample = group.sample(n=1).iloc[0]\n",
    "    \n",
    "    return selected_sample\n",
    "\n",
    "\n",
    "# Select the single subject_id per patient which has most positive disease \n",
    "def sampling_datasets(training_dataset):\n",
    "\n",
    "    training_dataset = training_dataset.groupby('subject_id', group_keys=False).apply(select_most_positive_sample)\n",
    "    training_dataset.drop(columns=['positive_count'], inplace=True, errors='ignore')\n",
    "    \n",
    "    return training_dataset\n",
    "\n",
    "\n",
    "def cleaning_datasets(traning_dataset):\n",
    "\n",
    "    traning_dataset[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']] = (traning_dataset[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']].fillna(0.0) == 1.0).astype(int)  # In The limits of fair medical imaging paper they treat uncertain label as negative and fill NA with 0.\n",
    "\n",
    "    #Select only Frontal View \n",
    "    traning_dataset = traning_dataset[traning_dataset['Frontal/Lateral'] == 'Frontal']\n",
    "\n",
    "    return traning_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframe(training_data, demographic_data):\n",
    "    path = training_data['Path']\n",
    "    patientid = []\n",
    "    for i in path:\n",
    "        id = i.split(sep='/')[2]\n",
    "        id = id.replace(\"patient\", \"\")\n",
    "        patientid.append(float(id))\n",
    "\n",
    "    temp_patient = pd.DataFrame(patientid,columns=['patient_id'])\n",
    "    training_data = training_data.reset_index(drop=True)\n",
    "    training_data['subject_id'] = temp_patient['patient_id']\n",
    "    training_data_merge = training_data.merge(demographic_data, on='subject_id')\n",
    "    return training_data_merge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\chexpert\\demographics_CXP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = merge_dataframe(training_data, demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_merge_data = cleaning_datasets(merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_merge_data = sampling_datasets(clean_merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_merge_data['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ethnic_group = sampling_merge_data.groupby('ethnicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_data = {}\n",
    "for group, data in train_ethnic_group:\n",
    "     group_by_data[group] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = 60 \n",
    "ethnicity_col = 'Ethnicity'\n",
    "diseases = [\n",
    "    \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\",\n",
    "    \"Enlarged Cardiomediastinum\", \"Fracture\", \"Lung Lesion\",\n",
    "    \"Lung Opacity\", \"No Finding\", \"Pleural Effusion\", \"Pleural Other\",\n",
    "    \"Pneumonia\", \"Pneumothorax\", 'Support Devices'\n",
    "]\n",
    "ethnic_groups = [\"Non-Hispanic/Non-Latino\", \"Hispanic/Latino\"]\n",
    "\n",
    "df = sampling_merge_data\n",
    "\n",
    "# Helper to sample for test set\n",
    "def sample_test(df_group, disease, n_samples=N):\n",
    "    positives = df_group[df_group[disease] == 1]\n",
    "\n",
    "    sampled_pos = positives.sample(n=n_samples, random_state=42)\n",
    "\n",
    "    return sampled_pos\n",
    "\n",
    "# Sample the test set\n",
    "test_dfs = []\n",
    "\n",
    "for ethnicity in ethnic_groups:\n",
    "    df_ethnicity = group_by_data[ethnicity]\n",
    "    \n",
    "    for disease in diseases:\n",
    "        sampled_test = sample_test(df_ethnicity, disease, N)\n",
    "        test_dfs.append(sampled_test)\n",
    "\n",
    "# Combine test samples\n",
    "final_test_df = pd.concat(test_dfs).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Save\n",
    "final_test_df.to_csv('test_split.csv', index=False)\n",
    "\n",
    "print(\"✅ Done! Test shape:\", final_test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\",\n",
    "    \"Enlarged Cardiomediastinum\", \"Fracture\", \"Lung Lesion\",\n",
    "    \"Lung Opacity\", \"No Finding\", \"Pleural Effusion\", \"Pleural Other\",\n",
    "    \"Pneumonia\", \"Pneumothorax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Support Devices'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['No Finding'].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test[test['Fracture'] == 1].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['Fracture'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test[test['Fracture'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_merge_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['Sampled_Ethnicity', 'Sampled_Disease'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_data = {}\n",
    "group_data = test.groupby('ethnicity')\n",
    "for g, d in group_data:\n",
    "     group_by_data[g] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices'\n",
    "]\n",
    "\n",
    "feature_counts = []\n",
    "\n",
    "for feature in features:\n",
    "    counts = group_by_data['Non-Hispanic/Non-Latino'][feature].value_counts()\n",
    "    for value, count in counts.items():\n",
    "        feature_counts.append({\n",
    "            'Feature': feature,\n",
    "            'Value': value,\n",
    "            'Count': count,\n",
    "            'ratio': np.round(count / len(group_by_data['Non-Hispanic/Non-Latino']),2)\n",
    "        })\n",
    "\n",
    "summary_non_hispanic_df = pd.DataFrame(feature_counts)\n",
    "\n",
    "summary_non_hispanic_df = summary_non_hispanic_df.sort_values(by=['Feature', 'Count']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_non_hispanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dataset = pd.read_csv(r\"E:\\Thesis\\CXR_Preprocessing\\cxr_preprocessing\\sampling_data_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r'E:\\Thesis\\CXR_Preprocessing\\cxr_preprocessing\\test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['subject_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_data = sampling_dataset[~sampling_dataset['subject_id'].isin(test['subject_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampling_dataset) - len(traning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices'\n",
    "]\n",
    "\n",
    "feature_counts = []\n",
    "\n",
    "for feature in features:\n",
    "    counts = group_by_data['Hispanic/Latino'][feature].value_counts()\n",
    "    for value, count in counts.items():\n",
    "        feature_counts.append({\n",
    "            'Feature': feature,\n",
    "            'Value': value,\n",
    "            'Count': count,\n",
    "            'ratio': np.round(count / len(group_by_data['Hispanic/Latino']),2)\n",
    "        })\n",
    "\n",
    "summary_hispanic_df = pd.DataFrame(feature_counts)\n",
    "\n",
    "summary_hispanic_df = summary_hispanic_df.sort_values(by=['Feature', 'Count']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hispanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_non_hispanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_data['Non-Hispanic/Non-Latino'].value_counts(subset=[\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.read_csv(r\"E:\\Thesis\\CXR_Preprocessing\\sampling_data_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = np.random.randint(len(training_data_merge['Path']), size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for i in range(len(random_numbers)):\n",
    "     img = cv2.imread(r'..//..//datasets/' + str(training_data_merge['Path'][random_numbers[i]]), cv2.IMREAD_GRAYSCALE)\n",
    "     imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in imgs:\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    adaptive_img = clahe.apply(i)\n",
    "    \n",
    "    images = [i ,adaptive_img]\n",
    "    titles = ['Original Image',  'Adaptive Hist']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))  \n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(titles[i])\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].hist(image.ravel(), bins=256)\n",
    "        axes[i, 1].set_title('Histogram')\n",
    "\n",
    "        axes[i, 2].hist(image.ravel(), bins=256, cumulative=True)\n",
    "        axes[i, 2].set_title('Cumulative Distribution')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in imgs:\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    adaptive_img = clahe.apply(i)\n",
    "\n",
    "\n",
    "    images = [i ,adaptive_img]\n",
    "    titles = ['Original Image',  'Adaptive Hist']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))  \n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(titles[i])\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].hist(image.ravel(), bins=256)\n",
    "        axes[i, 1].set_title('Histogram')\n",
    "\n",
    "        axes[i, 2].hist(image.ravel(), bins=256, cumulative=True)\n",
    "        axes[i, 2].set_title('Cumulative Distribution')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_hist_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_hist_img = cv2.equalizeHist(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "ada_hist_img = clahe.apply(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "# Plot original histogram and cumulative histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(eq_hist_img, cmap='gray')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(), bins=255, range=(1,255))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(eq_hist_img.ravel()*255, bins=255,  range=(1,255))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(ada_hist_img.ravel()*255, bins=255,  range=(1,255))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ada_hist_img, cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(), bins=256, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(ada_hist_img.ravel(), bins=256, cumulative=True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(eq_hist_img.ravel(), bins=256, cumulative=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset = pd.read_csv('../../mimic_cxr/mimic_cxr_jpg_with_demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_datasets(traning_dataset):\n",
    "\n",
    "    traning_dataset[['Atelectasis', 'Cardiomegaly',\n",
    "       'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture',\n",
    "       'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n",
    "       'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']] = (traning_dataset[['Atelectasis', 'Cardiomegaly',\n",
    "       'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture',\n",
    "       'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n",
    "       'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']].fillna(0.0) == 1.0).astype(int) # In The limits of fair medical imaging paper they treat uncertain label as negative and fill NA with 0.\n",
    "\n",
    "    return traning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_most_positive_sample(group):\n",
    "    disease_columns = [\n",
    "        'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "        'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', \n",
    "        'Lung Opacity', 'No Finding', 'Pleural Effusion', \n",
    "        'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n",
    "    ]\n",
    "    \n",
    "    # Count positive cases\n",
    "    group['positive_count'] = group[disease_columns].sum(axis=1)\n",
    "\n",
    "    # Select cases with at least one positive disease\n",
    "    positive_cases = group[group['positive_count'] > 0]\n",
    "    \n",
    "    if not positive_cases.empty:\n",
    "        selected_sample = positive_cases.loc[[positive_cases['positive_count'].idxmax()]].copy()\n",
    "    else:\n",
    "        selected_sample = group.sample(n=1)\n",
    "\n",
    "    return selected_sample\n",
    "\n",
    "# Select one subject_id per patient with the most positive diseases\n",
    "def sampling_datasets(training_dataset):\n",
    "    training_dataset = training_dataset.groupby('subject_id', group_keys=False).apply(select_most_positive_sample)\n",
    "    \n",
    "    # Drop helper column\n",
    "    training_dataset.drop(columns=['positive_count'], inplace=True, errors='ignore')\n",
    "\n",
    "    return training_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset = cleaning_datasets(mimic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_mimic_dataset = sampling_datasets(mimic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id, study_id = sampling_mimic_dataset['subject_id'], sampling_mimic_dataset['study_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "with open('IMAGE_FILENAMES.txt', 'r') as f:\n",
    "     paths = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = []\n",
    "for i in paths:\n",
    "     clean_path.append(i[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][0:-1][11:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][0:-1][21:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "for path in clean_path:\n",
    "    \n",
    "    patient_id = path[11:19]\n",
    "    study_id = path[21:29]\n",
    "    data.append((patient_id, study_id, path))\n",
    "\n",
    "df_paths = pd.DataFrame(data, columns=['subject_id', 'study_id', 'file_path'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths['subject_id'] = df_paths['subject_id'].astype(str)\n",
    "df_paths['study_id'] = df_paths['study_id'].astype(str)\n",
    "\n",
    "# Ensure mimic_dataset has the same data types\n",
    "sampling_mimic_dataset['subject_id'] = sampling_mimic_dataset['subject_id'].astype(str)\n",
    "sampling_mimic_dataset['study_id'] = sampling_mimic_dataset['study_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mimic_dataset = sampling_mimic_dataset.merge(df_paths, on=['subject_id', 'study_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mimic_dataset.drop_duplicates(subset=['subject_id', 'study_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = '//gaia/imageData/public/MIMIC-CXR/physionet.org/files/mimic-cxr-jpg/2.1.0' + str(merge_mimic_dataset['file_path'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imread(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, img1, img2,img3 ,img4, img5,img6,img7,img8 = plt.imread(r\"E:\\Thesis\\demo.jpg\"),plt.imread(r\"E:\\Thesis\\demo1.jpg\"), plt.imread(r\"E:\\Thesis\\demo (2).jpg\"), plt.imread(r\"E:\\Thesis\\demo (3).jpg\"), plt.imread(r\"E:\\Thesis\\demo (4).jpg\"), plt.imread(r\"E:\\Thesis\\demo (5).jpg\"), plt.imread(r\"E:\\Thesis\\demo (6).jpg\"), plt.imread(r\"E:\\Thesis\\demo (7).jpg\"), plt.imread(r\"E:\\Thesis\\demo (8).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_img, eq_img1, eq_img2, eq_img3,eq_img4,eq_img5,eq_img6,eq_img7,eq_img8 = exposure.equalize_hist(img), exposure.equalize_hist(img1), exposure.equalize_hist(img2), exposure.equalize_hist(img3), exposure.equalize_hist(img4), exposure.equalize_hist(img5), exposure.equalize_hist(img6), exposure.equalize_hist(img7), exposure.equalize_hist(img8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [img, img1, img2, img3, img4, img5, img6, img7, img8]\n",
    "equalized_images = [eq_img, eq_img1, eq_img2, eq_img3, eq_img4, eq_img5, eq_img6, eq_img7, eq_img8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_equalize(image_path):\n",
    "    # Read image in grayscale\n",
    "    original_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image to (224, 224)\n",
    "    resized_img = cv2.resize(original_img, (224,224))\n",
    "    \n",
    "    # Exclude pixels with 0 value for processing\n",
    "    img = resized_img[(resized_img > 0)]\n",
    "    \n",
    "    # Equalize the histogram of the image\n",
    "    hist = exposure.equalize_hist(img,nbins=255)\n",
    "    \n",
    "    return resized_img, hist\n",
    "\n",
    "# Paths for your 9 images\n",
    "image_paths = [\n",
    "    r\"E:\\Thesis\\demo.jpg\",\n",
    "    r\"E:\\Thesis\\demo1.jpg\",\n",
    "    r\"E:\\Thesis\\demo (2).jpg\",\n",
    "    r\"E:\\Thesis\\demo (3).jpg\",\n",
    "    r\"E:\\Thesis\\demo (4).jpg\",\n",
    "    r\"E:\\Thesis\\demo (5).jpg\",\n",
    "    r\"E:\\Thesis\\demo (6).jpg\",\n",
    "    r\"E:\\Thesis\\demo (7).jpg\",\n",
    "    r\"E:\\Thesis\\demo (9).jpg\"\n",
    "]\n",
    "\n",
    "# Prepare lists to store results\n",
    "original_images = []\n",
    "equalized_images = []\n",
    "\n",
    "# Process each image\n",
    "for path in image_paths:\n",
    "    orig, eq = read_and_equalize(path)\n",
    "    original_images.append(orig)\n",
    "    equalized_images.append(eq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, hist = read_and_equalize(r\"C:\\Users\\sutariya\\Downloads\\IM-0035-0001.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_img = exposure.equalize_hist(im,nbins=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(im.ravel(), bins=255, range=(1,255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eq_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eq_img.ravel()*255, bins=255, range=(1,255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot original histogram and cumulative histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(equalized_images[i].ravel(), bins=255, range=(1,255))\n",
    "    plt.title(f\"Histogram of Original Image {i+1} (No 0s)\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(original_images[i].ravel(), bins=255, cumulative=True)\n",
    "    plt.title(f\"Cumulative Histogram of Original Image {i+1} (No 0s)\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Cumulative Frequency\")\n",
    "\n",
    "    # Plot equalized histogram and cumulative histogram\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(equalized_images[i].ravel(), bins=255, range=(1,255))\n",
    "    plt.title(f\"Histogram of Equalized Image {i+1} (No 0s)\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(equalized_images[i].ravel(), bins=255, cumulative=True)\n",
    "    plt.title(f\"Cumulative Histogram of Equalized Image {i+1} (No 0s)\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Cumulative Frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(equalized_images[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot original histogram and cumulative histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_images[i], cmap='gray')\n",
    "    plt.title(f\"Image {i}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(equalized_images[i],cmap='gray')\n",
    "    plt.title(f\"Eq_image {i}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file names\n",
    "chexpert_file = \"../../datasets/mimic-cxr-2.0.0-chexpert.csv\"  # MIMIC-CXR metadata\n",
    "patients_file = \"../../datasets/admissions.csv\"  # MIMIC-IV patient demographics\n",
    "output_file = \"mimic_cxr_jpg_with_demographics.csv\"  # Merged output file\n",
    "\n",
    "# Load the CSV files\n",
    "df_chexpert = pd.read_csv(chexpert_file)\n",
    "df_patients = pd.read_csv(patients_file)\n",
    "\n",
    "# Check for duplicate subject_id in patients dataset\n",
    "df_patients_unique = df_patients[['subject_id', 'race']].drop_duplicates(subset=['subject_id'])\n",
    "\n",
    "# Verify uniqueness\n",
    "assert df_patients_unique.duplicated(subset=['subject_id']).sum() == 0, \"Duplicate subject_id found in patients dataset\"\n",
    "\n",
    "# Merge using 'subject_id' (left join to retain only rows in chexpert file)\n",
    "df_merged = df_chexpert.merge(df_patients_unique, on=\"subject_id\", how=\"left\")\n",
    "\n",
    "# Save the merged dataset\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "# Display the number of rows before and after merging\n",
    "print(f\"Original dataset size: {df_chexpert.shape[0]}\")\n",
    "print(f\"Merged dataset size: {df_merged.shape[0]}\")\n",
    "print(f\"Merged dataset saved as: {output_file}\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv('mimic_cxr_jpg_with_demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset['race'] = mimic_dataset['race'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df_cleaned.isnull().sum())  # Shows missing values per column\n",
    "print(df_cleaned.info())  # Checks data types\n",
    "print(df_cleaned.head())  # Displays sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_merged.dropna(subset=['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cleaned.to_csv(\"mimic_cxr_jpg_with_demographics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.read_csv('sampling_data_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = img[(img > 0) & (img < 255)]\n",
    "    hist = cv2.calcHist([img], [0], None, [254], [1, 255])\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_file = \"histogram_count_data.h5\"\n",
    "batch_size = 3000\n",
    "\n",
    "# Open HDF5 file for writing with variable-length data\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    dt = h5py.vlen_dtype(np.int16)  # Variable-length data type\n",
    "    f.create_dataset('hist', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "\n",
    "# Process and store amplitude spectra in batches\n",
    "for i in range(0, len(training_data_merge), batch_size):\n",
    "    batch_paths = training_data_merge['Path'][i:i + batch_size]\n",
    "\n",
    "    # Compute amplitude spectra for batch\n",
    "    batch_hist = [get_histogram(r'..//..//datasets/' + str(p)) for p in batch_paths]\n",
    "\n",
    "    # Append batch to HDF5 file\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        current_size = f['hist'].shape[0]\n",
    "        f['hist'].resize(current_size + len(batch_hist), axis=0)\n",
    "        f['hist'][current_size:] = batch_hist\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(len(training_data_merge) // batch_size) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files_by_race_group(file_path, output_dir, dataframe,  batch_size=1000, is_hist=True):\n",
    "    race_labels = dataframe['race'].values\n",
    "    race_dict = {}\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        for i in range(0, len(dataframe), batch_size):\n",
    "            batch_race_labels = race_labels[i:i + batch_size]\n",
    "            batch_data = f['hist'][i:i + batch_size] if is_hist else f['amplitude_spectrum'][i:i + batch_size]\n",
    "            for data, race in zip(batch_data, batch_race_labels):\n",
    "                if race not in race_dict:\n",
    "                    race_dict[race] = []\n",
    "                race_dict[race].append(data)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for race, data in race_dict.items():\n",
    "        race_file = os.path.join(output_dir, f\"{race}.h5\")\n",
    "        num_samples = len(data)\n",
    "        feature_size = len(data[0])\n",
    "\n",
    "        with h5py.File(race_file, 'w') as f:\n",
    "            dset = f.create_dataset(\n",
    "                \"hist\" if is_hist else \"amplitude_spectrum\",\n",
    "                shape=(0, feature_size),\n",
    "                maxshape=(None, feature_size),\n",
    "                dtype= np.int16 if is_hist else np.float32,\n",
    "                compression=\"gzip\"\n",
    "            )\n",
    "\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                batch = np.array(data[i : i + batch_size], dtype=np.int16 if is_hist else np.float32)\n",
    "                dset.resize(dset.shape[0] + batch.shape[0], axis=0)\n",
    "                dset[-batch.shape[0] :] = batch  \n",
    "                print(f\"Saved batch {i // batch_size + 1} for {race} ({dset.shape[0]} samples total)\")\n",
    "\n",
    "        print(f\"{race} data saved in {race_file} (Total samples: {num_samples})\")\n",
    "\n",
    "    print(\"Race-wise data saved in files successfully!\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(h5_file_path, batch_size=2000, is_hist=True):\n",
    "     mean = 0\n",
    "     with h5py.File(h5_file_path, 'r') as f:\n",
    "          num_samples = len(f['hist']) if is_hist else len(f['amplitude_spectrum'])\n",
    "          if num_samples < batch_size:\n",
    "               batch_size = num_samples\n",
    "          num_batch = num_samples / batch_size\n",
    "          for i in range(0, num_samples, batch_size):\n",
    "               batch = np.array(f['hist'][i : i + batch_size], dtype=np.float32) if is_hist else np.array(f['amplitude_spectrum'][i : i + batch_size], dtype=np.float32)\n",
    "               mean += np.mean(batch, axis=0)\n",
    "     \n",
    "     mean /= num_batch\n",
    "     return mean\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_difference(hist_A, hist_B):\n",
    "    return (hist_B - hist_A) / (hist_A + 1e-6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_race_group('histogram_count_data.h5', 'histograms', training_data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('histograms/Native American.h5', 'r') as f:\n",
    "     hist = f['hist'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hist:\n",
    "     print(i[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hist:\n",
    "     print(i[251:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_hist_mean = calculate_mean('histograms/Asian.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_hist_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_hist_mean = calculate_mean('histograms/White.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asian_specific = calculate_mean('histograms/Pacific Islander.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_hist_mean = calculate_mean('histograms/Other.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_hist_mean = calculate_mean('histograms/Black.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = relative_difference(white_hist_mean, Asian_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_american = calculate_mean('histograms/Native American.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = relative_difference(Asian_specific, unknown_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_mean = calculate_mean('histograms/Unknown.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_american"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(other_hist_mean, color='r', linewidth=2, alpha=0.4)\n",
    "plt.plot(white_hist_mean, color='y', linewidth=2, alpha=0.4)\n",
    "plt.plot(asian_hist_mean, color='g', linewidth=2, alpha=0.4)\n",
    "plt.plot(black_hist_mean, color='b', linewidth=2, alpha=0.4)\n",
    "plt.plot(native_american, color='purple', linewidth=2, alpha=0.4)\n",
    "plt.plot(Asian_specific, color='brown', linewidth=2, alpha=0.4)\n",
    "plt.plot(unknown_mean, color='white', linewidth=2, alpha=0.4)\n",
    "plt.xlabel(\"Pixel Intensity (0-255)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Grayscale Histogram\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_difference(relative_diff, title=\"Relative Difference in Histograms\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(relative_diff, label=\"Relative Difference\")\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.xlabel(\"Histogram Bin\")\n",
    "    plt.ylabel(\"Relative Difference\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eq_hist(path):\n",
    "     img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "     img = cv2.resize(img, (224,224))\n",
    "     img = img[(img > 0) & (img < 255)]\n",
    "     eq_img = exposure.equalize_hist(img, nbins=254)\n",
    "     hist, bins = np.histogram(eq_img, bins=254, range=(0, 1))\n",
    "     return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_file = \"equalize_hist_data.h5\"\n",
    "batch_size = 3000\n",
    "\n",
    "# Open HDF5 file for writing with variable-length data\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    dt = h5py.vlen_dtype(np.int16)  # Variable-length data type\n",
    "    f.create_dataset('hist', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "\n",
    "# Process and store amplitude spectra in batches\n",
    "for i in range(0, len(training_data_merge), batch_size):\n",
    "    batch_paths = training_data_merge['Path'][i:i + batch_size]\n",
    "\n",
    "    # Compute amplitude spectra for batch\n",
    "    batch_hist = [apply_eq_hist(r'..//..//datasets/' + str(p)) for p in batch_paths]\n",
    "\n",
    "    # Append batch to HDF5 file\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        current_size = f['hist'].shape[0]\n",
    "        f['hist'].resize(current_size + len(batch_hist), axis=0)\n",
    "        f['hist'][current_size:] = batch_hist\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(len(training_data_merge) // batch_size) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_race_group(\"equalize_hist_data.h5\", 'equalize_histograms', training_data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_adaptive_equalize_hist(path):\n",
    "     img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "     img = cv2.resize(img, (224,224))\n",
    "     img = img[(img > 0) & (img < 255)]\n",
    "     eq_img = exposure.equalize_adapthist(img, nbins=254)\n",
    "     hist, bins = np.histogram(eq_img, bins=254, range=(0, 1))\n",
    "     return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_file = \"equalize_adaptive_hist_data.h5\"\n",
    "batch_size = 3000\n",
    "\n",
    "# Open HDF5 file for writing with variable-length data\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    dt = h5py.vlen_dtype(np.int16)  # Variable-length data type\n",
    "    f.create_dataset('hist', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "\n",
    "# Process and store amplitude spectra in batches\n",
    "for i in range(0, len(training_data_merge), batch_size):\n",
    "    batch_paths = training_data_merge['Path'][i:i + batch_size]\n",
    "\n",
    "    # Compute amplitude spectra for batch\n",
    "    batch_hist = [apply_adaptive_equalize_hist(r'..//..//datasets/' + str(p)) for p in batch_paths]\n",
    "\n",
    "    # Append batch to HDF5 file\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        current_size = f['hist'].shape[0]\n",
    "        f['hist'].resize(current_size + len(batch_hist), axis=0)\n",
    "        f['hist'][current_size:] = batch_hist\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(len(training_data_merge) // batch_size) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_race_group(\"equalize_adaptive_hist_data.h5\", 'equalize_adaptive_histograms', training_data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_eq_mean, white_eq_mean = calculate_mean('equalize_histograms/Asian.h5'), calculate_mean('equalize_histograms/White.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_ada_eq_mean, white_ada_eq_mean = calculate_mean('equalize_adaptive_histograms/Asian.h5'), calculate_mean('equalize_adaptive_histograms/White.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_ada_eq_mean, black_eq_mean = calculate_mean('equalize_adaptive_histograms/Black.h5'), calculate_mean('equalize_histograms/Black.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_hist_mean = calculate_mean('histograms/Black.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_eq_diff = relative_difference(white_ada_eq_mean, black_ada_eq_mean)\n",
    "diff  = relative_difference(asian_hist_mean, black_hist_mean)\n",
    "eq_diff  = relative_difference(white_eq_mean, black_eq_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(eq_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(ada_eq_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = np.random.randint(len(training_data_merge['Path']), size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(len(random_numbers)):\n",
    "     img = cv2.imread(r'..//..//datasets/' + str(training_data_merge['Path'][random_numbers[i]]), cv2.IMREAD_GRAYSCALE)\n",
    "     images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "\n",
    "# Set the background color of the entire figure\n",
    "fig.patch.set_facecolor('#ADD8E6')  # Light blue background (use any valid color code)\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each subplot and display an image\n",
    "for i in range(100):\n",
    "    mask=images[i]\n",
    "    axes[i].imshow(mask, cmap='gray')\n",
    "    axes[i].axis('off')  # Hide axis labels\n",
    "\n",
    "    # Set background color for each subplot\n",
    "    axes[i].set_facecolor('#F5F5DC')  # Beige background for each subplot\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "\n",
    "# Set the background color of the entire figure\n",
    "fig.patch.set_facecolor('#ADD8E6')  # Light blue background (use any valid color code)\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each subplot and display an image\n",
    "for i in range(100):\n",
    "    mask = np.isin(images[i], [0,1,2,3,4,5,6,7,8,255])\n",
    "    axes[i].imshow(mask, cmap='gray')\n",
    "    axes[i].axis('off')  # Hide axis labels\n",
    "\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_ada_eq_mean = calculate_mean(\"equalize_adaptive_histograms/Black.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_ada_eq_mean, other_hist_mean, other_eq_mean = calculate_mean('equalize_adaptive_histograms/Other.h5'), calculate_mean('histograms/Other.h5'), calculate_mean('equalize_histograms/Other.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_hist_mean = (asian_hist_mean + black_hist_mean + white_hist_mean+other_hist_mean) / 4\n",
    "avg_eq_mean = (asian_eq_mean + black_eq_mean + white_eq_mean+other_eq_mean) / 4\n",
    "avg_ada_eq_mean = (asian_ada_eq_mean + black_ada_eq_mean + white_ada_eq_mean+other_ada_eq_mean) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('equalize_histograms/Other.h5', 'r') as f:\n",
    "     hist = f['hist'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))  # Wider figure for better spacing\n",
    "\n",
    "# Subplot 1: Histogram Means\n",
    "plt.subplot(1, 3, 1)  # (rows, columns, index)\n",
    "plt.plot(asian_hist_mean, color='r', linewidth=2, alpha=0.5, label='Asian Hist')\n",
    "plt.plot(black_hist_mean, color='b', linewidth=2, alpha=0.5, label='Black Hist')\n",
    "plt.plot(white_hist_mean, color='y', linewidth=2, alpha=0.5, label='White Hist')\n",
    "plt.plot(other_hist_mean, color='g', linewidth=2, alpha=0.5, label='Other Hist')\n",
    "plt.plot(avg_hist_mean, color='#000', linewidth=2, alpha=0.5, label='Average Hist')\n",
    "plt.title(\"Histogram Means\")\n",
    "plt.xlabel(\"Pixel Intensity (0-255)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Equalized Means\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(asian_eq_mean, color='r', linewidth=2, alpha=0.5, label='Asian EQ')\n",
    "plt.plot(black_eq_mean, color='b', linewidth=2, alpha=0.5, label='Black EQ')\n",
    "plt.plot(white_eq_mean, color='y', linewidth=2, alpha=0.5, label='White EQ')\n",
    "plt.plot(other_hist_mean, color='g', linewidth=2, alpha=0.5, label='Other EQ')\n",
    "plt.plot(avg_eq_mean, color='#000', linewidth=2, alpha=0.5, label='Average EQ')\n",
    "plt.title(\"Equalized Means\")\n",
    "plt.xlabel(\"Pixel Intensity (0-255)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 3: Adaptive Equalized Means\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(asian_ada_eq_mean, color='r', linewidth=2, alpha=0.5, label='Asian ADA EQ')\n",
    "plt.plot(black_ada_eq_mean, color='b', linewidth=2, alpha=0.5, label='Black ADA EQ')\n",
    "plt.plot(white_ada_eq_mean, color='y', linewidth=2, alpha=0.5, label='White ADA EQ')\n",
    "plt.plot(other_ada_eq_mean, color='g', linewidth=2, alpha=0.5, label='Other ADA EQ')\n",
    "plt.plot(avg_ada_eq_mean, color='#000', linewidth=2, alpha=0.5, label='Average ADA EQ')\n",
    "plt.title(\"Adaptive Equalized Means\")\n",
    "plt.xlabel(\"Pixel Intensity (0-255)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_diff = relative_difference(other_eq_mean, asian_eq_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_difference(eq_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amplitude_spectrum(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (224,224))\n",
    "    tukey_window = tukey(image.shape[0], alpha=0.5)\n",
    "    tukey_2d = np.outer(tukey_window, tukey_window)\n",
    "    tukey_image = image * tukey_2d\n",
    "    tukey_image = (tukey_image - tukey_image.min()) / (tukey_image.max() - tukey_image.min())\n",
    "    tukey_image = tukey_image - tukey_image.mean()\n",
    "\n",
    "    f_image = np.fft.fft2(tukey_image)\n",
    "    f_shift = np.fft.fftshift(f_image)\n",
    "    amplitude_spectrum = np.abs(f_shift).astype(np.float32)\n",
    "    return amplitude_spectrum.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_file = \"amplitude_data.h5\"\n",
    "batch_size = 4000\n",
    "\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    f.create_dataset('amplitude_spectrum', shape=(0, 50176), maxshape=(None, 50176),\n",
    "                     dtype=np.float32, compression=\"gzip\", chunks=True)\n",
    "\n",
    "for i in range(0, len(training_data_merge), batch_size):\n",
    "    batch_paths = training_data_merge['Path'][i:i + batch_size]\n",
    "\n",
    "    batch_spectra = [get_amplitude_spectrum(r'..//..//datasets/' + str(p)) for p in batch_paths]\n",
    "\n",
    "    batch_spectra = np.vstack(batch_spectra)\n",
    "\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        f['amplitude_spectrum'].resize(f['amplitude_spectrum'].shape[0] + batch_spectra.shape[0], axis=0)\n",
    "        f['amplitude_spectrum'][-batch_spectra.shape[0]:] = batch_spectra\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(len(training_data_merge) // batch_size) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_race_group('amplitude_data.h5', 'spectrums', training_data_merge, 2000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_spec_mean, black_spec_mean = calculate_mean('spectrums/Asian.h5', is_hist=False), calculate_mean('spectrums/Black.h5', is_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_spec_mean , white_spec_mean = calculate_mean('spectrums/Other.h5', is_hist=False), calculate_mean('spectrums/White.h5', is_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_diff = relative_difference(other_spec_mean, black_spec_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(spec_diff.reshape(224,224), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "def pca_reduce_large(x_hdf5, y_hdf5, n_components, batchsize=None, normalize=True):\n",
    "    with h5py.File(x_hdf5, 'r') as x_file:\n",
    "        nfeat = x_file['amplitude_spectrum'].shape[1] \n",
    "        xlen = x_file['amplitude_spectrum'].shape[0]\n",
    "\n",
    "    with h5py.File(y_hdf5, 'r') as y_file:\n",
    "        assert y_file['amplitude_spectrum'].shape[1] == nfeat\n",
    "        ylen = y_file['amplitude_spectrum'].shape[0]\n",
    "\n",
    "    if batchsize is None:\n",
    "        batchsize = min(min(xlen, ylen), int(1e9 / 4 / (224 * 112) / 4))\n",
    "\n",
    "    target_feat_size = 224 * 112  \n",
    "\n",
    "    pca = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "    if normalize:\n",
    "        avg = np.zeros(target_feat_size)\n",
    "        var = np.zeros(target_feat_size)\n",
    "        \n",
    "        with h5py.File(x_hdf5, 'r') as x_file:\n",
    "            for chunk_idx in range(0, xlen, batchsize):\n",
    "                chunk = x_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "                avg += np.mean(chunk, axis=0) * chunk.shape[0] / (xlen + ylen)\n",
    "                var += np.var(chunk, axis=0) * chunk.shape[0] / (xlen + ylen)\n",
    "\n",
    "        with h5py.File(y_hdf5, 'r') as y_file:\n",
    "            for chunk_idx in range(0, ylen, batchsize):\n",
    "                chunk = y_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "                avg += np.mean(chunk, axis=0) * chunk.shape[0] / (xlen + ylen)\n",
    "                var += np.var(chunk, axis=0) * chunk.shape[0] / (xlen + ylen)\n",
    "        \n",
    "        std = np.sqrt(var)\n",
    "\n",
    "    with h5py.File(x_hdf5, 'r') as x_file:\n",
    "        for chunk_idx in range(0, xlen, batchsize):\n",
    "            chunk = x_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "            if normalize:\n",
    "                chunk = (chunk - avg) / std\n",
    "            pca.partial_fit(chunk)\n",
    "\n",
    "    with h5py.File(y_hdf5, 'r') as y_file:\n",
    "        for chunk_idx in range(0, ylen, batchsize):\n",
    "            chunk = y_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "            if normalize:\n",
    "                chunk = (chunk - avg) / std\n",
    "            pca.partial_fit(chunk)\n",
    "\n",
    "    x_transformed = np.zeros((xlen, n_components))\n",
    "    y_transformed = np.zeros((ylen, n_components))\n",
    "\n",
    "    with h5py.File(x_hdf5, 'r') as x_file:\n",
    "        for chunk_idx in range(0, xlen, batchsize):\n",
    "            chunk = x_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "            if normalize:\n",
    "                chunk = (chunk - avg) / std\n",
    "            x_transformed[chunk_idx:chunk_idx+batchsize, :] = pca.transform(chunk)\n",
    "\n",
    "    with h5py.File(y_hdf5, 'r') as y_file:\n",
    "        for chunk_idx in range(0, ylen, batchsize):\n",
    "            chunk = y_file['amplitude_spectrum'][chunk_idx:chunk_idx+batchsize, :target_feat_size]\n",
    "            if normalize:\n",
    "                chunk = (chunk - avg) / std\n",
    "            y_transformed[chunk_idx:chunk_idx+batchsize, :] = pca.transform(chunk)\n",
    "\n",
    "    print(f'Dim-reduced using incremental PCA from {nfeat} to {x_transformed.shape[1]} features to explain {pca.explained_variance_ratio_.sum():.4f}% of observed variance.')  \n",
    "\n",
    "    return x_transformed, y_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_transformed, asian_transformed = pca_reduce_large(\"race_wise_hdf5/Black.h5\", \"race_wise_hdf5/Asian.h5\", n_components=100, batchsize=3000, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True, fastmath=True)\n",
    "def mean_axis_0(x):\n",
    "    mean = np.zeros(x.shape[1], dtype=x.dtype)\n",
    "    x = x / x.shape[0]\n",
    "    for i in prange(x.shape[0]):\n",
    "        mean += x[i, :]\n",
    "\n",
    "    return mean \n",
    "\n",
    "\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def direct_covariance(x, mean_x):\n",
    "    n, p = x.shape\n",
    "    cov = np.zeros((p, p))\n",
    "    \n",
    "    for i in prange(p):\n",
    "        for j in range(i + 1):  # Use symmetry for efficiency\n",
    "            sum_val = 0.0\n",
    "            #for k in range(n):\n",
    "            #    sum_val += (x[k, i] - mean_x[i]) * (x[k, j] - mean_x[j])\n",
    "            #cov[i, j] = sum_val / n\n",
    "            cov[i, j] = np.sum((x[:, i] - mean_x[i]) * (x[:, j] - mean_x[j]))\n",
    "            if i != j:\n",
    "                cov[j, i] = cov[i, j]  # Fill the symmetric part\n",
    "    \n",
    "    return cov\n",
    "\n",
    "\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def cov(x, mean_x):\n",
    "    for ii in prange(x.shape[0]):\n",
    "        x[ii, :] -= mean_x\n",
    "    c = np.dot(x.T, x)  # this does recognize and exploit that the result will be symmetrical, see https://stackoverflow.com/a/43454451/2207840\n",
    "    c *= np.true_divide(1, x.shape[0] - 1)\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def hotelling_t2(x, y, bessel=True):\n",
    "    r\"\"\"hotelling_t2.\n",
    "\n",
    "    Compute the Hotelling (T2) test statistic.\n",
    "\n",
    "    It is the multivariate extension of the Student's t-test.\n",
    "    Test the null hypothesis that two multivariate samples have the same underlying\n",
    "    probability distribution, when specifying samples for x and y. The number of samples do not have\n",
    "    to be the same, but the number of features does have to be equal.\n",
    "\n",
    "    Equation:\n",
    "\n",
    "    Hotelling's t-squared statistic is defined as:\n",
    "\n",
    "    .. math::\n",
    "        T^2 = n (\\\\bar{x} - {\\mu})^{T} S^{-1} (\\\\bar{x} - {\\mu})\n",
    "\n",
    "    Where S is the pooled covariance matrix and ᵀ represents the transpose.\n",
    "\n",
    "    The two sample t-squared statistic is defined as:\n",
    "\n",
    "    .. math::\n",
    "        T^2 = (\\\\bar{x} - \\\\bar{y})^{T} [S(\\\\frac1 n_x +\\\\frac 1 n_y)]^{-1} (\\\\bar{x}̄ - \\\\bar{y})\n",
    "\n",
    "    References:\n",
    "        - Hotelling, Harold. (1931). The Generalization of Student's Ratio. Ann. Math. Statist. 2, no. 3, 360--378.\n",
    "          doi:10.1214/aoms/1177732979. https://projecteuclid.org/euclid.aoms/1177732979\n",
    "\n",
    "        - Hotelling, Harold. (1955) Les Rapports entre les Methodes Statistiques recentes portant sur des Variables Multiples\n",
    "          et l'Analyse Factorielle. 107-119.\n",
    "          In: L'Analyse Factorielle et ses Applications. Centre National de la Recherche Scientifique, Paris.\n",
    "\n",
    "        - Anderson T.W. (1992) Introduction to Hotelling (1931) The Generalization of Student’s Ratio.\n",
    "          In: Kotz S., Johnson N.L. (eds) Breakthroughs in Statistics.\n",
    "          Springer Series in Statistics (Perspectives in Statistics). Springer, New York, NY\n",
    "\n",
    "    :param x: array-like, samples of observations\n",
    "    :param y: array-like, samples of observations\n",
    "    :param bessel: bool, apply bessel correction (default)\n",
    "    :return:\n",
    "        statistic: float,\n",
    "            the t2 statistic\n",
    "        #f_value: float,\n",
    "        #    the f value\n",
    "        #p_value: float,\n",
    "        #    the p value\n",
    "        #s: 2d array,\n",
    "        #    the pooled variance\n",
    "    \"\"\"  # noqa: W605\n",
    "\n",
    "    nx, ny = x.shape[0], y.shape[0]\n",
    "\n",
    "    n1 = nx - 1 if bessel else nx\n",
    "    n2 = ny - 1 if bessel else ny\n",
    "\n",
    "    x_bar = mean_axis_0(x)\n",
    "    y_bar = mean_axis_0(y)\n",
    "    diff_bar = x_bar - y_bar\n",
    "\n",
    "    p = x.shape[1:]\n",
    "    p = p[0] if p else 1\n",
    "\n",
    "    #s1 = n1 * np.cov(x, rowvar=False).astype(np.float32)\n",
    "    #s2 = n2 * np.cov(y, rowvar=False).astype(np.float32)\n",
    "    #s1 = n1 * direct_covariance(x, x_bar)\n",
    "    #s2 = n2 * direct_covariance(y, y_bar)\n",
    "    s1 = n1 * cov(x, x_bar)\n",
    "    s2 = n2 * cov(y, y_bar)\n",
    "\n",
    "    pooled_cov = (s1 + s2) / (n1 + n2)\n",
    "\n",
    "    #if p > n1 + n2 - 2:\n",
    "        # For high-dimensional data, add regularization and compute pseudoinverse\n",
    "    #    inv_pooled_cov = np.linalg.pinv(pooled_cov + 1e-6 * np.eye(p))\n",
    "    #else:\n",
    "        #inv_pooled_cov = np.linalg.inv(pooled_cov)\n",
    "    #    inv_pooled_cov = np.linalg.solve(pooled_cov, np.identity(p))\n",
    "\n",
    "    #t2_stat = nx * ny / (nx + ny) * (diff_bar.T @ inv_pooled_cov @ diff_bar)\n",
    "\n",
    "    z = np.linalg.solve(pooled_cov, diff_bar)\n",
    "    t2_stat = nx * ny / (nx + ny) * (diff_bar.T @ z)\n",
    "    \n",
    "    return t2_stat\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def compute_null_distribution(xy, xlen, n_permut, test_statistic):\n",
    "    null_distribution = np.zeros((n_permut,))\n",
    "\n",
    "    for ii in prange(n_permut):\n",
    "        indices = np.random.permutation(xy.shape[0])\n",
    "\n",
    "        perm_x = xy[indices[:xlen], :]\n",
    "        perm_y = xy[indices[xlen:], :]\n",
    "\n",
    "        null_distribution[ii] = test_statistic(perm_x, perm_y)\n",
    "\n",
    "    return null_distribution\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def euclidean_distance(x, y):\n",
    "    \"\"\"Calculate Euclidean distance between mean spectra\"\"\"\n",
    "    mean1 = mean_axis_0(x)\n",
    "    mean2 = mean_axis_0(y)\n",
    "    return np.sqrt(np.sum((mean1 - mean2)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_differences(x, y, normalize=True, n_permut=999, test_statistic='hotelling'):\n",
    "\n",
    "    assert x.shape[1] == y.shape[1], \"x and y can have different numbers of samples but must have the same number of features\"\n",
    "\n",
    "    xlen, ylen = x.shape[0], y.shape[0]\n",
    "    nfeat = x.shape[1]\n",
    "\n",
    "    assert xlen > 1\n",
    "    assert ylen > 1\n",
    "\n",
    "    if nfeat > 1000 and test_statistic == 'hotelling':\n",
    "        warn(\"Dimensionality reduction is strongly advised for the hotelling statistic with very large feature space; your memory will explode.\")\n",
    "\n",
    "    x = x.astype(np.float32, copy=False)\n",
    "    y = y.astype(np.float32, copy=False)\n",
    "    xy = np.vstack((x, y))\n",
    "\n",
    "    if normalize:\n",
    "        print('Start normalization')\n",
    "        xy = (xy - np.mean(xy, axis=0)) / np.std(xy, axis=0)\n",
    "        x = xy[:xlen, :]\n",
    "        y = xy[xlen:, :]\n",
    "\n",
    "    if test_statistic == 'hotelling':\n",
    "        #test_statistic = lambda x, y: hotelling_t2(x, y)[0]\n",
    "        test_statistic = hotelling_t2\n",
    "    elif test_statistic == \"euclidean\":\n",
    "        test_statistic = euclidean_distance\n",
    "    else:\n",
    "        assert callable(test_statistic), \"test_statistic must be a callable with two arguments (the two samples) that returns a single number, the value of the statistic.\"\n",
    "\n",
    "    print('Calculating observed stat')\n",
    "    observed_stat = test_statistic(x, y)\n",
    "\n",
    "    print('Calculating null distribution')\n",
    "    null_distribution = compute_null_distribution(xy, xlen, n_permut, test_statistic)\n",
    "\n",
    "    pval = np.mean(np.abs(null_distribution) >= np.abs(observed_stat))\n",
    "\n",
    "    return pval, observed_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "print('Start')\n",
    "start = time()\n",
    "pval, stat = test_differences(asian_transformed, black_transformed, normalize=False)  # normalization not necessary since we already did that + PCA\n",
    "end = time()\n",
    "print(f\"Runtime with hotelling: {end-start:.2f}. Pval, stat:\")\n",
    "print([pval, stat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']] = training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "    'Support Devices']].replace(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx] / 255.0\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.read_csv('sampling_data_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_group = training_data_merge.groupby('race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = pd.DataFrame(race_group['Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_ethnicity(data, ethnicity_column):\n",
    "    \n",
    "    ethnicity_groups = {}\n",
    "\n",
    "    # Group by ethnicity\n",
    "    for ethnicity_name, group_data in data.groupby(ethnicity_column):\n",
    "        ethnicity_groups[ethnicity_name] = (group_data)\n",
    "\n",
    "    return ethnicity_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = training_data_merge[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_by_ethnicity(training_data_merge, 'ethnicity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Hispanic/Latino']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.ethnicity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups_data = {}\n",
    "for name, group_data in training_data_merge.groupby('ethnicity'):\n",
    "    images = []\n",
    "    paths = tqdm(group_data['Path'], desc=\"Loading images\")\n",
    "    for p in paths:\n",
    "        full_path = '//gaia/imageData/deep_learning/output/Sutariya/chexpert' + '/' + str(p)\n",
    "        img = read_image(full_path)\n",
    "        images.append(img)\n",
    "        paths.set_postfix({'Loaded': len(group_data['Path'])})\n",
    "    data_labels = group_data[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']].values\n",
    "    groups_data[name] = (images, torch.tensor(data_labels, dtype=torch.long))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_data['Non-Hispanic/Non-Latino'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_images['Non-Hispanic/Non-Latino'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnic = training_data_merge.ethnicity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_ethnic_images_labels(dataset, path, device):\n",
    "    \"\"\"\n",
    "    Loads images and processes ethnic labels.\n",
    "\n",
    "    Args:\n",
    "    - training_data_merge (DataFrame): Contains image paths and diagnostic labels.\n",
    "\n",
    "    Returns:\n",
    "    - data_images (List[Tensor]): List of image tensors.\n",
    "    - data_labels (Tensor): Tensor of diagnostic class labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        data_images = []\n",
    "        paths = tqdm(dataset['Path'], desc=\"Loading images\")\n",
    "        for path in paths:\n",
    "            full_path = '/deep_learning/output/Sutariya/chexpert' + '/' + str(path)\n",
    "            img = read_image(full_path)\n",
    "            data_images.append(img)\n",
    "            paths.set_postfix({'Loaded': len(data_images)})\n",
    "        torch.save(data_images, path)\n",
    "    else:\n",
    "        data_images = torch.load(path, map_location=device, weights_only=True)\n",
    "\n",
    "    data_labels = dataset[['Non-Hispanic/Non-Latino', 'Hispanic/Latino', 'Unknown',\n",
    "       'Patient Refused']].values\n",
    "    data_labels = torch.tensor(data_labels, dtype=torch.float32)\n",
    "    \n",
    "    return data_images, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = pd.read_csv(r\"\\\\gaia\\imageData\\deep_learning\\output\\Sutariya\\chexpert\\valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images = []\n",
    "for path in training_data_merge['Path']:\n",
    "     full_path = '../../datasets' + '/' + str(path)\n",
    "     img = read_image(full_path)\n",
    "     data_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data_merge['No Finding'].values\n",
    "data_labels = torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Lambda(lambda i: i/255),\n",
    "    transforms.Lambda(lambda i: i.to(torch.float32)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data_images,labels,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in dataloader:\n",
    "     print(i.shape)\n",
    "     print(l.shape)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_diagnostic_images_labels(training_data_merge):\n",
    "    data_images = []\n",
    "    paths = tqdm(training_data_merge['Path'], desc=\"Loading images\")\n",
    "    for path in paths:\n",
    "        full_path = '../../datasets' + '/' + str(path)\n",
    "        img = read_image(full_path)\n",
    "        data_images.append(img)\n",
    "        paths.set_postfix({'Loaded': len(data_images)})\n",
    "\n",
    "    data_labels = training_data_merge[['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "    'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "    'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture']].values\n",
    "    data_labels = torch.tensor(data_labels, dtype=torch.float32)\n",
    "    data_labels = torch.argmax(data_labels, dim=1)\n",
    "\n",
    "    return data_images, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(data_images, labels,shuffle=True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop((256,256), scale=(0.7, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.Lambda(lambda i: i/255),\n",
    "        transforms.Lambda(lambda i: i.to(torch.float32)),\n",
    "        transforms.Normalize(mean=[0.50,0.50,0.50], std=[0.28,0.28,0.28])\n",
    "    ])\n",
    "\n",
    "    dataset = MyDataset(data_images,labels,transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=32, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv('../../datasets/train_clean_dataset.csv')\n",
    "validation_dataset = pd.read_csv('../../datasets/validation_clean_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_images,train_labels = store_diagnostic_images_labels(training_dataset)\n",
    "val_data_images, val_lables = store_diagnostic_images_labels(validation_dataset)\n",
    "train_loader = prepare_dataloaders(train_data_images,train_labels, shuffle=True)\n",
    "val_loader = prepare_dataloaders(val_data_images, val_lables,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store pixel values\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "for images, _ in dataloader:\n",
    "    # images shape: (batch_size, 1, 256 * 256)\n",
    "    batch_samples = images.size(0)  # Batch size\n",
    "    images = images.view(batch_samples, -1)  # Flatten pixels\n",
    "    mean += images.mean(dim=1).sum()\n",
    "    std += images.std(dim=1).sum()\n",
    "    num_samples += batch_samples\n",
    "\n",
    "# Final mean and std\n",
    "mean /= num_samples\n",
    "std /= num_samples\n",
    "\n",
    "print(f\"Mean: {mean.item():.4f}, Std: {std.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_races = training_data_merge['race'].value_counts().index[:5]\n",
    "training_data_merge['race'] = training_data_merge['race'].where(training_data_merge['race'].isin(top_5_races))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge = pd.get_dummies(training_data_merge, columns=['race'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data_merge[['race_Asian', 'race_Black', 'race_Other', 'race_Unknown', 'race_White']].values\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.argmax(labels, dim=1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Lambda(lambda x: x.to(torch.float32)),\n",
    "    transforms.Lambda(lambda i: i.repeat(3, 1, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet_Model(nn.Module):\n",
    "     def __init__(self, weights, out_feature):\n",
    "          super().__init__()\n",
    "          self.weight = weights\n",
    "          self.out_feature = out_feature\n",
    "          self.encoder = torchvision.models.densenet121(weights=weights)\n",
    "          self.layer1 = nn.Linear(1000, 120)\n",
    "          self.clf = nn.Linear(120, out_feature)\n",
    "\n",
    "     \n",
    "     def encode(self, x):\n",
    "          return self.encoder(x)\n",
    "\n",
    "     def forward(self, x):\n",
    "          z = self.encode(x)\n",
    "          z = self.layer1(z)\n",
    "          return self.clf(z)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet_Model(weights=None, out_feature=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torchvision.models.densenet121(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data_images,labels,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset)) \n",
    "val_size = int(0.10 * len(dataset))  \n",
    "test_size = len(dataset) - train_size - val_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, img in train_loader:\n",
    "     print(label.shape, img.shape)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_roc_auc(y_true, y_scores, log=True, log_name=\"roc_auc_curve\"):\n",
    "\n",
    "    y_scores = np.array(y_scores)\n",
    "    classes = np.unique(y_true) \n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    \n",
    "    total_roc = 0\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    \n",
    "    for i, class_label in enumerate(classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        total_roc += roc_auc\n",
    "        ax.plot(fpr, tpr, lw=2, label=f\"Class {class_label} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "    # Plot settings\n",
    "    ax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"Multi-Class ROC Curve\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Log to wandb\n",
    "    if log:\n",
    "        wandb.log({log_name: wandb.Image(fig)})\n",
    "    else:\n",
    "        print(f\"{log_name} : {total_roc/len(classes)}\")\n",
    "\n",
    "    # Close figure to free memory\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, train_loader, val_loader, num_epochs=10, device=None, is_binary=False):\n",
    "    model = model.to(device)\n",
    "\n",
    "    all_train_labels, all_train_preds = [], []\n",
    "    all_val_labels, all_val_preds = [], []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    early_stopper = EarlyStopper(patience=3)\n",
    "    criterion = nn.BCEWithLogitsLoss() if is_binary else nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\", leave=False)\n",
    "        for inputs, tr_labels in train_loop:\n",
    "            inputs, tr_labels = inputs.to(device), tr_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).to(device)\n",
    "\n",
    "            # Convert predictions & labels\n",
    "            if is_binary:\n",
    "                tr_labels = tr_labels.unsqueeze(dim=1)\n",
    "                tr_preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            else:\n",
    "                tr_preds = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            all_train_labels.extend(tr_labels.cpu().numpy())\n",
    "            all_train_preds.extend(tr_preds)\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, tr_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, vl_labels in val_loop:\n",
    "                inputs, vl_labels = inputs.to(device), vl_labels.to(device)\n",
    "\n",
    "                outputs = model(inputs).to(device)\n",
    "                if is_binary:\n",
    "                    vl_labels = vl_labels.unsqueeze(dim=1)\n",
    "                    vl_preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "                else:\n",
    "                    vl_preds = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "                all_val_labels.extend(vl_labels.cpu().numpy())\n",
    "                all_val_preds.extend(vl_preds)\n",
    "\n",
    "                loss = criterion(outputs, vl_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "        # Log metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        log_roc_auc(all_train_labels, all_train_preds, log=True, log_name='Training ROC-AUC')\n",
    "        log_roc_auc(all_val_labels, all_val_preds, log=True, log_name='Validation ROC-AUC')\n",
    "\n",
    "    # Final ROC-AUC Logging\n",
    "    log_roc_auc(all_train_labels, all_train_preds, log=False, log_name='Training ROC-AUC')\n",
    "    log_roc_auc(all_val_labels, all_val_preds, log=False, log_name='Validation ROC-AUC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('no_finding_model_weights.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_model = DenseNet_Model(weights=weights,out_feature=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in race_model.encoder.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training(race_model,train_loader,val_loader,num_epochs=10,is_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(race_model.state_dict(), 'race_finding_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = DenseNet_Model(weights=None, out_feature=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('race_finding_model_weights.pth',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_state_dict(weights,strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(test_loader, model, device, is_binary=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_test_labels, all_test_preds = [], []\n",
    "\n",
    "    loader = tqdm(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if is_binary:\n",
    "                labels = labels.view(-1, 1)\n",
    "                predicted = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            else:\n",
    "                predicted = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "            all_test_preds.extend(predicted)\n",
    "\n",
    "    log_roc_auc(all_test_labels, all_test_preds, log=True, log_name='Testing ROC-AUC')\n",
    "    log_roc_auc(all_test_labels, all_test_preds, log=False, log_name='Testing ROC-AUC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model(test_loader,test_model,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
